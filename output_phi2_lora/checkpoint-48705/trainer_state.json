{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 48705,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006159531875577456,
      "grad_norm": 1.9486867189407349,
      "learning_rate": 0.00019996304280874653,
      "loss": 2.285,
      "step": 10
    },
    {
      "epoch": 0.0012319063751154912,
      "grad_norm": 2.0884346961975098,
      "learning_rate": 0.00019992197926290935,
      "loss": 0.7923,
      "step": 20
    },
    {
      "epoch": 0.0018478595626732369,
      "grad_norm": 1.230968952178955,
      "learning_rate": 0.00019988091571707217,
      "loss": 0.4988,
      "step": 30
    },
    {
      "epoch": 0.0024638127502309825,
      "grad_norm": 1.4644906520843506,
      "learning_rate": 0.000199839852171235,
      "loss": 0.4068,
      "step": 40
    },
    {
      "epoch": 0.003079765937788728,
      "grad_norm": 1.200763463973999,
      "learning_rate": 0.00019979878862539781,
      "loss": 0.3743,
      "step": 50
    },
    {
      "epoch": 0.0036957191253464737,
      "grad_norm": 0.9988047480583191,
      "learning_rate": 0.00019975772507956063,
      "loss": 0.3485,
      "step": 60
    },
    {
      "epoch": 0.004311672312904219,
      "grad_norm": 0.897251307964325,
      "learning_rate": 0.00019971666153372343,
      "loss": 0.3166,
      "step": 70
    },
    {
      "epoch": 0.004927625500461965,
      "grad_norm": 0.9473065137863159,
      "learning_rate": 0.00019967559798788627,
      "loss": 0.3212,
      "step": 80
    },
    {
      "epoch": 0.00554357868801971,
      "grad_norm": 0.9027938842773438,
      "learning_rate": 0.00019963453444204907,
      "loss": 0.302,
      "step": 90
    },
    {
      "epoch": 0.006159531875577456,
      "grad_norm": 0.6537424325942993,
      "learning_rate": 0.0001995934708962119,
      "loss": 0.2907,
      "step": 100
    },
    {
      "epoch": 0.006775485063135201,
      "grad_norm": 0.6740816235542297,
      "learning_rate": 0.0001995524073503747,
      "loss": 0.2855,
      "step": 110
    },
    {
      "epoch": 0.0073914382506929475,
      "grad_norm": 0.6534305810928345,
      "learning_rate": 0.00019951134380453753,
      "loss": 0.2909,
      "step": 120
    },
    {
      "epoch": 0.008007391438250694,
      "grad_norm": 0.9894264340400696,
      "learning_rate": 0.00019947028025870035,
      "loss": 0.2778,
      "step": 130
    },
    {
      "epoch": 0.008623344625808438,
      "grad_norm": 0.8143343329429626,
      "learning_rate": 0.00019942921671286317,
      "loss": 0.2767,
      "step": 140
    },
    {
      "epoch": 0.009239297813366184,
      "grad_norm": 0.8326659202575684,
      "learning_rate": 0.000199388153167026,
      "loss": 0.2634,
      "step": 150
    },
    {
      "epoch": 0.00985525100092393,
      "grad_norm": 0.8508174419403076,
      "learning_rate": 0.0001993470896211888,
      "loss": 0.2882,
      "step": 160
    },
    {
      "epoch": 0.010471204188481676,
      "grad_norm": 0.9226109385490417,
      "learning_rate": 0.00019930602607535163,
      "loss": 0.2555,
      "step": 170
    },
    {
      "epoch": 0.01108715737603942,
      "grad_norm": 0.7546514272689819,
      "learning_rate": 0.00019926496252951442,
      "loss": 0.2779,
      "step": 180
    },
    {
      "epoch": 0.011703110563597166,
      "grad_norm": 0.5877305865287781,
      "learning_rate": 0.00019922389898367727,
      "loss": 0.2542,
      "step": 190
    },
    {
      "epoch": 0.012319063751154912,
      "grad_norm": 0.636070191860199,
      "learning_rate": 0.00019918283543784006,
      "loss": 0.2568,
      "step": 200
    },
    {
      "epoch": 0.012935016938712659,
      "grad_norm": 0.9961565136909485,
      "learning_rate": 0.00019914177189200288,
      "loss": 0.2719,
      "step": 210
    },
    {
      "epoch": 0.013550970126270403,
      "grad_norm": 0.7961704730987549,
      "learning_rate": 0.0001991007083461657,
      "loss": 0.271,
      "step": 220
    },
    {
      "epoch": 0.014166923313828149,
      "grad_norm": 0.5708934664726257,
      "learning_rate": 0.00019905964480032852,
      "loss": 0.2461,
      "step": 230
    },
    {
      "epoch": 0.014782876501385895,
      "grad_norm": 0.5702700614929199,
      "learning_rate": 0.00019901858125449134,
      "loss": 0.2482,
      "step": 240
    },
    {
      "epoch": 0.015398829688943641,
      "grad_norm": 0.5482965707778931,
      "learning_rate": 0.00019897751770865416,
      "loss": 0.2443,
      "step": 250
    },
    {
      "epoch": 0.016014782876501387,
      "grad_norm": 0.6279165744781494,
      "learning_rate": 0.00019893645416281695,
      "loss": 0.2509,
      "step": 260
    },
    {
      "epoch": 0.016630736064059133,
      "grad_norm": 0.6176535487174988,
      "learning_rate": 0.0001988953906169798,
      "loss": 0.2427,
      "step": 270
    },
    {
      "epoch": 0.017246689251616876,
      "grad_norm": 0.5658271908760071,
      "learning_rate": 0.0001988543270711426,
      "loss": 0.251,
      "step": 280
    },
    {
      "epoch": 0.017862642439174622,
      "grad_norm": 0.6696558594703674,
      "learning_rate": 0.00019881326352530541,
      "loss": 0.2462,
      "step": 290
    },
    {
      "epoch": 0.018478595626732368,
      "grad_norm": 0.4986957907676697,
      "learning_rate": 0.00019877219997946823,
      "loss": 0.2451,
      "step": 300
    },
    {
      "epoch": 0.019094548814290114,
      "grad_norm": 0.5280934572219849,
      "learning_rate": 0.00019873113643363105,
      "loss": 0.2303,
      "step": 310
    },
    {
      "epoch": 0.01971050200184786,
      "grad_norm": 0.5697254538536072,
      "learning_rate": 0.00019869007288779387,
      "loss": 0.2489,
      "step": 320
    },
    {
      "epoch": 0.020326455189405606,
      "grad_norm": 0.5775102972984314,
      "learning_rate": 0.0001986490093419567,
      "loss": 0.2399,
      "step": 330
    },
    {
      "epoch": 0.020942408376963352,
      "grad_norm": 0.7006068825721741,
      "learning_rate": 0.0001986079457961195,
      "loss": 0.2477,
      "step": 340
    },
    {
      "epoch": 0.021558361564521098,
      "grad_norm": 0.5282434821128845,
      "learning_rate": 0.00019856688225028233,
      "loss": 0.2511,
      "step": 350
    },
    {
      "epoch": 0.02217431475207884,
      "grad_norm": 0.4974449574947357,
      "learning_rate": 0.00019852581870444513,
      "loss": 0.2439,
      "step": 360
    },
    {
      "epoch": 0.022790267939636587,
      "grad_norm": 0.7058334350585938,
      "learning_rate": 0.00019848475515860795,
      "loss": 0.2446,
      "step": 370
    },
    {
      "epoch": 0.023406221127194333,
      "grad_norm": 0.6031906008720398,
      "learning_rate": 0.00019844369161277077,
      "loss": 0.2325,
      "step": 380
    },
    {
      "epoch": 0.02402217431475208,
      "grad_norm": 0.5118757486343384,
      "learning_rate": 0.0001984026280669336,
      "loss": 0.234,
      "step": 390
    },
    {
      "epoch": 0.024638127502309825,
      "grad_norm": 0.6801791787147522,
      "learning_rate": 0.0001983615645210964,
      "loss": 0.2317,
      "step": 400
    },
    {
      "epoch": 0.02525408068986757,
      "grad_norm": 0.6679694652557373,
      "learning_rate": 0.00019832050097525923,
      "loss": 0.2341,
      "step": 410
    },
    {
      "epoch": 0.025870033877425317,
      "grad_norm": 0.5349563360214233,
      "learning_rate": 0.00019827943742942202,
      "loss": 0.2333,
      "step": 420
    },
    {
      "epoch": 0.02648598706498306,
      "grad_norm": 0.5318285822868347,
      "learning_rate": 0.00019823837388358487,
      "loss": 0.2389,
      "step": 430
    },
    {
      "epoch": 0.027101940252540806,
      "grad_norm": 0.5997408628463745,
      "learning_rate": 0.00019819731033774766,
      "loss": 0.2362,
      "step": 440
    },
    {
      "epoch": 0.027717893440098552,
      "grad_norm": 0.5628076195716858,
      "learning_rate": 0.00019815624679191048,
      "loss": 0.242,
      "step": 450
    },
    {
      "epoch": 0.028333846627656298,
      "grad_norm": 0.4700116515159607,
      "learning_rate": 0.0001981151832460733,
      "loss": 0.2298,
      "step": 460
    },
    {
      "epoch": 0.028949799815214044,
      "grad_norm": 0.49836161732673645,
      "learning_rate": 0.00019807411970023612,
      "loss": 0.2283,
      "step": 470
    },
    {
      "epoch": 0.02956575300277179,
      "grad_norm": 0.42456579208374023,
      "learning_rate": 0.00019803305615439894,
      "loss": 0.2305,
      "step": 480
    },
    {
      "epoch": 0.030181706190329536,
      "grad_norm": 0.47326773405075073,
      "learning_rate": 0.00019799199260856176,
      "loss": 0.2419,
      "step": 490
    },
    {
      "epoch": 0.030797659377887282,
      "grad_norm": 0.5624811053276062,
      "learning_rate": 0.00019795092906272455,
      "loss": 0.2354,
      "step": 500
    },
    {
      "epoch": 0.031413612565445025,
      "grad_norm": 0.6617653965950012,
      "learning_rate": 0.0001979098655168874,
      "loss": 0.2496,
      "step": 510
    },
    {
      "epoch": 0.032029565753002774,
      "grad_norm": 0.5393556356430054,
      "learning_rate": 0.0001978688019710502,
      "loss": 0.2296,
      "step": 520
    },
    {
      "epoch": 0.03264551894056052,
      "grad_norm": 0.49934837222099304,
      "learning_rate": 0.00019782773842521304,
      "loss": 0.2422,
      "step": 530
    },
    {
      "epoch": 0.033261472128118266,
      "grad_norm": 0.4643009305000305,
      "learning_rate": 0.00019778667487937583,
      "loss": 0.2336,
      "step": 540
    },
    {
      "epoch": 0.03387742531567601,
      "grad_norm": 0.46540412306785583,
      "learning_rate": 0.00019774561133353865,
      "loss": 0.2285,
      "step": 550
    },
    {
      "epoch": 0.03449337850323375,
      "grad_norm": 0.484007328748703,
      "learning_rate": 0.0001977045477877015,
      "loss": 0.2355,
      "step": 560
    },
    {
      "epoch": 0.0351093316907915,
      "grad_norm": 0.5688291788101196,
      "learning_rate": 0.0001976634842418643,
      "loss": 0.2401,
      "step": 570
    },
    {
      "epoch": 0.035725284878349244,
      "grad_norm": 0.7963610291481018,
      "learning_rate": 0.00019762242069602711,
      "loss": 0.2314,
      "step": 580
    },
    {
      "epoch": 0.03634123806590699,
      "grad_norm": 0.562644898891449,
      "learning_rate": 0.00019758135715018993,
      "loss": 0.225,
      "step": 590
    },
    {
      "epoch": 0.036957191253464736,
      "grad_norm": 0.47331294417381287,
      "learning_rate": 0.00019754029360435275,
      "loss": 0.226,
      "step": 600
    },
    {
      "epoch": 0.037573144441022485,
      "grad_norm": 0.6198853254318237,
      "learning_rate": 0.00019749923005851557,
      "loss": 0.233,
      "step": 610
    },
    {
      "epoch": 0.03818909762858023,
      "grad_norm": 0.5782467722892761,
      "learning_rate": 0.0001974581665126784,
      "loss": 0.2272,
      "step": 620
    },
    {
      "epoch": 0.03880505081613797,
      "grad_norm": 0.4429034888744354,
      "learning_rate": 0.0001974171029668412,
      "loss": 0.2237,
      "step": 630
    },
    {
      "epoch": 0.03942100400369572,
      "grad_norm": 0.7647493481636047,
      "learning_rate": 0.00019737603942100403,
      "loss": 0.2305,
      "step": 640
    },
    {
      "epoch": 0.04003695719125346,
      "grad_norm": 1.048838496208191,
      "learning_rate": 0.00019733497587516683,
      "loss": 0.2377,
      "step": 650
    },
    {
      "epoch": 0.04065291037881121,
      "grad_norm": 0.6822594404220581,
      "learning_rate": 0.00019729391232932965,
      "loss": 0.2463,
      "step": 660
    },
    {
      "epoch": 0.041268863566368955,
      "grad_norm": 0.48566463589668274,
      "learning_rate": 0.00019725284878349247,
      "loss": 0.2347,
      "step": 670
    },
    {
      "epoch": 0.041884816753926704,
      "grad_norm": 0.4584192931652069,
      "learning_rate": 0.0001972117852376553,
      "loss": 0.2268,
      "step": 680
    },
    {
      "epoch": 0.04250076994148445,
      "grad_norm": 0.4825360178947449,
      "learning_rate": 0.0001971707216918181,
      "loss": 0.2239,
      "step": 690
    },
    {
      "epoch": 0.043116723129042196,
      "grad_norm": 0.5523925423622131,
      "learning_rate": 0.00019712965814598093,
      "loss": 0.2304,
      "step": 700
    },
    {
      "epoch": 0.04373267631659994,
      "grad_norm": 0.6728958487510681,
      "learning_rate": 0.00019708859460014372,
      "loss": 0.2266,
      "step": 710
    },
    {
      "epoch": 0.04434862950415768,
      "grad_norm": 0.4391653537750244,
      "learning_rate": 0.00019704753105430657,
      "loss": 0.2212,
      "step": 720
    },
    {
      "epoch": 0.04496458269171543,
      "grad_norm": 0.5685914158821106,
      "learning_rate": 0.00019700646750846936,
      "loss": 0.2202,
      "step": 730
    },
    {
      "epoch": 0.045580535879273174,
      "grad_norm": 0.4938400983810425,
      "learning_rate": 0.00019696540396263218,
      "loss": 0.2232,
      "step": 740
    },
    {
      "epoch": 0.04619648906683092,
      "grad_norm": 0.42418429255485535,
      "learning_rate": 0.000196924340416795,
      "loss": 0.2255,
      "step": 750
    },
    {
      "epoch": 0.046812442254388666,
      "grad_norm": 0.4838430881500244,
      "learning_rate": 0.00019688327687095782,
      "loss": 0.225,
      "step": 760
    },
    {
      "epoch": 0.047428395441946415,
      "grad_norm": 0.5119753479957581,
      "learning_rate": 0.00019684221332512064,
      "loss": 0.2268,
      "step": 770
    },
    {
      "epoch": 0.04804434862950416,
      "grad_norm": 0.5338042974472046,
      "learning_rate": 0.00019680114977928346,
      "loss": 0.229,
      "step": 780
    },
    {
      "epoch": 0.0486603018170619,
      "grad_norm": 0.5252170562744141,
      "learning_rate": 0.00019676008623344625,
      "loss": 0.2244,
      "step": 790
    },
    {
      "epoch": 0.04927625500461965,
      "grad_norm": 0.5897310972213745,
      "learning_rate": 0.0001967190226876091,
      "loss": 0.225,
      "step": 800
    },
    {
      "epoch": 0.04989220819217739,
      "grad_norm": 0.5149499177932739,
      "learning_rate": 0.0001966779591417719,
      "loss": 0.2199,
      "step": 810
    },
    {
      "epoch": 0.05050816137973514,
      "grad_norm": 0.5199355483055115,
      "learning_rate": 0.0001966368955959347,
      "loss": 0.2252,
      "step": 820
    },
    {
      "epoch": 0.051124114567292885,
      "grad_norm": 0.6549488306045532,
      "learning_rate": 0.00019659583205009753,
      "loss": 0.2164,
      "step": 830
    },
    {
      "epoch": 0.051740067754850634,
      "grad_norm": 0.39145633578300476,
      "learning_rate": 0.00019655476850426035,
      "loss": 0.2199,
      "step": 840
    },
    {
      "epoch": 0.05235602094240838,
      "grad_norm": 0.5612101554870605,
      "learning_rate": 0.00019651370495842317,
      "loss": 0.2219,
      "step": 850
    },
    {
      "epoch": 0.05297197412996612,
      "grad_norm": 0.5449150800704956,
      "learning_rate": 0.000196472641412586,
      "loss": 0.2263,
      "step": 860
    },
    {
      "epoch": 0.05358792731752387,
      "grad_norm": 0.5727001428604126,
      "learning_rate": 0.00019643157786674879,
      "loss": 0.2174,
      "step": 870
    },
    {
      "epoch": 0.05420388050508161,
      "grad_norm": 0.5006176233291626,
      "learning_rate": 0.00019639051432091163,
      "loss": 0.2271,
      "step": 880
    },
    {
      "epoch": 0.05481983369263936,
      "grad_norm": 0.5841690301895142,
      "learning_rate": 0.00019634945077507443,
      "loss": 0.2189,
      "step": 890
    },
    {
      "epoch": 0.055435786880197103,
      "grad_norm": 0.5184009075164795,
      "learning_rate": 0.00019630838722923725,
      "loss": 0.2201,
      "step": 900
    },
    {
      "epoch": 0.05605174006775485,
      "grad_norm": 0.507953941822052,
      "learning_rate": 0.00019626732368340007,
      "loss": 0.2164,
      "step": 910
    },
    {
      "epoch": 0.056667693255312596,
      "grad_norm": 0.6960206627845764,
      "learning_rate": 0.00019622626013756289,
      "loss": 0.2201,
      "step": 920
    },
    {
      "epoch": 0.057283646442870345,
      "grad_norm": 0.4962351620197296,
      "learning_rate": 0.0001961851965917257,
      "loss": 0.2107,
      "step": 930
    },
    {
      "epoch": 0.05789959963042809,
      "grad_norm": 0.5915812849998474,
      "learning_rate": 0.00019614413304588853,
      "loss": 0.2273,
      "step": 940
    },
    {
      "epoch": 0.05851555281798583,
      "grad_norm": 0.4819912016391754,
      "learning_rate": 0.00019610306950005132,
      "loss": 0.2179,
      "step": 950
    },
    {
      "epoch": 0.05913150600554358,
      "grad_norm": 0.4621017873287201,
      "learning_rate": 0.00019606200595421417,
      "loss": 0.2238,
      "step": 960
    },
    {
      "epoch": 0.05974745919310132,
      "grad_norm": 0.5599814057350159,
      "learning_rate": 0.00019602094240837696,
      "loss": 0.2276,
      "step": 970
    },
    {
      "epoch": 0.06036341238065907,
      "grad_norm": 0.41454583406448364,
      "learning_rate": 0.00019597987886253978,
      "loss": 0.2156,
      "step": 980
    },
    {
      "epoch": 0.060979365568216815,
      "grad_norm": 0.48895448446273804,
      "learning_rate": 0.00019593881531670263,
      "loss": 0.2127,
      "step": 990
    },
    {
      "epoch": 0.061595318755774564,
      "grad_norm": 0.45397892594337463,
      "learning_rate": 0.00019589775177086542,
      "loss": 0.2133,
      "step": 1000
    },
    {
      "epoch": 0.06221127194333231,
      "grad_norm": 0.47799330949783325,
      "learning_rate": 0.00019585668822502824,
      "loss": 0.2162,
      "step": 1010
    },
    {
      "epoch": 0.06282722513089005,
      "grad_norm": 0.43993115425109863,
      "learning_rate": 0.00019581562467919106,
      "loss": 0.2313,
      "step": 1020
    },
    {
      "epoch": 0.0634431783184478,
      "grad_norm": 0.5551782250404358,
      "learning_rate": 0.00019577456113335388,
      "loss": 0.2129,
      "step": 1030
    },
    {
      "epoch": 0.06405913150600555,
      "grad_norm": 0.5151287317276001,
      "learning_rate": 0.0001957334975875167,
      "loss": 0.2184,
      "step": 1040
    },
    {
      "epoch": 0.06467508469356328,
      "grad_norm": 0.601294755935669,
      "learning_rate": 0.00019569243404167952,
      "loss": 0.2091,
      "step": 1050
    },
    {
      "epoch": 0.06529103788112103,
      "grad_norm": 0.5456333756446838,
      "learning_rate": 0.0001956513704958423,
      "loss": 0.2132,
      "step": 1060
    },
    {
      "epoch": 0.06590699106867878,
      "grad_norm": 0.5314806699752808,
      "learning_rate": 0.00019561030695000516,
      "loss": 0.2164,
      "step": 1070
    },
    {
      "epoch": 0.06652294425623653,
      "grad_norm": 0.5823097825050354,
      "learning_rate": 0.00019556924340416795,
      "loss": 0.2165,
      "step": 1080
    },
    {
      "epoch": 0.06713889744379427,
      "grad_norm": 0.9363100528717041,
      "learning_rate": 0.00019552817985833077,
      "loss": 0.2291,
      "step": 1090
    },
    {
      "epoch": 0.06775485063135202,
      "grad_norm": 0.5847882032394409,
      "learning_rate": 0.0001954871163124936,
      "loss": 0.2137,
      "step": 1100
    },
    {
      "epoch": 0.06837080381890977,
      "grad_norm": 0.4867282807826996,
      "learning_rate": 0.0001954460527666564,
      "loss": 0.2172,
      "step": 1110
    },
    {
      "epoch": 0.0689867570064675,
      "grad_norm": 0.5010663866996765,
      "learning_rate": 0.00019540498922081923,
      "loss": 0.2155,
      "step": 1120
    },
    {
      "epoch": 0.06960271019402525,
      "grad_norm": 0.6361901760101318,
      "learning_rate": 0.00019536392567498205,
      "loss": 0.2186,
      "step": 1130
    },
    {
      "epoch": 0.070218663381583,
      "grad_norm": 0.48160940408706665,
      "learning_rate": 0.00019532286212914485,
      "loss": 0.2178,
      "step": 1140
    },
    {
      "epoch": 0.07083461656914075,
      "grad_norm": 0.6055055260658264,
      "learning_rate": 0.0001952817985833077,
      "loss": 0.2201,
      "step": 1150
    },
    {
      "epoch": 0.07145056975669849,
      "grad_norm": 0.4186427593231201,
      "learning_rate": 0.00019524073503747049,
      "loss": 0.2077,
      "step": 1160
    },
    {
      "epoch": 0.07206652294425624,
      "grad_norm": 0.48861753940582275,
      "learning_rate": 0.0001951996714916333,
      "loss": 0.2143,
      "step": 1170
    },
    {
      "epoch": 0.07268247613181399,
      "grad_norm": 0.4946463704109192,
      "learning_rate": 0.00019515860794579613,
      "loss": 0.2159,
      "step": 1180
    },
    {
      "epoch": 0.07329842931937172,
      "grad_norm": 0.5217118263244629,
      "learning_rate": 0.00019511754439995895,
      "loss": 0.2091,
      "step": 1190
    },
    {
      "epoch": 0.07391438250692947,
      "grad_norm": 0.6605638265609741,
      "learning_rate": 0.00019507648085412177,
      "loss": 0.2204,
      "step": 1200
    },
    {
      "epoch": 0.07453033569448722,
      "grad_norm": 0.4790785610675812,
      "learning_rate": 0.00019503541730828459,
      "loss": 0.2166,
      "step": 1210
    },
    {
      "epoch": 0.07514628888204497,
      "grad_norm": 0.5027427673339844,
      "learning_rate": 0.00019499435376244738,
      "loss": 0.213,
      "step": 1220
    },
    {
      "epoch": 0.0757622420696027,
      "grad_norm": 0.5578036308288574,
      "learning_rate": 0.00019495329021661023,
      "loss": 0.2172,
      "step": 1230
    },
    {
      "epoch": 0.07637819525716046,
      "grad_norm": 0.43335410952568054,
      "learning_rate": 0.00019491222667077302,
      "loss": 0.2089,
      "step": 1240
    },
    {
      "epoch": 0.0769941484447182,
      "grad_norm": 0.5026850700378418,
      "learning_rate": 0.00019487116312493584,
      "loss": 0.2075,
      "step": 1250
    },
    {
      "epoch": 0.07761010163227594,
      "grad_norm": 0.7910992503166199,
      "learning_rate": 0.00019483009957909866,
      "loss": 0.2216,
      "step": 1260
    },
    {
      "epoch": 0.07822605481983369,
      "grad_norm": 0.32583048939704895,
      "learning_rate": 0.00019478903603326148,
      "loss": 0.202,
      "step": 1270
    },
    {
      "epoch": 0.07884200800739144,
      "grad_norm": 0.34277623891830444,
      "learning_rate": 0.0001947479724874243,
      "loss": 0.2072,
      "step": 1280
    },
    {
      "epoch": 0.07945796119494919,
      "grad_norm": 1.0406414270401,
      "learning_rate": 0.00019470690894158712,
      "loss": 0.2161,
      "step": 1290
    },
    {
      "epoch": 0.08007391438250692,
      "grad_norm": 0.4362963140010834,
      "learning_rate": 0.0001946658453957499,
      "loss": 0.2119,
      "step": 1300
    },
    {
      "epoch": 0.08068986757006467,
      "grad_norm": 0.9396405816078186,
      "learning_rate": 0.00019462478184991276,
      "loss": 0.2158,
      "step": 1310
    },
    {
      "epoch": 0.08130582075762242,
      "grad_norm": 0.48845618963241577,
      "learning_rate": 0.00019458371830407555,
      "loss": 0.2117,
      "step": 1320
    },
    {
      "epoch": 0.08192177394518016,
      "grad_norm": 0.47800371050834656,
      "learning_rate": 0.00019454265475823837,
      "loss": 0.2077,
      "step": 1330
    },
    {
      "epoch": 0.08253772713273791,
      "grad_norm": 0.4794730246067047,
      "learning_rate": 0.0001945015912124012,
      "loss": 0.2086,
      "step": 1340
    },
    {
      "epoch": 0.08315368032029566,
      "grad_norm": 0.6218608021736145,
      "learning_rate": 0.000194460527666564,
      "loss": 0.2175,
      "step": 1350
    },
    {
      "epoch": 0.08376963350785341,
      "grad_norm": 0.4757714867591858,
      "learning_rate": 0.00019441946412072683,
      "loss": 0.2227,
      "step": 1360
    },
    {
      "epoch": 0.08438558669541114,
      "grad_norm": 0.5956975817680359,
      "learning_rate": 0.00019437840057488965,
      "loss": 0.2125,
      "step": 1370
    },
    {
      "epoch": 0.0850015398829689,
      "grad_norm": 0.5204429030418396,
      "learning_rate": 0.00019433733702905245,
      "loss": 0.2153,
      "step": 1380
    },
    {
      "epoch": 0.08561749307052664,
      "grad_norm": 0.5345360040664673,
      "learning_rate": 0.0001942962734832153,
      "loss": 0.214,
      "step": 1390
    },
    {
      "epoch": 0.08623344625808439,
      "grad_norm": 0.47319722175598145,
      "learning_rate": 0.00019425520993737809,
      "loss": 0.2079,
      "step": 1400
    },
    {
      "epoch": 0.08684939944564213,
      "grad_norm": 0.41318222880363464,
      "learning_rate": 0.0001942141463915409,
      "loss": 0.2128,
      "step": 1410
    },
    {
      "epoch": 0.08746535263319988,
      "grad_norm": 0.3291952610015869,
      "learning_rate": 0.00019417308284570375,
      "loss": 0.2065,
      "step": 1420
    },
    {
      "epoch": 0.08808130582075763,
      "grad_norm": 0.49312955141067505,
      "learning_rate": 0.00019413201929986655,
      "loss": 0.209,
      "step": 1430
    },
    {
      "epoch": 0.08869725900831536,
      "grad_norm": 0.49743303656578064,
      "learning_rate": 0.0001940909557540294,
      "loss": 0.2136,
      "step": 1440
    },
    {
      "epoch": 0.08931321219587311,
      "grad_norm": 0.376360148191452,
      "learning_rate": 0.00019404989220819219,
      "loss": 0.2113,
      "step": 1450
    },
    {
      "epoch": 0.08992916538343086,
      "grad_norm": 0.4456486999988556,
      "learning_rate": 0.000194008828662355,
      "loss": 0.2097,
      "step": 1460
    },
    {
      "epoch": 0.09054511857098861,
      "grad_norm": 0.49643874168395996,
      "learning_rate": 0.00019396776511651783,
      "loss": 0.2071,
      "step": 1470
    },
    {
      "epoch": 0.09116107175854635,
      "grad_norm": 0.8686973452568054,
      "learning_rate": 0.00019392670157068065,
      "loss": 0.221,
      "step": 1480
    },
    {
      "epoch": 0.0917770249461041,
      "grad_norm": 0.5029222369194031,
      "learning_rate": 0.00019388563802484344,
      "loss": 0.2105,
      "step": 1490
    },
    {
      "epoch": 0.09239297813366185,
      "grad_norm": 0.4396584630012512,
      "learning_rate": 0.00019384457447900629,
      "loss": 0.2111,
      "step": 1500
    },
    {
      "epoch": 0.09300893132121958,
      "grad_norm": 0.5490801930427551,
      "learning_rate": 0.00019380351093316908,
      "loss": 0.2189,
      "step": 1510
    },
    {
      "epoch": 0.09362488450877733,
      "grad_norm": 0.7902935743331909,
      "learning_rate": 0.00019376244738733193,
      "loss": 0.2082,
      "step": 1520
    },
    {
      "epoch": 0.09424083769633508,
      "grad_norm": 0.5639961361885071,
      "learning_rate": 0.00019372138384149472,
      "loss": 0.2108,
      "step": 1530
    },
    {
      "epoch": 0.09485679088389283,
      "grad_norm": 0.42792007327079773,
      "learning_rate": 0.00019368032029565754,
      "loss": 0.219,
      "step": 1540
    },
    {
      "epoch": 0.09547274407145057,
      "grad_norm": 0.3990280330181122,
      "learning_rate": 0.00019363925674982036,
      "loss": 0.2056,
      "step": 1550
    },
    {
      "epoch": 0.09608869725900832,
      "grad_norm": 0.4216531217098236,
      "learning_rate": 0.00019359819320398318,
      "loss": 0.2032,
      "step": 1560
    },
    {
      "epoch": 0.09670465044656607,
      "grad_norm": 0.5653061866760254,
      "learning_rate": 0.000193557129658146,
      "loss": 0.2039,
      "step": 1570
    },
    {
      "epoch": 0.0973206036341238,
      "grad_norm": 0.5493690371513367,
      "learning_rate": 0.00019351606611230882,
      "loss": 0.2098,
      "step": 1580
    },
    {
      "epoch": 0.09793655682168155,
      "grad_norm": 0.5033530592918396,
      "learning_rate": 0.0001934750025664716,
      "loss": 0.2065,
      "step": 1590
    },
    {
      "epoch": 0.0985525100092393,
      "grad_norm": 0.6790467500686646,
      "learning_rate": 0.00019343393902063446,
      "loss": 0.2104,
      "step": 1600
    },
    {
      "epoch": 0.09916846319679705,
      "grad_norm": 0.34905195236206055,
      "learning_rate": 0.00019339287547479725,
      "loss": 0.2029,
      "step": 1610
    },
    {
      "epoch": 0.09978441638435478,
      "grad_norm": 0.42305663228034973,
      "learning_rate": 0.00019335181192896007,
      "loss": 0.2159,
      "step": 1620
    },
    {
      "epoch": 0.10040036957191253,
      "grad_norm": 0.42949622869491577,
      "learning_rate": 0.0001933107483831229,
      "loss": 0.2002,
      "step": 1630
    },
    {
      "epoch": 0.10101632275947028,
      "grad_norm": 0.566960871219635,
      "learning_rate": 0.0001932696848372857,
      "loss": 0.2019,
      "step": 1640
    },
    {
      "epoch": 0.10163227594702802,
      "grad_norm": 0.5786846876144409,
      "learning_rate": 0.00019322862129144853,
      "loss": 0.207,
      "step": 1650
    },
    {
      "epoch": 0.10224822913458577,
      "grad_norm": 0.4344591200351715,
      "learning_rate": 0.00019318755774561135,
      "loss": 0.2047,
      "step": 1660
    },
    {
      "epoch": 0.10286418232214352,
      "grad_norm": 0.38502421975135803,
      "learning_rate": 0.00019314649419977415,
      "loss": 0.2119,
      "step": 1670
    },
    {
      "epoch": 0.10348013550970127,
      "grad_norm": 0.38945138454437256,
      "learning_rate": 0.000193105430653937,
      "loss": 0.2062,
      "step": 1680
    },
    {
      "epoch": 0.104096088697259,
      "grad_norm": 0.5425887703895569,
      "learning_rate": 0.00019306436710809979,
      "loss": 0.2043,
      "step": 1690
    },
    {
      "epoch": 0.10471204188481675,
      "grad_norm": 0.6451876163482666,
      "learning_rate": 0.0001930233035622626,
      "loss": 0.1991,
      "step": 1700
    },
    {
      "epoch": 0.1053279950723745,
      "grad_norm": 0.37144771218299866,
      "learning_rate": 0.00019298224001642543,
      "loss": 0.1954,
      "step": 1710
    },
    {
      "epoch": 0.10594394825993224,
      "grad_norm": 0.4078567624092102,
      "learning_rate": 0.00019294117647058825,
      "loss": 0.2064,
      "step": 1720
    },
    {
      "epoch": 0.10655990144748999,
      "grad_norm": 0.4609326124191284,
      "learning_rate": 0.00019290011292475107,
      "loss": 0.2051,
      "step": 1730
    },
    {
      "epoch": 0.10717585463504774,
      "grad_norm": 0.4681556522846222,
      "learning_rate": 0.00019285904937891389,
      "loss": 0.2079,
      "step": 1740
    },
    {
      "epoch": 0.10779180782260549,
      "grad_norm": 0.42749401926994324,
      "learning_rate": 0.00019281798583307668,
      "loss": 0.2087,
      "step": 1750
    },
    {
      "epoch": 0.10840776101016322,
      "grad_norm": 0.40938907861709595,
      "learning_rate": 0.00019277692228723953,
      "loss": 0.2039,
      "step": 1760
    },
    {
      "epoch": 0.10902371419772097,
      "grad_norm": 1.7997773885726929,
      "learning_rate": 0.00019273585874140232,
      "loss": 0.1975,
      "step": 1770
    },
    {
      "epoch": 0.10963966738527872,
      "grad_norm": 0.6724539399147034,
      "learning_rate": 0.00019269479519556514,
      "loss": 0.2019,
      "step": 1780
    },
    {
      "epoch": 0.11025562057283647,
      "grad_norm": 0.516549289226532,
      "learning_rate": 0.00019265373164972796,
      "loss": 0.2063,
      "step": 1790
    },
    {
      "epoch": 0.11087157376039421,
      "grad_norm": 0.4196205139160156,
      "learning_rate": 0.00019261266810389078,
      "loss": 0.2089,
      "step": 1800
    },
    {
      "epoch": 0.11148752694795196,
      "grad_norm": 0.34336569905281067,
      "learning_rate": 0.0001925716045580536,
      "loss": 0.2017,
      "step": 1810
    },
    {
      "epoch": 0.1121034801355097,
      "grad_norm": 0.3771158754825592,
      "learning_rate": 0.00019253054101221642,
      "loss": 0.2078,
      "step": 1820
    },
    {
      "epoch": 0.11271943332306744,
      "grad_norm": 0.5411345958709717,
      "learning_rate": 0.0001924894774663792,
      "loss": 0.2077,
      "step": 1830
    },
    {
      "epoch": 0.11333538651062519,
      "grad_norm": 0.4404023289680481,
      "learning_rate": 0.00019244841392054206,
      "loss": 0.2053,
      "step": 1840
    },
    {
      "epoch": 0.11395133969818294,
      "grad_norm": 0.45837873220443726,
      "learning_rate": 0.00019240735037470488,
      "loss": 0.2064,
      "step": 1850
    },
    {
      "epoch": 0.11456729288574069,
      "grad_norm": 0.4248811602592468,
      "learning_rate": 0.00019236628682886767,
      "loss": 0.2016,
      "step": 1860
    },
    {
      "epoch": 0.11518324607329843,
      "grad_norm": 0.6566168665885925,
      "learning_rate": 0.00019232522328303052,
      "loss": 0.2054,
      "step": 1870
    },
    {
      "epoch": 0.11579919926085618,
      "grad_norm": 0.42913618683815,
      "learning_rate": 0.0001922841597371933,
      "loss": 0.2008,
      "step": 1880
    },
    {
      "epoch": 0.11641515244841392,
      "grad_norm": 0.30718371272087097,
      "learning_rate": 0.00019224309619135613,
      "loss": 0.2026,
      "step": 1890
    },
    {
      "epoch": 0.11703110563597166,
      "grad_norm": 0.4635305404663086,
      "learning_rate": 0.00019220203264551895,
      "loss": 0.1951,
      "step": 1900
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 0.49054792523384094,
      "learning_rate": 0.00019216096909968177,
      "loss": 0.2023,
      "step": 1910
    },
    {
      "epoch": 0.11826301201108716,
      "grad_norm": 0.7529637813568115,
      "learning_rate": 0.0001921199055538446,
      "loss": 0.2026,
      "step": 1920
    },
    {
      "epoch": 0.11887896519864491,
      "grad_norm": 0.436138778924942,
      "learning_rate": 0.0001920788420080074,
      "loss": 0.2015,
      "step": 1930
    },
    {
      "epoch": 0.11949491838620264,
      "grad_norm": 0.43348902463912964,
      "learning_rate": 0.0001920377784621702,
      "loss": 0.1991,
      "step": 1940
    },
    {
      "epoch": 0.1201108715737604,
      "grad_norm": 0.5282987356185913,
      "learning_rate": 0.00019199671491633305,
      "loss": 0.2068,
      "step": 1950
    },
    {
      "epoch": 0.12072682476131814,
      "grad_norm": 0.36606720089912415,
      "learning_rate": 0.00019195565137049585,
      "loss": 0.2019,
      "step": 1960
    },
    {
      "epoch": 0.12134277794887588,
      "grad_norm": 0.3836289346218109,
      "learning_rate": 0.00019191458782465867,
      "loss": 0.1997,
      "step": 1970
    },
    {
      "epoch": 0.12195873113643363,
      "grad_norm": 0.38354039192199707,
      "learning_rate": 0.00019187352427882149,
      "loss": 0.1973,
      "step": 1980
    },
    {
      "epoch": 0.12257468432399138,
      "grad_norm": 0.4030212461948395,
      "learning_rate": 0.0001918324607329843,
      "loss": 0.1965,
      "step": 1990
    },
    {
      "epoch": 0.12319063751154913,
      "grad_norm": 0.47417840361595154,
      "learning_rate": 0.00019179139718714713,
      "loss": 0.1964,
      "step": 2000
    },
    {
      "epoch": 0.12380659069910686,
      "grad_norm": 0.46421322226524353,
      "learning_rate": 0.00019175033364130995,
      "loss": 0.2047,
      "step": 2010
    },
    {
      "epoch": 0.12442254388666461,
      "grad_norm": 0.39598187804222107,
      "learning_rate": 0.00019170927009547274,
      "loss": 0.1997,
      "step": 2020
    },
    {
      "epoch": 0.12503849707422235,
      "grad_norm": 0.41791272163391113,
      "learning_rate": 0.00019166820654963559,
      "loss": 0.2024,
      "step": 2030
    },
    {
      "epoch": 0.1256544502617801,
      "grad_norm": 0.4543781280517578,
      "learning_rate": 0.00019162714300379838,
      "loss": 0.1911,
      "step": 2040
    },
    {
      "epoch": 0.12627040344933785,
      "grad_norm": 0.7354587316513062,
      "learning_rate": 0.0001915860794579612,
      "loss": 0.2021,
      "step": 2050
    },
    {
      "epoch": 0.1268863566368956,
      "grad_norm": 0.4261068105697632,
      "learning_rate": 0.00019154501591212402,
      "loss": 0.2034,
      "step": 2060
    },
    {
      "epoch": 0.12750230982445335,
      "grad_norm": 0.2925398051738739,
      "learning_rate": 0.00019150395236628684,
      "loss": 0.1947,
      "step": 2070
    },
    {
      "epoch": 0.1281182630120111,
      "grad_norm": 0.3603372275829315,
      "learning_rate": 0.00019146288882044966,
      "loss": 0.208,
      "step": 2080
    },
    {
      "epoch": 0.12873421619956885,
      "grad_norm": 0.41374924778938293,
      "learning_rate": 0.00019142182527461248,
      "loss": 0.1971,
      "step": 2090
    },
    {
      "epoch": 0.12935016938712657,
      "grad_norm": 0.5116744637489319,
      "learning_rate": 0.00019138076172877527,
      "loss": 0.2,
      "step": 2100
    },
    {
      "epoch": 0.12996612257468432,
      "grad_norm": 0.36189889907836914,
      "learning_rate": 0.00019133969818293812,
      "loss": 0.1966,
      "step": 2110
    },
    {
      "epoch": 0.13058207576224207,
      "grad_norm": 0.5679619908332825,
      "learning_rate": 0.0001912986346371009,
      "loss": 0.2031,
      "step": 2120
    },
    {
      "epoch": 0.13119802894979982,
      "grad_norm": 0.3351510167121887,
      "learning_rate": 0.00019125757109126373,
      "loss": 0.1967,
      "step": 2130
    },
    {
      "epoch": 0.13181398213735757,
      "grad_norm": 0.5235759019851685,
      "learning_rate": 0.00019121650754542655,
      "loss": 0.2001,
      "step": 2140
    },
    {
      "epoch": 0.13242993532491532,
      "grad_norm": 0.39504504203796387,
      "learning_rate": 0.00019117544399958937,
      "loss": 0.194,
      "step": 2150
    },
    {
      "epoch": 0.13304588851247307,
      "grad_norm": 0.5959985852241516,
      "learning_rate": 0.0001911343804537522,
      "loss": 0.1983,
      "step": 2160
    },
    {
      "epoch": 0.1336618417000308,
      "grad_norm": 0.37686750292778015,
      "learning_rate": 0.000191093316907915,
      "loss": 0.1994,
      "step": 2170
    },
    {
      "epoch": 0.13427779488758854,
      "grad_norm": 0.6210644841194153,
      "learning_rate": 0.0001910522533620778,
      "loss": 0.1939,
      "step": 2180
    },
    {
      "epoch": 0.13489374807514629,
      "grad_norm": 0.676125705242157,
      "learning_rate": 0.00019101118981624065,
      "loss": 0.2036,
      "step": 2190
    },
    {
      "epoch": 0.13550970126270404,
      "grad_norm": 0.42718565464019775,
      "learning_rate": 0.00019097012627040344,
      "loss": 0.198,
      "step": 2200
    },
    {
      "epoch": 0.13612565445026178,
      "grad_norm": 0.33829012513160706,
      "learning_rate": 0.00019092906272456626,
      "loss": 0.2023,
      "step": 2210
    },
    {
      "epoch": 0.13674160763781953,
      "grad_norm": 0.3953021168708801,
      "learning_rate": 0.00019088799917872908,
      "loss": 0.2,
      "step": 2220
    },
    {
      "epoch": 0.13735756082537728,
      "grad_norm": 0.4504813253879547,
      "learning_rate": 0.0001908469356328919,
      "loss": 0.1971,
      "step": 2230
    },
    {
      "epoch": 0.137973514012935,
      "grad_norm": 0.4958283305168152,
      "learning_rate": 0.00019080587208705472,
      "loss": 0.1965,
      "step": 2240
    },
    {
      "epoch": 0.13858946720049276,
      "grad_norm": 0.48288342356681824,
      "learning_rate": 0.00019076480854121754,
      "loss": 0.1998,
      "step": 2250
    },
    {
      "epoch": 0.1392054203880505,
      "grad_norm": 0.39929625391960144,
      "learning_rate": 0.00019072374499538036,
      "loss": 0.1968,
      "step": 2260
    },
    {
      "epoch": 0.13982137357560825,
      "grad_norm": 0.5083920359611511,
      "learning_rate": 0.00019068268144954318,
      "loss": 0.1964,
      "step": 2270
    },
    {
      "epoch": 0.140437326763166,
      "grad_norm": 0.4434220790863037,
      "learning_rate": 0.000190641617903706,
      "loss": 0.1975,
      "step": 2280
    },
    {
      "epoch": 0.14105327995072375,
      "grad_norm": 0.42778828740119934,
      "learning_rate": 0.0001906005543578688,
      "loss": 0.2069,
      "step": 2290
    },
    {
      "epoch": 0.1416692331382815,
      "grad_norm": 0.4955173134803772,
      "learning_rate": 0.00019055949081203165,
      "loss": 0.1955,
      "step": 2300
    },
    {
      "epoch": 0.14228518632583922,
      "grad_norm": 0.4437647759914398,
      "learning_rate": 0.00019051842726619444,
      "loss": 0.1951,
      "step": 2310
    },
    {
      "epoch": 0.14290113951339697,
      "grad_norm": 0.4044667184352875,
      "learning_rate": 0.00019047736372035726,
      "loss": 0.1988,
      "step": 2320
    },
    {
      "epoch": 0.14351709270095472,
      "grad_norm": 0.5859926342964172,
      "learning_rate": 0.00019043630017452008,
      "loss": 0.1967,
      "step": 2330
    },
    {
      "epoch": 0.14413304588851247,
      "grad_norm": 0.43428876996040344,
      "learning_rate": 0.0001903952366286829,
      "loss": 0.1989,
      "step": 2340
    },
    {
      "epoch": 0.14474899907607022,
      "grad_norm": 0.5618798732757568,
      "learning_rate": 0.00019035417308284572,
      "loss": 0.1977,
      "step": 2350
    },
    {
      "epoch": 0.14536495226362797,
      "grad_norm": 0.6605483293533325,
      "learning_rate": 0.00019031310953700854,
      "loss": 0.195,
      "step": 2360
    },
    {
      "epoch": 0.14598090545118572,
      "grad_norm": 0.6407440304756165,
      "learning_rate": 0.00019027204599117133,
      "loss": 0.1968,
      "step": 2370
    },
    {
      "epoch": 0.14659685863874344,
      "grad_norm": 0.4719698131084442,
      "learning_rate": 0.00019023098244533418,
      "loss": 0.1966,
      "step": 2380
    },
    {
      "epoch": 0.1472128118263012,
      "grad_norm": 0.42540669441223145,
      "learning_rate": 0.00019018991889949697,
      "loss": 0.2006,
      "step": 2390
    },
    {
      "epoch": 0.14782876501385894,
      "grad_norm": 0.426364928483963,
      "learning_rate": 0.00019014885535365982,
      "loss": 0.198,
      "step": 2400
    },
    {
      "epoch": 0.1484447182014167,
      "grad_norm": 0.6722053289413452,
      "learning_rate": 0.0001901077918078226,
      "loss": 0.1909,
      "step": 2410
    },
    {
      "epoch": 0.14906067138897444,
      "grad_norm": 0.33872929215431213,
      "learning_rate": 0.00019006672826198543,
      "loss": 0.1901,
      "step": 2420
    },
    {
      "epoch": 0.1496766245765322,
      "grad_norm": 0.38129934668540955,
      "learning_rate": 0.00019002566471614825,
      "loss": 0.1986,
      "step": 2430
    },
    {
      "epoch": 0.15029257776408994,
      "grad_norm": 0.44356051087379456,
      "learning_rate": 0.00018998460117031107,
      "loss": 0.1958,
      "step": 2440
    },
    {
      "epoch": 0.15090853095164766,
      "grad_norm": 0.35587191581726074,
      "learning_rate": 0.00018994353762447386,
      "loss": 0.1998,
      "step": 2450
    },
    {
      "epoch": 0.1515244841392054,
      "grad_norm": 0.44142502546310425,
      "learning_rate": 0.0001899024740786367,
      "loss": 0.1981,
      "step": 2460
    },
    {
      "epoch": 0.15214043732676316,
      "grad_norm": 0.3672106862068176,
      "learning_rate": 0.0001898614105327995,
      "loss": 0.2018,
      "step": 2470
    },
    {
      "epoch": 0.1527563905143209,
      "grad_norm": 0.3897787630558014,
      "learning_rate": 0.00018982034698696235,
      "loss": 0.1973,
      "step": 2480
    },
    {
      "epoch": 0.15337234370187866,
      "grad_norm": 0.3585745096206665,
      "learning_rate": 0.00018977928344112514,
      "loss": 0.1997,
      "step": 2490
    },
    {
      "epoch": 0.1539882968894364,
      "grad_norm": 0.40344154834747314,
      "learning_rate": 0.00018973821989528796,
      "loss": 0.1962,
      "step": 2500
    },
    {
      "epoch": 0.15460425007699416,
      "grad_norm": 0.4090549647808075,
      "learning_rate": 0.00018969715634945078,
      "loss": 0.1929,
      "step": 2510
    },
    {
      "epoch": 0.15522020326455188,
      "grad_norm": 0.49843883514404297,
      "learning_rate": 0.0001896560928036136,
      "loss": 0.2013,
      "step": 2520
    },
    {
      "epoch": 0.15583615645210963,
      "grad_norm": 0.44797849655151367,
      "learning_rate": 0.0001896150292577764,
      "loss": 0.1933,
      "step": 2530
    },
    {
      "epoch": 0.15645210963966738,
      "grad_norm": 0.3963472247123718,
      "learning_rate": 0.00018957396571193924,
      "loss": 0.196,
      "step": 2540
    },
    {
      "epoch": 0.15706806282722513,
      "grad_norm": 0.34064432978630066,
      "learning_rate": 0.00018953290216610204,
      "loss": 0.1935,
      "step": 2550
    },
    {
      "epoch": 0.15768401601478288,
      "grad_norm": 0.3805800676345825,
      "learning_rate": 0.00018949183862026488,
      "loss": 0.2017,
      "step": 2560
    },
    {
      "epoch": 0.15829996920234063,
      "grad_norm": 0.3801049292087555,
      "learning_rate": 0.00018945077507442768,
      "loss": 0.1912,
      "step": 2570
    },
    {
      "epoch": 0.15891592238989838,
      "grad_norm": 0.47283658385276794,
      "learning_rate": 0.0001894097115285905,
      "loss": 0.1982,
      "step": 2580
    },
    {
      "epoch": 0.1595318755774561,
      "grad_norm": 0.48127469420433044,
      "learning_rate": 0.00018936864798275332,
      "loss": 0.195,
      "step": 2590
    },
    {
      "epoch": 0.16014782876501385,
      "grad_norm": 0.38765963912010193,
      "learning_rate": 0.00018932758443691614,
      "loss": 0.1952,
      "step": 2600
    },
    {
      "epoch": 0.1607637819525716,
      "grad_norm": 0.3678646981716156,
      "learning_rate": 0.00018928652089107896,
      "loss": 0.199,
      "step": 2610
    },
    {
      "epoch": 0.16137973514012935,
      "grad_norm": 0.3606019914150238,
      "learning_rate": 0.00018924545734524178,
      "loss": 0.1983,
      "step": 2620
    },
    {
      "epoch": 0.1619956883276871,
      "grad_norm": 0.6971378922462463,
      "learning_rate": 0.00018920439379940457,
      "loss": 0.2017,
      "step": 2630
    },
    {
      "epoch": 0.16261164151524485,
      "grad_norm": 0.4932501018047333,
      "learning_rate": 0.00018916333025356742,
      "loss": 0.1993,
      "step": 2640
    },
    {
      "epoch": 0.1632275947028026,
      "grad_norm": 0.5938268899917603,
      "learning_rate": 0.0001891222667077302,
      "loss": 0.2039,
      "step": 2650
    },
    {
      "epoch": 0.16384354789036032,
      "grad_norm": 0.5888773202896118,
      "learning_rate": 0.00018908120316189303,
      "loss": 0.1996,
      "step": 2660
    },
    {
      "epoch": 0.16445950107791807,
      "grad_norm": 0.6258536577224731,
      "learning_rate": 0.00018904013961605585,
      "loss": 0.1973,
      "step": 2670
    },
    {
      "epoch": 0.16507545426547582,
      "grad_norm": 0.3649784028530121,
      "learning_rate": 0.00018899907607021867,
      "loss": 0.1989,
      "step": 2680
    },
    {
      "epoch": 0.16569140745303357,
      "grad_norm": 0.3305732011795044,
      "learning_rate": 0.0001889580125243815,
      "loss": 0.1969,
      "step": 2690
    },
    {
      "epoch": 0.16630736064059132,
      "grad_norm": 0.4506874680519104,
      "learning_rate": 0.0001889169489785443,
      "loss": 0.2027,
      "step": 2700
    },
    {
      "epoch": 0.16692331382814907,
      "grad_norm": 0.46704545617103577,
      "learning_rate": 0.00018887588543270713,
      "loss": 0.1991,
      "step": 2710
    },
    {
      "epoch": 0.16753926701570682,
      "grad_norm": 0.43654951453208923,
      "learning_rate": 0.00018883482188686995,
      "loss": 0.1979,
      "step": 2720
    },
    {
      "epoch": 0.16815522020326454,
      "grad_norm": 0.352787584066391,
      "learning_rate": 0.00018879375834103277,
      "loss": 0.1968,
      "step": 2730
    },
    {
      "epoch": 0.1687711733908223,
      "grad_norm": 0.41496068239212036,
      "learning_rate": 0.00018875269479519556,
      "loss": 0.1919,
      "step": 2740
    },
    {
      "epoch": 0.16938712657838004,
      "grad_norm": 0.37896209955215454,
      "learning_rate": 0.0001887116312493584,
      "loss": 0.1969,
      "step": 2750
    },
    {
      "epoch": 0.1700030797659378,
      "grad_norm": 0.48593634366989136,
      "learning_rate": 0.0001886705677035212,
      "loss": 0.1964,
      "step": 2760
    },
    {
      "epoch": 0.17061903295349554,
      "grad_norm": 0.3671749532222748,
      "learning_rate": 0.00018862950415768402,
      "loss": 0.2012,
      "step": 2770
    },
    {
      "epoch": 0.17123498614105329,
      "grad_norm": 0.3514409065246582,
      "learning_rate": 0.00018858844061184684,
      "loss": 0.1946,
      "step": 2780
    },
    {
      "epoch": 0.17185093932861104,
      "grad_norm": 0.36098483204841614,
      "learning_rate": 0.00018854737706600966,
      "loss": 0.1971,
      "step": 2790
    },
    {
      "epoch": 0.17246689251616878,
      "grad_norm": 0.7065672278404236,
      "learning_rate": 0.00018850631352017248,
      "loss": 0.1988,
      "step": 2800
    },
    {
      "epoch": 0.1730828457037265,
      "grad_norm": 0.47268399596214294,
      "learning_rate": 0.0001884652499743353,
      "loss": 0.1938,
      "step": 2810
    },
    {
      "epoch": 0.17369879889128426,
      "grad_norm": 0.4131484031677246,
      "learning_rate": 0.0001884241864284981,
      "loss": 0.1931,
      "step": 2820
    },
    {
      "epoch": 0.174314752078842,
      "grad_norm": 0.3640715479850769,
      "learning_rate": 0.00018838312288266094,
      "loss": 0.1915,
      "step": 2830
    },
    {
      "epoch": 0.17493070526639976,
      "grad_norm": 0.6641087532043457,
      "learning_rate": 0.00018834205933682374,
      "loss": 0.1934,
      "step": 2840
    },
    {
      "epoch": 0.1755466584539575,
      "grad_norm": 0.3437945246696472,
      "learning_rate": 0.00018830099579098656,
      "loss": 0.1939,
      "step": 2850
    },
    {
      "epoch": 0.17616261164151525,
      "grad_norm": 0.3268985450267792,
      "learning_rate": 0.00018825993224514938,
      "loss": 0.1943,
      "step": 2860
    },
    {
      "epoch": 0.176778564829073,
      "grad_norm": 0.3203287720680237,
      "learning_rate": 0.0001882188686993122,
      "loss": 0.193,
      "step": 2870
    },
    {
      "epoch": 0.17739451801663073,
      "grad_norm": 0.34243738651275635,
      "learning_rate": 0.00018817780515347502,
      "loss": 0.192,
      "step": 2880
    },
    {
      "epoch": 0.17801047120418848,
      "grad_norm": 0.5685077905654907,
      "learning_rate": 0.00018813674160763784,
      "loss": 0.1988,
      "step": 2890
    },
    {
      "epoch": 0.17862642439174622,
      "grad_norm": 0.4883541166782379,
      "learning_rate": 0.00018809567806180063,
      "loss": 0.1996,
      "step": 2900
    },
    {
      "epoch": 0.17924237757930397,
      "grad_norm": 0.593941867351532,
      "learning_rate": 0.00018805461451596348,
      "loss": 0.1973,
      "step": 2910
    },
    {
      "epoch": 0.17985833076686172,
      "grad_norm": 0.43752938508987427,
      "learning_rate": 0.00018801355097012627,
      "loss": 0.1921,
      "step": 2920
    },
    {
      "epoch": 0.18047428395441947,
      "grad_norm": 0.7086010575294495,
      "learning_rate": 0.0001879724874242891,
      "loss": 0.1974,
      "step": 2930
    },
    {
      "epoch": 0.18109023714197722,
      "grad_norm": 0.3582015633583069,
      "learning_rate": 0.0001879314238784519,
      "loss": 0.1947,
      "step": 2940
    },
    {
      "epoch": 0.18170619032953494,
      "grad_norm": 0.39776068925857544,
      "learning_rate": 0.00018789036033261473,
      "loss": 0.1963,
      "step": 2950
    },
    {
      "epoch": 0.1823221435170927,
      "grad_norm": 0.2991332411766052,
      "learning_rate": 0.00018784929678677755,
      "loss": 0.1907,
      "step": 2960
    },
    {
      "epoch": 0.18293809670465044,
      "grad_norm": 0.36151573061943054,
      "learning_rate": 0.00018780823324094037,
      "loss": 0.1972,
      "step": 2970
    },
    {
      "epoch": 0.1835540498922082,
      "grad_norm": 0.3872625529766083,
      "learning_rate": 0.00018776716969510316,
      "loss": 0.1975,
      "step": 2980
    },
    {
      "epoch": 0.18417000307976594,
      "grad_norm": 0.36896219849586487,
      "learning_rate": 0.000187726106149266,
      "loss": 0.1916,
      "step": 2990
    },
    {
      "epoch": 0.1847859562673237,
      "grad_norm": 0.41280996799468994,
      "learning_rate": 0.0001876850426034288,
      "loss": 0.1942,
      "step": 3000
    },
    {
      "epoch": 0.18540190945488144,
      "grad_norm": 0.4357489347457886,
      "learning_rate": 0.00018764397905759162,
      "loss": 0.1945,
      "step": 3010
    },
    {
      "epoch": 0.18601786264243916,
      "grad_norm": 0.5056526064872742,
      "learning_rate": 0.00018760291551175444,
      "loss": 0.1962,
      "step": 3020
    },
    {
      "epoch": 0.1866338158299969,
      "grad_norm": 0.47692105174064636,
      "learning_rate": 0.00018756185196591726,
      "loss": 0.193,
      "step": 3030
    },
    {
      "epoch": 0.18724976901755466,
      "grad_norm": 0.3190147280693054,
      "learning_rate": 0.00018752078842008008,
      "loss": 0.1962,
      "step": 3040
    },
    {
      "epoch": 0.1878657222051124,
      "grad_norm": 0.372543603181839,
      "learning_rate": 0.0001874797248742429,
      "loss": 0.1915,
      "step": 3050
    },
    {
      "epoch": 0.18848167539267016,
      "grad_norm": 0.319395512342453,
      "learning_rate": 0.0001874386613284057,
      "loss": 0.1913,
      "step": 3060
    },
    {
      "epoch": 0.1890976285802279,
      "grad_norm": 0.4087885320186615,
      "learning_rate": 0.00018739759778256854,
      "loss": 0.1878,
      "step": 3070
    },
    {
      "epoch": 0.18971358176778566,
      "grad_norm": 0.30226850509643555,
      "learning_rate": 0.00018735653423673134,
      "loss": 0.1903,
      "step": 3080
    },
    {
      "epoch": 0.19032953495534338,
      "grad_norm": 0.3167138397693634,
      "learning_rate": 0.00018731547069089416,
      "loss": 0.1928,
      "step": 3090
    },
    {
      "epoch": 0.19094548814290113,
      "grad_norm": 0.3870493769645691,
      "learning_rate": 0.00018727440714505698,
      "loss": 0.1946,
      "step": 3100
    },
    {
      "epoch": 0.19156144133045888,
      "grad_norm": 0.443910151720047,
      "learning_rate": 0.0001872333435992198,
      "loss": 0.1891,
      "step": 3110
    },
    {
      "epoch": 0.19217739451801663,
      "grad_norm": 0.6178895235061646,
      "learning_rate": 0.00018719228005338262,
      "loss": 0.1934,
      "step": 3120
    },
    {
      "epoch": 0.19279334770557438,
      "grad_norm": 0.49586084485054016,
      "learning_rate": 0.00018715121650754544,
      "loss": 0.1917,
      "step": 3130
    },
    {
      "epoch": 0.19340930089313213,
      "grad_norm": 0.29122909903526306,
      "learning_rate": 0.00018711015296170826,
      "loss": 0.1903,
      "step": 3140
    },
    {
      "epoch": 0.19402525408068988,
      "grad_norm": 0.5784405469894409,
      "learning_rate": 0.00018706908941587108,
      "loss": 0.1925,
      "step": 3150
    },
    {
      "epoch": 0.1946412072682476,
      "grad_norm": 0.2806091904640198,
      "learning_rate": 0.0001870280258700339,
      "loss": 0.192,
      "step": 3160
    },
    {
      "epoch": 0.19525716045580535,
      "grad_norm": 0.34507736563682556,
      "learning_rate": 0.0001869869623241967,
      "loss": 0.1891,
      "step": 3170
    },
    {
      "epoch": 0.1958731136433631,
      "grad_norm": 0.42759519815444946,
      "learning_rate": 0.00018694589877835954,
      "loss": 0.1899,
      "step": 3180
    },
    {
      "epoch": 0.19648906683092085,
      "grad_norm": 0.34879958629608154,
      "learning_rate": 0.00018690483523252233,
      "loss": 0.1954,
      "step": 3190
    },
    {
      "epoch": 0.1971050200184786,
      "grad_norm": 0.4115632474422455,
      "learning_rate": 0.00018686377168668515,
      "loss": 0.1952,
      "step": 3200
    },
    {
      "epoch": 0.19772097320603635,
      "grad_norm": 0.4943172037601471,
      "learning_rate": 0.00018682270814084797,
      "loss": 0.1987,
      "step": 3210
    },
    {
      "epoch": 0.1983369263935941,
      "grad_norm": 0.39259395003318787,
      "learning_rate": 0.0001867816445950108,
      "loss": 0.191,
      "step": 3220
    },
    {
      "epoch": 0.19895287958115182,
      "grad_norm": 0.28673112392425537,
      "learning_rate": 0.0001867405810491736,
      "loss": 0.193,
      "step": 3230
    },
    {
      "epoch": 0.19956883276870957,
      "grad_norm": 0.3497733175754547,
      "learning_rate": 0.00018669951750333643,
      "loss": 0.1913,
      "step": 3240
    },
    {
      "epoch": 0.20018478595626732,
      "grad_norm": 0.36403778195381165,
      "learning_rate": 0.00018665845395749922,
      "loss": 0.1903,
      "step": 3250
    },
    {
      "epoch": 0.20080073914382507,
      "grad_norm": 0.39007341861724854,
      "learning_rate": 0.00018661739041166207,
      "loss": 0.1893,
      "step": 3260
    },
    {
      "epoch": 0.20141669233138282,
      "grad_norm": 0.3113159239292145,
      "learning_rate": 0.00018657632686582486,
      "loss": 0.1926,
      "step": 3270
    },
    {
      "epoch": 0.20203264551894057,
      "grad_norm": 0.33794352412223816,
      "learning_rate": 0.00018653526331998768,
      "loss": 0.1938,
      "step": 3280
    },
    {
      "epoch": 0.20264859870649832,
      "grad_norm": 0.3910806179046631,
      "learning_rate": 0.0001864941997741505,
      "loss": 0.1928,
      "step": 3290
    },
    {
      "epoch": 0.20326455189405604,
      "grad_norm": 0.3324458599090576,
      "learning_rate": 0.00018645313622831332,
      "loss": 0.2017,
      "step": 3300
    },
    {
      "epoch": 0.2038805050816138,
      "grad_norm": 0.4114878177642822,
      "learning_rate": 0.00018641207268247614,
      "loss": 0.1938,
      "step": 3310
    },
    {
      "epoch": 0.20449645826917154,
      "grad_norm": 0.37940117716789246,
      "learning_rate": 0.00018637100913663896,
      "loss": 0.1916,
      "step": 3320
    },
    {
      "epoch": 0.2051124114567293,
      "grad_norm": 0.3742982745170593,
      "learning_rate": 0.00018632994559080176,
      "loss": 0.1907,
      "step": 3330
    },
    {
      "epoch": 0.20572836464428704,
      "grad_norm": 0.30723339319229126,
      "learning_rate": 0.0001862888820449646,
      "loss": 0.187,
      "step": 3340
    },
    {
      "epoch": 0.2063443178318448,
      "grad_norm": 0.31496715545654297,
      "learning_rate": 0.0001862478184991274,
      "loss": 0.1875,
      "step": 3350
    },
    {
      "epoch": 0.20696027101940254,
      "grad_norm": 0.43731436133384705,
      "learning_rate": 0.00018620675495329022,
      "loss": 0.19,
      "step": 3360
    },
    {
      "epoch": 0.20757622420696026,
      "grad_norm": 0.4308774471282959,
      "learning_rate": 0.00018616569140745304,
      "loss": 0.1889,
      "step": 3370
    },
    {
      "epoch": 0.208192177394518,
      "grad_norm": 0.5344767570495605,
      "learning_rate": 0.00018612462786161586,
      "loss": 0.1934,
      "step": 3380
    },
    {
      "epoch": 0.20880813058207576,
      "grad_norm": 0.7494576573371887,
      "learning_rate": 0.00018608356431577868,
      "loss": 0.186,
      "step": 3390
    },
    {
      "epoch": 0.2094240837696335,
      "grad_norm": 0.30698758363723755,
      "learning_rate": 0.0001860425007699415,
      "loss": 0.1905,
      "step": 3400
    },
    {
      "epoch": 0.21004003695719126,
      "grad_norm": 0.2853989899158478,
      "learning_rate": 0.0001860014372241043,
      "loss": 0.1943,
      "step": 3410
    },
    {
      "epoch": 0.210655990144749,
      "grad_norm": 0.3880818784236908,
      "learning_rate": 0.00018596037367826714,
      "loss": 0.1915,
      "step": 3420
    },
    {
      "epoch": 0.21127194333230676,
      "grad_norm": 0.34942129254341125,
      "learning_rate": 0.00018591931013242993,
      "loss": 0.1897,
      "step": 3430
    },
    {
      "epoch": 0.21188789651986448,
      "grad_norm": 0.280159592628479,
      "learning_rate": 0.00018587824658659278,
      "loss": 0.1876,
      "step": 3440
    },
    {
      "epoch": 0.21250384970742223,
      "grad_norm": 0.3275480568408966,
      "learning_rate": 0.00018583718304075557,
      "loss": 0.1924,
      "step": 3450
    },
    {
      "epoch": 0.21311980289497998,
      "grad_norm": 0.2710099518299103,
      "learning_rate": 0.0001857961194949184,
      "loss": 0.1945,
      "step": 3460
    },
    {
      "epoch": 0.21373575608253773,
      "grad_norm": 0.3407118618488312,
      "learning_rate": 0.0001857550559490812,
      "loss": 0.1898,
      "step": 3470
    },
    {
      "epoch": 0.21435170927009548,
      "grad_norm": 0.31640326976776123,
      "learning_rate": 0.00018571399240324403,
      "loss": 0.1933,
      "step": 3480
    },
    {
      "epoch": 0.21496766245765322,
      "grad_norm": 0.39257746934890747,
      "learning_rate": 0.00018567292885740682,
      "loss": 0.194,
      "step": 3490
    },
    {
      "epoch": 0.21558361564521097,
      "grad_norm": 0.5750039219856262,
      "learning_rate": 0.00018563186531156967,
      "loss": 0.1873,
      "step": 3500
    },
    {
      "epoch": 0.2161995688327687,
      "grad_norm": 0.40243837237358093,
      "learning_rate": 0.00018559080176573246,
      "loss": 0.1906,
      "step": 3510
    },
    {
      "epoch": 0.21681552202032645,
      "grad_norm": 0.306738018989563,
      "learning_rate": 0.0001855497382198953,
      "loss": 0.1894,
      "step": 3520
    },
    {
      "epoch": 0.2174314752078842,
      "grad_norm": 0.2859647870063782,
      "learning_rate": 0.0001855086746740581,
      "loss": 0.1905,
      "step": 3530
    },
    {
      "epoch": 0.21804742839544194,
      "grad_norm": 0.47278016805648804,
      "learning_rate": 0.00018546761112822092,
      "loss": 0.1916,
      "step": 3540
    },
    {
      "epoch": 0.2186633815829997,
      "grad_norm": 0.39499107003211975,
      "learning_rate": 0.00018542654758238377,
      "loss": 0.1935,
      "step": 3550
    },
    {
      "epoch": 0.21927933477055744,
      "grad_norm": 0.35460489988327026,
      "learning_rate": 0.00018538548403654656,
      "loss": 0.1886,
      "step": 3560
    },
    {
      "epoch": 0.2198952879581152,
      "grad_norm": 0.3148716390132904,
      "learning_rate": 0.00018534442049070938,
      "loss": 0.1919,
      "step": 3570
    },
    {
      "epoch": 0.22051124114567294,
      "grad_norm": 0.2963656187057495,
      "learning_rate": 0.0001853033569448722,
      "loss": 0.1879,
      "step": 3580
    },
    {
      "epoch": 0.22112719433323066,
      "grad_norm": 0.3142869174480438,
      "learning_rate": 0.00018526229339903502,
      "loss": 0.1905,
      "step": 3590
    },
    {
      "epoch": 0.22174314752078841,
      "grad_norm": 0.3613278865814209,
      "learning_rate": 0.00018522122985319784,
      "loss": 0.1929,
      "step": 3600
    },
    {
      "epoch": 0.22235910070834616,
      "grad_norm": 0.35037532448768616,
      "learning_rate": 0.00018518016630736066,
      "loss": 0.1939,
      "step": 3610
    },
    {
      "epoch": 0.2229750538959039,
      "grad_norm": 0.3179756700992584,
      "learning_rate": 0.00018513910276152346,
      "loss": 0.19,
      "step": 3620
    },
    {
      "epoch": 0.22359100708346166,
      "grad_norm": 0.5638105869293213,
      "learning_rate": 0.0001850980392156863,
      "loss": 0.1865,
      "step": 3630
    },
    {
      "epoch": 0.2242069602710194,
      "grad_norm": 0.38738542795181274,
      "learning_rate": 0.0001850569756698491,
      "loss": 0.1897,
      "step": 3640
    },
    {
      "epoch": 0.22482291345857716,
      "grad_norm": 0.39409422874450684,
      "learning_rate": 0.00018501591212401192,
      "loss": 0.1891,
      "step": 3650
    },
    {
      "epoch": 0.22543886664613488,
      "grad_norm": 0.28444620966911316,
      "learning_rate": 0.00018497484857817474,
      "loss": 0.1897,
      "step": 3660
    },
    {
      "epoch": 0.22605481983369263,
      "grad_norm": 0.638178825378418,
      "learning_rate": 0.00018493378503233756,
      "loss": 0.1951,
      "step": 3670
    },
    {
      "epoch": 0.22667077302125038,
      "grad_norm": 0.3878794014453888,
      "learning_rate": 0.00018489272148650038,
      "loss": 0.1926,
      "step": 3680
    },
    {
      "epoch": 0.22728672620880813,
      "grad_norm": 0.3311064839363098,
      "learning_rate": 0.0001848516579406632,
      "loss": 0.1881,
      "step": 3690
    },
    {
      "epoch": 0.22790267939636588,
      "grad_norm": 0.30906057357788086,
      "learning_rate": 0.000184810594394826,
      "loss": 0.1884,
      "step": 3700
    },
    {
      "epoch": 0.22851863258392363,
      "grad_norm": 0.33492711186408997,
      "learning_rate": 0.00018476953084898884,
      "loss": 0.1898,
      "step": 3710
    },
    {
      "epoch": 0.22913458577148138,
      "grad_norm": 0.37247082591056824,
      "learning_rate": 0.00018472846730315163,
      "loss": 0.1877,
      "step": 3720
    },
    {
      "epoch": 0.2297505389590391,
      "grad_norm": 0.2799328565597534,
      "learning_rate": 0.00018468740375731445,
      "loss": 0.1912,
      "step": 3730
    },
    {
      "epoch": 0.23036649214659685,
      "grad_norm": 0.3802105784416199,
      "learning_rate": 0.00018464634021147727,
      "loss": 0.1906,
      "step": 3740
    },
    {
      "epoch": 0.2309824453341546,
      "grad_norm": 0.374310165643692,
      "learning_rate": 0.0001846052766656401,
      "loss": 0.1916,
      "step": 3750
    },
    {
      "epoch": 0.23159839852171235,
      "grad_norm": 0.3671802282333374,
      "learning_rate": 0.0001845642131198029,
      "loss": 0.193,
      "step": 3760
    },
    {
      "epoch": 0.2322143517092701,
      "grad_norm": 0.3387574255466461,
      "learning_rate": 0.00018452314957396573,
      "loss": 0.1915,
      "step": 3770
    },
    {
      "epoch": 0.23283030489682785,
      "grad_norm": 0.32987046241760254,
      "learning_rate": 0.00018448208602812852,
      "loss": 0.1864,
      "step": 3780
    },
    {
      "epoch": 0.2334462580843856,
      "grad_norm": 0.2701258361339569,
      "learning_rate": 0.00018444102248229137,
      "loss": 0.1933,
      "step": 3790
    },
    {
      "epoch": 0.23406221127194332,
      "grad_norm": 0.3885954022407532,
      "learning_rate": 0.00018439995893645416,
      "loss": 0.1847,
      "step": 3800
    },
    {
      "epoch": 0.23467816445950107,
      "grad_norm": 0.27042317390441895,
      "learning_rate": 0.00018435889539061698,
      "loss": 0.1863,
      "step": 3810
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.46807825565338135,
      "learning_rate": 0.0001843178318447798,
      "loss": 0.1894,
      "step": 3820
    },
    {
      "epoch": 0.23591007083461657,
      "grad_norm": 0.26586899161338806,
      "learning_rate": 0.00018427676829894262,
      "loss": 0.1898,
      "step": 3830
    },
    {
      "epoch": 0.23652602402217432,
      "grad_norm": 0.3136551082134247,
      "learning_rate": 0.00018423570475310544,
      "loss": 0.1898,
      "step": 3840
    },
    {
      "epoch": 0.23714197720973207,
      "grad_norm": 0.23044182360172272,
      "learning_rate": 0.00018419464120726826,
      "loss": 0.19,
      "step": 3850
    },
    {
      "epoch": 0.23775793039728982,
      "grad_norm": 0.4046034514904022,
      "learning_rate": 0.00018415357766143106,
      "loss": 0.1909,
      "step": 3860
    },
    {
      "epoch": 0.23837388358484754,
      "grad_norm": 0.4319339692592621,
      "learning_rate": 0.0001841125141155939,
      "loss": 0.193,
      "step": 3870
    },
    {
      "epoch": 0.2389898367724053,
      "grad_norm": 0.6679793000221252,
      "learning_rate": 0.0001840714505697567,
      "loss": 0.1883,
      "step": 3880
    },
    {
      "epoch": 0.23960578995996304,
      "grad_norm": 0.42285963892936707,
      "learning_rate": 0.00018403038702391952,
      "loss": 0.1947,
      "step": 3890
    },
    {
      "epoch": 0.2402217431475208,
      "grad_norm": 0.2500990033149719,
      "learning_rate": 0.00018398932347808234,
      "loss": 0.1857,
      "step": 3900
    },
    {
      "epoch": 0.24083769633507854,
      "grad_norm": 0.2617420554161072,
      "learning_rate": 0.00018394825993224516,
      "loss": 0.1903,
      "step": 3910
    },
    {
      "epoch": 0.2414536495226363,
      "grad_norm": 0.24662744998931885,
      "learning_rate": 0.00018390719638640798,
      "loss": 0.1907,
      "step": 3920
    },
    {
      "epoch": 0.24206960271019404,
      "grad_norm": 0.7549862861633301,
      "learning_rate": 0.0001838661328405708,
      "loss": 0.189,
      "step": 3930
    },
    {
      "epoch": 0.24268555589775176,
      "grad_norm": 0.3042100667953491,
      "learning_rate": 0.0001838250692947336,
      "loss": 0.1879,
      "step": 3940
    },
    {
      "epoch": 0.2433015090853095,
      "grad_norm": 0.3683854639530182,
      "learning_rate": 0.00018378400574889644,
      "loss": 0.1959,
      "step": 3950
    },
    {
      "epoch": 0.24391746227286726,
      "grad_norm": 0.359847754240036,
      "learning_rate": 0.00018374294220305926,
      "loss": 0.1912,
      "step": 3960
    },
    {
      "epoch": 0.244533415460425,
      "grad_norm": 0.3687392771244049,
      "learning_rate": 0.00018370187865722205,
      "loss": 0.1911,
      "step": 3970
    },
    {
      "epoch": 0.24514936864798276,
      "grad_norm": 0.34592095017433167,
      "learning_rate": 0.0001836608151113849,
      "loss": 0.1896,
      "step": 3980
    },
    {
      "epoch": 0.2457653218355405,
      "grad_norm": 0.22667261958122253,
      "learning_rate": 0.0001836197515655477,
      "loss": 0.1863,
      "step": 3990
    },
    {
      "epoch": 0.24638127502309826,
      "grad_norm": 0.34283965826034546,
      "learning_rate": 0.0001835786880197105,
      "loss": 0.1848,
      "step": 4000
    },
    {
      "epoch": 0.24699722821065598,
      "grad_norm": 0.3824295103549957,
      "learning_rate": 0.00018353762447387333,
      "loss": 0.1872,
      "step": 4010
    },
    {
      "epoch": 0.24761318139821373,
      "grad_norm": 0.3082994818687439,
      "learning_rate": 0.00018349656092803615,
      "loss": 0.1897,
      "step": 4020
    },
    {
      "epoch": 0.24822913458577148,
      "grad_norm": 0.7644195556640625,
      "learning_rate": 0.00018345549738219897,
      "loss": 0.1907,
      "step": 4030
    },
    {
      "epoch": 0.24884508777332923,
      "grad_norm": 0.40500712394714355,
      "learning_rate": 0.0001834144338363618,
      "loss": 0.1915,
      "step": 4040
    },
    {
      "epoch": 0.24946104096088698,
      "grad_norm": 0.323814332485199,
      "learning_rate": 0.00018337337029052458,
      "loss": 0.1919,
      "step": 4050
    },
    {
      "epoch": 0.2500769941484447,
      "grad_norm": 0.2858547568321228,
      "learning_rate": 0.00018333230674468743,
      "loss": 0.1846,
      "step": 4060
    },
    {
      "epoch": 0.25069294733600245,
      "grad_norm": 0.3211553394794464,
      "learning_rate": 0.00018329124319885022,
      "loss": 0.1941,
      "step": 4070
    },
    {
      "epoch": 0.2513089005235602,
      "grad_norm": 0.27529647946357727,
      "learning_rate": 0.00018325017965301304,
      "loss": 0.1931,
      "step": 4080
    },
    {
      "epoch": 0.25192485371111795,
      "grad_norm": 0.39107128977775574,
      "learning_rate": 0.00018320911610717586,
      "loss": 0.1913,
      "step": 4090
    },
    {
      "epoch": 0.2525408068986757,
      "grad_norm": 0.29976245760917664,
      "learning_rate": 0.00018316805256133868,
      "loss": 0.1865,
      "step": 4100
    },
    {
      "epoch": 0.25315676008623345,
      "grad_norm": 0.3064842224121094,
      "learning_rate": 0.0001831269890155015,
      "loss": 0.1882,
      "step": 4110
    },
    {
      "epoch": 0.2537727132737912,
      "grad_norm": 0.37565386295318604,
      "learning_rate": 0.00018308592546966432,
      "loss": 0.1864,
      "step": 4120
    },
    {
      "epoch": 0.25438866646134894,
      "grad_norm": 0.29785290360450745,
      "learning_rate": 0.00018304486192382712,
      "loss": 0.1941,
      "step": 4130
    },
    {
      "epoch": 0.2550046196489067,
      "grad_norm": 0.29130101203918457,
      "learning_rate": 0.00018300379837798996,
      "loss": 0.1864,
      "step": 4140
    },
    {
      "epoch": 0.25562057283646444,
      "grad_norm": 0.29627561569213867,
      "learning_rate": 0.00018296273483215276,
      "loss": 0.1864,
      "step": 4150
    },
    {
      "epoch": 0.2562365260240222,
      "grad_norm": 0.3000469505786896,
      "learning_rate": 0.00018292167128631558,
      "loss": 0.1873,
      "step": 4160
    },
    {
      "epoch": 0.25685247921157994,
      "grad_norm": 0.2540222704410553,
      "learning_rate": 0.0001828806077404784,
      "loss": 0.1826,
      "step": 4170
    },
    {
      "epoch": 0.2574684323991377,
      "grad_norm": 0.2664772868156433,
      "learning_rate": 0.00018283954419464122,
      "loss": 0.1848,
      "step": 4180
    },
    {
      "epoch": 0.2580843855866954,
      "grad_norm": 0.21959245204925537,
      "learning_rate": 0.00018279848064880404,
      "loss": 0.1885,
      "step": 4190
    },
    {
      "epoch": 0.25870033877425314,
      "grad_norm": 0.2786618173122406,
      "learning_rate": 0.00018275741710296686,
      "loss": 0.1873,
      "step": 4200
    },
    {
      "epoch": 0.2593162919618109,
      "grad_norm": 0.24349670112133026,
      "learning_rate": 0.00018271635355712965,
      "loss": 0.187,
      "step": 4210
    },
    {
      "epoch": 0.25993224514936863,
      "grad_norm": 0.3219289183616638,
      "learning_rate": 0.0001826752900112925,
      "loss": 0.1933,
      "step": 4220
    },
    {
      "epoch": 0.2605481983369264,
      "grad_norm": 0.34170833230018616,
      "learning_rate": 0.0001826342264654553,
      "loss": 0.1932,
      "step": 4230
    },
    {
      "epoch": 0.26116415152448413,
      "grad_norm": 0.4747556149959564,
      "learning_rate": 0.0001825931629196181,
      "loss": 0.191,
      "step": 4240
    },
    {
      "epoch": 0.2617801047120419,
      "grad_norm": 0.5337989330291748,
      "learning_rate": 0.00018255209937378093,
      "loss": 0.1883,
      "step": 4250
    },
    {
      "epoch": 0.26239605789959963,
      "grad_norm": 0.2746410369873047,
      "learning_rate": 0.00018251103582794375,
      "loss": 0.1877,
      "step": 4260
    },
    {
      "epoch": 0.2630120110871574,
      "grad_norm": 0.2505245804786682,
      "learning_rate": 0.00018246997228210657,
      "loss": 0.1856,
      "step": 4270
    },
    {
      "epoch": 0.26362796427471513,
      "grad_norm": 0.345847487449646,
      "learning_rate": 0.0001824289087362694,
      "loss": 0.1904,
      "step": 4280
    },
    {
      "epoch": 0.2642439174622729,
      "grad_norm": 0.3050630986690521,
      "learning_rate": 0.00018238784519043218,
      "loss": 0.1886,
      "step": 4290
    },
    {
      "epoch": 0.26485987064983063,
      "grad_norm": 0.2551364600658417,
      "learning_rate": 0.00018234678164459503,
      "loss": 0.1847,
      "step": 4300
    },
    {
      "epoch": 0.2654758238373884,
      "grad_norm": 0.24435840547084808,
      "learning_rate": 0.00018230571809875782,
      "loss": 0.1833,
      "step": 4310
    },
    {
      "epoch": 0.26609177702494613,
      "grad_norm": 0.3204558789730072,
      "learning_rate": 0.00018226465455292064,
      "loss": 0.1884,
      "step": 4320
    },
    {
      "epoch": 0.2667077302125038,
      "grad_norm": 0.46410438418388367,
      "learning_rate": 0.00018222359100708346,
      "loss": 0.1885,
      "step": 4330
    },
    {
      "epoch": 0.2673236834000616,
      "grad_norm": 0.2954150438308716,
      "learning_rate": 0.00018218252746124628,
      "loss": 0.1914,
      "step": 4340
    },
    {
      "epoch": 0.2679396365876193,
      "grad_norm": 0.31591126322746277,
      "learning_rate": 0.0001821414639154091,
      "loss": 0.1937,
      "step": 4350
    },
    {
      "epoch": 0.2685555897751771,
      "grad_norm": 0.2882768511772156,
      "learning_rate": 0.00018210040036957192,
      "loss": 0.186,
      "step": 4360
    },
    {
      "epoch": 0.2691715429627348,
      "grad_norm": 0.4059904217720032,
      "learning_rate": 0.00018205933682373472,
      "loss": 0.1906,
      "step": 4370
    },
    {
      "epoch": 0.26978749615029257,
      "grad_norm": 0.29747557640075684,
      "learning_rate": 0.00018201827327789756,
      "loss": 0.184,
      "step": 4380
    },
    {
      "epoch": 0.2704034493378503,
      "grad_norm": 0.3023849427700043,
      "learning_rate": 0.00018197720973206038,
      "loss": 0.1891,
      "step": 4390
    },
    {
      "epoch": 0.27101940252540807,
      "grad_norm": 0.2989089787006378,
      "learning_rate": 0.00018193614618622318,
      "loss": 0.1893,
      "step": 4400
    },
    {
      "epoch": 0.2716353557129658,
      "grad_norm": 0.2679476737976074,
      "learning_rate": 0.00018189508264038602,
      "loss": 0.1886,
      "step": 4410
    },
    {
      "epoch": 0.27225130890052357,
      "grad_norm": 0.26613742113113403,
      "learning_rate": 0.00018185401909454882,
      "loss": 0.1878,
      "step": 4420
    },
    {
      "epoch": 0.2728672620880813,
      "grad_norm": 0.31628432869911194,
      "learning_rate": 0.00018181295554871166,
      "loss": 0.1918,
      "step": 4430
    },
    {
      "epoch": 0.27348321527563907,
      "grad_norm": 0.35386157035827637,
      "learning_rate": 0.00018177189200287446,
      "loss": 0.1827,
      "step": 4440
    },
    {
      "epoch": 0.2740991684631968,
      "grad_norm": 0.5453155636787415,
      "learning_rate": 0.00018173082845703728,
      "loss": 0.1904,
      "step": 4450
    },
    {
      "epoch": 0.27471512165075457,
      "grad_norm": 0.37402161955833435,
      "learning_rate": 0.0001816897649112001,
      "loss": 0.188,
      "step": 4460
    },
    {
      "epoch": 0.27533107483831226,
      "grad_norm": 0.3175639510154724,
      "learning_rate": 0.00018164870136536292,
      "loss": 0.1839,
      "step": 4470
    },
    {
      "epoch": 0.27594702802587,
      "grad_norm": 0.250975638628006,
      "learning_rate": 0.00018160763781952574,
      "loss": 0.1828,
      "step": 4480
    },
    {
      "epoch": 0.27656298121342776,
      "grad_norm": 0.2929421365261078,
      "learning_rate": 0.00018156657427368856,
      "loss": 0.188,
      "step": 4490
    },
    {
      "epoch": 0.2771789344009855,
      "grad_norm": 0.24655281007289886,
      "learning_rate": 0.00018152551072785135,
      "loss": 0.1883,
      "step": 4500
    },
    {
      "epoch": 0.27779488758854326,
      "grad_norm": 0.257697731256485,
      "learning_rate": 0.0001814844471820142,
      "loss": 0.1879,
      "step": 4510
    },
    {
      "epoch": 0.278410840776101,
      "grad_norm": 0.3405633866786957,
      "learning_rate": 0.000181443383636177,
      "loss": 0.1884,
      "step": 4520
    },
    {
      "epoch": 0.27902679396365876,
      "grad_norm": 0.3696563243865967,
      "learning_rate": 0.0001814023200903398,
      "loss": 0.1868,
      "step": 4530
    },
    {
      "epoch": 0.2796427471512165,
      "grad_norm": 0.2902500331401825,
      "learning_rate": 0.00018136125654450263,
      "loss": 0.1896,
      "step": 4540
    },
    {
      "epoch": 0.28025870033877426,
      "grad_norm": 0.31237947940826416,
      "learning_rate": 0.00018132019299866545,
      "loss": 0.1862,
      "step": 4550
    },
    {
      "epoch": 0.280874653526332,
      "grad_norm": 0.2581152617931366,
      "learning_rate": 0.00018127912945282827,
      "loss": 0.1883,
      "step": 4560
    },
    {
      "epoch": 0.28149060671388976,
      "grad_norm": 0.27800294756889343,
      "learning_rate": 0.0001812380659069911,
      "loss": 0.1844,
      "step": 4570
    },
    {
      "epoch": 0.2821065599014475,
      "grad_norm": 0.24976074695587158,
      "learning_rate": 0.00018119700236115388,
      "loss": 0.1877,
      "step": 4580
    },
    {
      "epoch": 0.28272251308900526,
      "grad_norm": 0.3733265697956085,
      "learning_rate": 0.00018115593881531673,
      "loss": 0.1871,
      "step": 4590
    },
    {
      "epoch": 0.283338466276563,
      "grad_norm": 0.28468966484069824,
      "learning_rate": 0.00018111487526947952,
      "loss": 0.1852,
      "step": 4600
    },
    {
      "epoch": 0.2839544194641207,
      "grad_norm": 0.27488452196121216,
      "learning_rate": 0.00018107381172364234,
      "loss": 0.1825,
      "step": 4610
    },
    {
      "epoch": 0.28457037265167845,
      "grad_norm": 0.24941350519657135,
      "learning_rate": 0.00018103274817780516,
      "loss": 0.185,
      "step": 4620
    },
    {
      "epoch": 0.2851863258392362,
      "grad_norm": 0.3401075303554535,
      "learning_rate": 0.00018099168463196798,
      "loss": 0.188,
      "step": 4630
    },
    {
      "epoch": 0.28580227902679395,
      "grad_norm": 0.2734154164791107,
      "learning_rate": 0.0001809506210861308,
      "loss": 0.1848,
      "step": 4640
    },
    {
      "epoch": 0.2864182322143517,
      "grad_norm": 0.27299413084983826,
      "learning_rate": 0.00018090955754029362,
      "loss": 0.1886,
      "step": 4650
    },
    {
      "epoch": 0.28703418540190945,
      "grad_norm": 0.3190351128578186,
      "learning_rate": 0.00018086849399445641,
      "loss": 0.1904,
      "step": 4660
    },
    {
      "epoch": 0.2876501385894672,
      "grad_norm": 0.40330302715301514,
      "learning_rate": 0.00018082743044861926,
      "loss": 0.1869,
      "step": 4670
    },
    {
      "epoch": 0.28826609177702495,
      "grad_norm": 0.38427072763442993,
      "learning_rate": 0.00018078636690278206,
      "loss": 0.1854,
      "step": 4680
    },
    {
      "epoch": 0.2888820449645827,
      "grad_norm": 0.31150248646736145,
      "learning_rate": 0.00018074530335694488,
      "loss": 0.1894,
      "step": 4690
    },
    {
      "epoch": 0.28949799815214045,
      "grad_norm": 0.30533769726753235,
      "learning_rate": 0.0001807042398111077,
      "loss": 0.1855,
      "step": 4700
    },
    {
      "epoch": 0.2901139513396982,
      "grad_norm": 0.40385133028030396,
      "learning_rate": 0.00018066317626527052,
      "loss": 0.1857,
      "step": 4710
    },
    {
      "epoch": 0.29072990452725594,
      "grad_norm": 0.344712495803833,
      "learning_rate": 0.00018062211271943334,
      "loss": 0.188,
      "step": 4720
    },
    {
      "epoch": 0.2913458577148137,
      "grad_norm": 0.5113805532455444,
      "learning_rate": 0.00018058104917359616,
      "loss": 0.1869,
      "step": 4730
    },
    {
      "epoch": 0.29196181090237144,
      "grad_norm": 0.4228465259075165,
      "learning_rate": 0.00018053998562775895,
      "loss": 0.1868,
      "step": 4740
    },
    {
      "epoch": 0.29257776408992914,
      "grad_norm": 0.26438257098197937,
      "learning_rate": 0.0001804989220819218,
      "loss": 0.1899,
      "step": 4750
    },
    {
      "epoch": 0.2931937172774869,
      "grad_norm": 0.31643813848495483,
      "learning_rate": 0.0001804578585360846,
      "loss": 0.1872,
      "step": 4760
    },
    {
      "epoch": 0.29380967046504464,
      "grad_norm": 0.24400821328163147,
      "learning_rate": 0.0001804167949902474,
      "loss": 0.19,
      "step": 4770
    },
    {
      "epoch": 0.2944256236526024,
      "grad_norm": 0.3786824643611908,
      "learning_rate": 0.00018037573144441023,
      "loss": 0.1878,
      "step": 4780
    },
    {
      "epoch": 0.29504157684016014,
      "grad_norm": 0.32364463806152344,
      "learning_rate": 0.00018033466789857305,
      "loss": 0.1861,
      "step": 4790
    },
    {
      "epoch": 0.2956575300277179,
      "grad_norm": 0.28450843691825867,
      "learning_rate": 0.00018029360435273587,
      "loss": 0.1887,
      "step": 4800
    },
    {
      "epoch": 0.29627348321527563,
      "grad_norm": 0.4821785092353821,
      "learning_rate": 0.0001802525408068987,
      "loss": 0.1855,
      "step": 4810
    },
    {
      "epoch": 0.2968894364028334,
      "grad_norm": 0.4039570689201355,
      "learning_rate": 0.0001802114772610615,
      "loss": 0.1867,
      "step": 4820
    },
    {
      "epoch": 0.29750538959039113,
      "grad_norm": 0.2707075774669647,
      "learning_rate": 0.00018017041371522433,
      "loss": 0.1876,
      "step": 4830
    },
    {
      "epoch": 0.2981213427779489,
      "grad_norm": 0.30473196506500244,
      "learning_rate": 0.00018012935016938715,
      "loss": 0.1884,
      "step": 4840
    },
    {
      "epoch": 0.29873729596550663,
      "grad_norm": 0.29810160398483276,
      "learning_rate": 0.00018008828662354994,
      "loss": 0.1888,
      "step": 4850
    },
    {
      "epoch": 0.2993532491530644,
      "grad_norm": 0.3946954309940338,
      "learning_rate": 0.00018005132943229647,
      "loss": 0.1877,
      "step": 4860
    },
    {
      "epoch": 0.29996920234062213,
      "grad_norm": 0.2572045624256134,
      "learning_rate": 0.0001800102658864593,
      "loss": 0.1891,
      "step": 4870
    },
    {
      "epoch": 0.3005851555281799,
      "grad_norm": 0.3022648096084595,
      "learning_rate": 0.0001799692023406221,
      "loss": 0.1861,
      "step": 4880
    },
    {
      "epoch": 0.30120110871573763,
      "grad_norm": 0.2635476887226105,
      "learning_rate": 0.00017992813879478493,
      "loss": 0.1913,
      "step": 4890
    },
    {
      "epoch": 0.3018170619032953,
      "grad_norm": 0.300546258687973,
      "learning_rate": 0.00017988707524894775,
      "loss": 0.1884,
      "step": 4900
    },
    {
      "epoch": 0.3024330150908531,
      "grad_norm": 0.38133034110069275,
      "learning_rate": 0.00017984601170311057,
      "loss": 0.1842,
      "step": 4910
    },
    {
      "epoch": 0.3030489682784108,
      "grad_norm": 0.35279324650764465,
      "learning_rate": 0.00017980494815727339,
      "loss": 0.1881,
      "step": 4920
    },
    {
      "epoch": 0.3036649214659686,
      "grad_norm": 0.259861558675766,
      "learning_rate": 0.0001797638846114362,
      "loss": 0.1848,
      "step": 4930
    },
    {
      "epoch": 0.3042808746535263,
      "grad_norm": 0.28268080949783325,
      "learning_rate": 0.00017972282106559903,
      "loss": 0.1885,
      "step": 4940
    },
    {
      "epoch": 0.3048968278410841,
      "grad_norm": 0.30903178453445435,
      "learning_rate": 0.00017968175751976185,
      "loss": 0.1905,
      "step": 4950
    },
    {
      "epoch": 0.3055127810286418,
      "grad_norm": 0.35005903244018555,
      "learning_rate": 0.00017964069397392467,
      "loss": 0.1832,
      "step": 4960
    },
    {
      "epoch": 0.30612873421619957,
      "grad_norm": 0.2172963172197342,
      "learning_rate": 0.00017959963042808749,
      "loss": 0.1905,
      "step": 4970
    },
    {
      "epoch": 0.3067446874037573,
      "grad_norm": 0.34898847341537476,
      "learning_rate": 0.0001795585668822503,
      "loss": 0.1915,
      "step": 4980
    },
    {
      "epoch": 0.30736064059131507,
      "grad_norm": 0.2329118400812149,
      "learning_rate": 0.0001795175033364131,
      "loss": 0.1912,
      "step": 4990
    },
    {
      "epoch": 0.3079765937788728,
      "grad_norm": 0.2889817953109741,
      "learning_rate": 0.00017947643979057595,
      "loss": 0.1836,
      "step": 5000
    },
    {
      "epoch": 0.30859254696643057,
      "grad_norm": 0.6442880034446716,
      "learning_rate": 0.00017943537624473874,
      "loss": 0.1861,
      "step": 5010
    },
    {
      "epoch": 0.3092085001539883,
      "grad_norm": 0.31584596633911133,
      "learning_rate": 0.00017939431269890156,
      "loss": 0.1856,
      "step": 5020
    },
    {
      "epoch": 0.30982445334154607,
      "grad_norm": 0.42304307222366333,
      "learning_rate": 0.00017935324915306438,
      "loss": 0.1877,
      "step": 5030
    },
    {
      "epoch": 0.31044040652910376,
      "grad_norm": 0.5156975984573364,
      "learning_rate": 0.0001793121856072272,
      "loss": 0.1869,
      "step": 5040
    },
    {
      "epoch": 0.3110563597166615,
      "grad_norm": 0.25591564178466797,
      "learning_rate": 0.00017927112206139002,
      "loss": 0.1852,
      "step": 5050
    },
    {
      "epoch": 0.31167231290421926,
      "grad_norm": 0.213660329580307,
      "learning_rate": 0.00017923005851555284,
      "loss": 0.187,
      "step": 5060
    },
    {
      "epoch": 0.312288266091777,
      "grad_norm": 0.9140397310256958,
      "learning_rate": 0.00017918899496971563,
      "loss": 0.1874,
      "step": 5070
    },
    {
      "epoch": 0.31290421927933476,
      "grad_norm": 0.32710278034210205,
      "learning_rate": 0.00017914793142387848,
      "loss": 0.1892,
      "step": 5080
    },
    {
      "epoch": 0.3135201724668925,
      "grad_norm": 0.4861152768135071,
      "learning_rate": 0.00017910686787804127,
      "loss": 0.1877,
      "step": 5090
    },
    {
      "epoch": 0.31413612565445026,
      "grad_norm": 0.2714218497276306,
      "learning_rate": 0.0001790658043322041,
      "loss": 0.1847,
      "step": 5100
    },
    {
      "epoch": 0.314752078842008,
      "grad_norm": 0.3143480122089386,
      "learning_rate": 0.0001790247407863669,
      "loss": 0.1888,
      "step": 5110
    },
    {
      "epoch": 0.31536803202956576,
      "grad_norm": 0.26658642292022705,
      "learning_rate": 0.00017898367724052973,
      "loss": 0.1861,
      "step": 5120
    },
    {
      "epoch": 0.3159839852171235,
      "grad_norm": 0.2510721683502197,
      "learning_rate": 0.00017894261369469255,
      "loss": 0.1878,
      "step": 5130
    },
    {
      "epoch": 0.31659993840468126,
      "grad_norm": 0.2865051329135895,
      "learning_rate": 0.00017890155014885537,
      "loss": 0.1898,
      "step": 5140
    },
    {
      "epoch": 0.317215891592239,
      "grad_norm": 0.3013227880001068,
      "learning_rate": 0.00017886048660301817,
      "loss": 0.1864,
      "step": 5150
    },
    {
      "epoch": 0.31783184477979676,
      "grad_norm": 0.5145537257194519,
      "learning_rate": 0.000178819423057181,
      "loss": 0.1854,
      "step": 5160
    },
    {
      "epoch": 0.3184477979673545,
      "grad_norm": 0.3894345760345459,
      "learning_rate": 0.0001787783595113438,
      "loss": 0.1872,
      "step": 5170
    },
    {
      "epoch": 0.3190637511549122,
      "grad_norm": 0.3524140417575836,
      "learning_rate": 0.00017873729596550663,
      "loss": 0.1892,
      "step": 5180
    },
    {
      "epoch": 0.31967970434246995,
      "grad_norm": 0.2750760316848755,
      "learning_rate": 0.00017869623241966945,
      "loss": 0.187,
      "step": 5190
    },
    {
      "epoch": 0.3202956575300277,
      "grad_norm": 0.30818837881088257,
      "learning_rate": 0.00017865516887383227,
      "loss": 0.1863,
      "step": 5200
    },
    {
      "epoch": 0.32091161071758545,
      "grad_norm": 0.3274992108345032,
      "learning_rate": 0.00017861410532799509,
      "loss": 0.1913,
      "step": 5210
    },
    {
      "epoch": 0.3215275639051432,
      "grad_norm": 0.2468799650669098,
      "learning_rate": 0.0001785730417821579,
      "loss": 0.1894,
      "step": 5220
    },
    {
      "epoch": 0.32214351709270095,
      "grad_norm": 0.39543530344963074,
      "learning_rate": 0.0001785319782363207,
      "loss": 0.1887,
      "step": 5230
    },
    {
      "epoch": 0.3227594702802587,
      "grad_norm": 0.3441774249076843,
      "learning_rate": 0.00017849091469048355,
      "loss": 0.1857,
      "step": 5240
    },
    {
      "epoch": 0.32337542346781645,
      "grad_norm": 0.2616150677204132,
      "learning_rate": 0.00017844985114464634,
      "loss": 0.1905,
      "step": 5250
    },
    {
      "epoch": 0.3239913766553742,
      "grad_norm": 0.2730618417263031,
      "learning_rate": 0.00017840878759880916,
      "loss": 0.1883,
      "step": 5260
    },
    {
      "epoch": 0.32460732984293195,
      "grad_norm": 0.33697405457496643,
      "learning_rate": 0.00017836772405297198,
      "loss": 0.1878,
      "step": 5270
    },
    {
      "epoch": 0.3252232830304897,
      "grad_norm": 0.32626765966415405,
      "learning_rate": 0.0001783266605071348,
      "loss": 0.1872,
      "step": 5280
    },
    {
      "epoch": 0.32583923621804745,
      "grad_norm": 0.2726602554321289,
      "learning_rate": 0.00017828559696129762,
      "loss": 0.1895,
      "step": 5290
    },
    {
      "epoch": 0.3264551894056052,
      "grad_norm": 0.24377217888832092,
      "learning_rate": 0.00017824453341546044,
      "loss": 0.1878,
      "step": 5300
    },
    {
      "epoch": 0.32707114259316294,
      "grad_norm": 0.3406885862350464,
      "learning_rate": 0.00017820346986962323,
      "loss": 0.1862,
      "step": 5310
    },
    {
      "epoch": 0.32768709578072064,
      "grad_norm": 0.35156482458114624,
      "learning_rate": 0.00017816240632378608,
      "loss": 0.1863,
      "step": 5320
    },
    {
      "epoch": 0.3283030489682784,
      "grad_norm": 0.2742030620574951,
      "learning_rate": 0.00017812134277794887,
      "loss": 0.1839,
      "step": 5330
    },
    {
      "epoch": 0.32891900215583614,
      "grad_norm": 0.36568114161491394,
      "learning_rate": 0.0001780802792321117,
      "loss": 0.1884,
      "step": 5340
    },
    {
      "epoch": 0.3295349553433939,
      "grad_norm": 0.22875210642814636,
      "learning_rate": 0.0001780392156862745,
      "loss": 0.183,
      "step": 5350
    },
    {
      "epoch": 0.33015090853095164,
      "grad_norm": 0.2562859356403351,
      "learning_rate": 0.00017799815214043733,
      "loss": 0.1844,
      "step": 5360
    },
    {
      "epoch": 0.3307668617185094,
      "grad_norm": 0.22257719933986664,
      "learning_rate": 0.00017795708859460015,
      "loss": 0.1874,
      "step": 5370
    },
    {
      "epoch": 0.33138281490606714,
      "grad_norm": 0.38759058713912964,
      "learning_rate": 0.00017791602504876297,
      "loss": 0.1852,
      "step": 5380
    },
    {
      "epoch": 0.3319987680936249,
      "grad_norm": 0.31661510467529297,
      "learning_rate": 0.0001778749615029258,
      "loss": 0.182,
      "step": 5390
    },
    {
      "epoch": 0.33261472128118263,
      "grad_norm": 0.40725070238113403,
      "learning_rate": 0.0001778338979570886,
      "loss": 0.1887,
      "step": 5400
    },
    {
      "epoch": 0.3332306744687404,
      "grad_norm": 0.3301738500595093,
      "learning_rate": 0.00017779283441125143,
      "loss": 0.1887,
      "step": 5410
    },
    {
      "epoch": 0.33384662765629813,
      "grad_norm": 0.3843924105167389,
      "learning_rate": 0.00017775177086541423,
      "loss": 0.1882,
      "step": 5420
    },
    {
      "epoch": 0.3344625808438559,
      "grad_norm": 0.294622004032135,
      "learning_rate": 0.00017771070731957707,
      "loss": 0.1853,
      "step": 5430
    },
    {
      "epoch": 0.33507853403141363,
      "grad_norm": 0.849892258644104,
      "learning_rate": 0.00017766964377373987,
      "loss": 0.1847,
      "step": 5440
    },
    {
      "epoch": 0.3356944872189714,
      "grad_norm": 0.30655351281166077,
      "learning_rate": 0.00017762858022790269,
      "loss": 0.1846,
      "step": 5450
    },
    {
      "epoch": 0.3363104404065291,
      "grad_norm": 0.6323156356811523,
      "learning_rate": 0.0001775875166820655,
      "loss": 0.1856,
      "step": 5460
    },
    {
      "epoch": 0.3369263935940868,
      "grad_norm": 0.24017657339572906,
      "learning_rate": 0.00017754645313622833,
      "loss": 0.1899,
      "step": 5470
    },
    {
      "epoch": 0.3375423467816446,
      "grad_norm": 0.40626513957977295,
      "learning_rate": 0.00017750538959039115,
      "loss": 0.1837,
      "step": 5480
    },
    {
      "epoch": 0.3381582999692023,
      "grad_norm": 0.4774624705314636,
      "learning_rate": 0.00017746432604455397,
      "loss": 0.1879,
      "step": 5490
    },
    {
      "epoch": 0.3387742531567601,
      "grad_norm": 0.2667333483695984,
      "learning_rate": 0.00017742326249871676,
      "loss": 0.1838,
      "step": 5500
    },
    {
      "epoch": 0.3393902063443178,
      "grad_norm": 0.2797757387161255,
      "learning_rate": 0.0001773821989528796,
      "loss": 0.1844,
      "step": 5510
    },
    {
      "epoch": 0.3400061595318756,
      "grad_norm": 0.2697664499282837,
      "learning_rate": 0.0001773411354070424,
      "loss": 0.1859,
      "step": 5520
    },
    {
      "epoch": 0.3406221127194333,
      "grad_norm": 0.2760717570781708,
      "learning_rate": 0.00017730007186120522,
      "loss": 0.1852,
      "step": 5530
    },
    {
      "epoch": 0.3412380659069911,
      "grad_norm": 0.3318120241165161,
      "learning_rate": 0.00017725900831536804,
      "loss": 0.1851,
      "step": 5540
    },
    {
      "epoch": 0.3418540190945488,
      "grad_norm": 0.22193709015846252,
      "learning_rate": 0.00017721794476953086,
      "loss": 0.18,
      "step": 5550
    },
    {
      "epoch": 0.34246997228210657,
      "grad_norm": 0.3087387979030609,
      "learning_rate": 0.00017717688122369368,
      "loss": 0.1901,
      "step": 5560
    },
    {
      "epoch": 0.3430859254696643,
      "grad_norm": 0.3744651973247528,
      "learning_rate": 0.0001771358176778565,
      "loss": 0.1833,
      "step": 5570
    },
    {
      "epoch": 0.34370187865722207,
      "grad_norm": 0.21127241849899292,
      "learning_rate": 0.0001770947541320193,
      "loss": 0.1792,
      "step": 5580
    },
    {
      "epoch": 0.3443178318447798,
      "grad_norm": 0.31959250569343567,
      "learning_rate": 0.00017705369058618214,
      "loss": 0.1879,
      "step": 5590
    },
    {
      "epoch": 0.34493378503233757,
      "grad_norm": 0.531651496887207,
      "learning_rate": 0.00017701262704034493,
      "loss": 0.1883,
      "step": 5600
    },
    {
      "epoch": 0.34554973821989526,
      "grad_norm": 0.26302844285964966,
      "learning_rate": 0.00017697156349450775,
      "loss": 0.1836,
      "step": 5610
    },
    {
      "epoch": 0.346165691407453,
      "grad_norm": 0.2592516541481018,
      "learning_rate": 0.00017693049994867057,
      "loss": 0.1864,
      "step": 5620
    },
    {
      "epoch": 0.34678164459501076,
      "grad_norm": 0.20447053015232086,
      "learning_rate": 0.0001768894364028334,
      "loss": 0.1863,
      "step": 5630
    },
    {
      "epoch": 0.3473975977825685,
      "grad_norm": 0.23463667929172516,
      "learning_rate": 0.0001768483728569962,
      "loss": 0.1837,
      "step": 5640
    },
    {
      "epoch": 0.34801355097012626,
      "grad_norm": 0.27105921506881714,
      "learning_rate": 0.00017680730931115903,
      "loss": 0.1841,
      "step": 5650
    },
    {
      "epoch": 0.348629504157684,
      "grad_norm": 0.22010910511016846,
      "learning_rate": 0.00017676624576532183,
      "loss": 0.1832,
      "step": 5660
    },
    {
      "epoch": 0.34924545734524176,
      "grad_norm": 0.2943556010723114,
      "learning_rate": 0.00017672518221948467,
      "loss": 0.1851,
      "step": 5670
    },
    {
      "epoch": 0.3498614105327995,
      "grad_norm": 0.2856407165527344,
      "learning_rate": 0.00017668411867364747,
      "loss": 0.1897,
      "step": 5680
    },
    {
      "epoch": 0.35047736372035726,
      "grad_norm": 0.4131998121738434,
      "learning_rate": 0.00017664305512781029,
      "loss": 0.1858,
      "step": 5690
    },
    {
      "epoch": 0.351093316907915,
      "grad_norm": 0.2715918719768524,
      "learning_rate": 0.0001766019915819731,
      "loss": 0.1853,
      "step": 5700
    },
    {
      "epoch": 0.35170927009547276,
      "grad_norm": 0.19757889211177826,
      "learning_rate": 0.00017656092803613593,
      "loss": 0.1832,
      "step": 5710
    },
    {
      "epoch": 0.3523252232830305,
      "grad_norm": 0.26901012659072876,
      "learning_rate": 0.00017651986449029875,
      "loss": 0.1838,
      "step": 5720
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.31468868255615234,
      "learning_rate": 0.00017647880094446157,
      "loss": 0.1808,
      "step": 5730
    },
    {
      "epoch": 0.353557129658146,
      "grad_norm": 0.2442817986011505,
      "learning_rate": 0.00017643773739862436,
      "loss": 0.1809,
      "step": 5740
    },
    {
      "epoch": 0.3541730828457037,
      "grad_norm": 0.2932649850845337,
      "learning_rate": 0.0001763966738527872,
      "loss": 0.1888,
      "step": 5750
    },
    {
      "epoch": 0.35478903603326145,
      "grad_norm": 0.21873222291469574,
      "learning_rate": 0.00017635561030695,
      "loss": 0.1835,
      "step": 5760
    },
    {
      "epoch": 0.3554049892208192,
      "grad_norm": 0.22685720026493073,
      "learning_rate": 0.00017631454676111282,
      "loss": 0.187,
      "step": 5770
    },
    {
      "epoch": 0.35602094240837695,
      "grad_norm": 0.35802170634269714,
      "learning_rate": 0.00017627348321527564,
      "loss": 0.1848,
      "step": 5780
    },
    {
      "epoch": 0.3566368955959347,
      "grad_norm": 0.23221774399280548,
      "learning_rate": 0.00017623241966943846,
      "loss": 0.1874,
      "step": 5790
    },
    {
      "epoch": 0.35725284878349245,
      "grad_norm": 0.33116114139556885,
      "learning_rate": 0.0001761913561236013,
      "loss": 0.1874,
      "step": 5800
    },
    {
      "epoch": 0.3578688019710502,
      "grad_norm": 0.38187870383262634,
      "learning_rate": 0.0001761502925777641,
      "loss": 0.1882,
      "step": 5810
    },
    {
      "epoch": 0.35848475515860795,
      "grad_norm": 0.25106561183929443,
      "learning_rate": 0.00017610922903192692,
      "loss": 0.1899,
      "step": 5820
    },
    {
      "epoch": 0.3591007083461657,
      "grad_norm": 0.37740108370780945,
      "learning_rate": 0.00017606816548608974,
      "loss": 0.1935,
      "step": 5830
    },
    {
      "epoch": 0.35971666153372345,
      "grad_norm": 0.35277649760246277,
      "learning_rate": 0.00017602710194025256,
      "loss": 0.1843,
      "step": 5840
    },
    {
      "epoch": 0.3603326147212812,
      "grad_norm": 0.27950650453567505,
      "learning_rate": 0.00017598603839441535,
      "loss": 0.1822,
      "step": 5850
    },
    {
      "epoch": 0.36094856790883895,
      "grad_norm": 0.28338372707366943,
      "learning_rate": 0.0001759449748485782,
      "loss": 0.1862,
      "step": 5860
    },
    {
      "epoch": 0.3615645210963967,
      "grad_norm": 0.28979969024658203,
      "learning_rate": 0.000175903911302741,
      "loss": 0.1875,
      "step": 5870
    },
    {
      "epoch": 0.36218047428395445,
      "grad_norm": 0.2743379473686218,
      "learning_rate": 0.00017586284775690384,
      "loss": 0.1887,
      "step": 5880
    },
    {
      "epoch": 0.36279642747151214,
      "grad_norm": 0.24915245175361633,
      "learning_rate": 0.00017582178421106663,
      "loss": 0.1831,
      "step": 5890
    },
    {
      "epoch": 0.3634123806590699,
      "grad_norm": 0.44741299748420715,
      "learning_rate": 0.00017578072066522945,
      "loss": 0.1871,
      "step": 5900
    },
    {
      "epoch": 0.36402833384662764,
      "grad_norm": 0.25653722882270813,
      "learning_rate": 0.00017573965711939227,
      "loss": 0.1846,
      "step": 5910
    },
    {
      "epoch": 0.3646442870341854,
      "grad_norm": 0.27046334743499756,
      "learning_rate": 0.0001756985935735551,
      "loss": 0.1876,
      "step": 5920
    },
    {
      "epoch": 0.36526024022174314,
      "grad_norm": 0.3051820695400238,
      "learning_rate": 0.00017565753002771788,
      "loss": 0.1913,
      "step": 5930
    },
    {
      "epoch": 0.3658761934093009,
      "grad_norm": 0.2973060607910156,
      "learning_rate": 0.00017561646648188073,
      "loss": 0.1822,
      "step": 5940
    },
    {
      "epoch": 0.36649214659685864,
      "grad_norm": 0.2885012924671173,
      "learning_rate": 0.00017557540293604352,
      "loss": 0.1861,
      "step": 5950
    },
    {
      "epoch": 0.3671080997844164,
      "grad_norm": 0.25454115867614746,
      "learning_rate": 0.00017553433939020637,
      "loss": 0.1807,
      "step": 5960
    },
    {
      "epoch": 0.36772405297197414,
      "grad_norm": 0.20108796656131744,
      "learning_rate": 0.00017549327584436916,
      "loss": 0.1878,
      "step": 5970
    },
    {
      "epoch": 0.3683400061595319,
      "grad_norm": 0.2317943423986435,
      "learning_rate": 0.00017545221229853198,
      "loss": 0.1864,
      "step": 5980
    },
    {
      "epoch": 0.36895595934708963,
      "grad_norm": 0.38364651799201965,
      "learning_rate": 0.0001754111487526948,
      "loss": 0.1847,
      "step": 5990
    },
    {
      "epoch": 0.3695719125346474,
      "grad_norm": 0.24081669747829437,
      "learning_rate": 0.00017537008520685762,
      "loss": 0.1841,
      "step": 6000
    },
    {
      "epoch": 0.37018786572220513,
      "grad_norm": 0.24314241111278534,
      "learning_rate": 0.00017532902166102045,
      "loss": 0.1866,
      "step": 6010
    },
    {
      "epoch": 0.3708038189097629,
      "grad_norm": 0.2952677309513092,
      "learning_rate": 0.00017528795811518327,
      "loss": 0.1829,
      "step": 6020
    },
    {
      "epoch": 0.3714197720973206,
      "grad_norm": 0.21715666353702545,
      "learning_rate": 0.00017524689456934606,
      "loss": 0.1848,
      "step": 6030
    },
    {
      "epoch": 0.3720357252848783,
      "grad_norm": 0.2286241054534912,
      "learning_rate": 0.0001752058310235089,
      "loss": 0.1808,
      "step": 6040
    },
    {
      "epoch": 0.3726516784724361,
      "grad_norm": 0.3614605963230133,
      "learning_rate": 0.0001751647674776717,
      "loss": 0.1884,
      "step": 6050
    },
    {
      "epoch": 0.3732676316599938,
      "grad_norm": 0.2444905936717987,
      "learning_rate": 0.00017512370393183452,
      "loss": 0.187,
      "step": 6060
    },
    {
      "epoch": 0.3738835848475516,
      "grad_norm": 0.935845136642456,
      "learning_rate": 0.00017508264038599734,
      "loss": 0.1873,
      "step": 6070
    },
    {
      "epoch": 0.3744995380351093,
      "grad_norm": 0.3726356327533722,
      "learning_rate": 0.00017504157684016016,
      "loss": 0.1866,
      "step": 6080
    },
    {
      "epoch": 0.3751154912226671,
      "grad_norm": 0.27260756492614746,
      "learning_rate": 0.00017500051329432298,
      "loss": 0.1895,
      "step": 6090
    },
    {
      "epoch": 0.3757314444102248,
      "grad_norm": 0.29240432381629944,
      "learning_rate": 0.0001749594497484858,
      "loss": 0.1839,
      "step": 6100
    },
    {
      "epoch": 0.3763473975977826,
      "grad_norm": 0.31256335973739624,
      "learning_rate": 0.0001749183862026486,
      "loss": 0.1841,
      "step": 6110
    },
    {
      "epoch": 0.3769633507853403,
      "grad_norm": 0.28143587708473206,
      "learning_rate": 0.00017487732265681144,
      "loss": 0.1875,
      "step": 6120
    },
    {
      "epoch": 0.3775793039728981,
      "grad_norm": 0.2736321687698364,
      "learning_rate": 0.00017483625911097423,
      "loss": 0.1859,
      "step": 6130
    },
    {
      "epoch": 0.3781952571604558,
      "grad_norm": 0.3255535662174225,
      "learning_rate": 0.00017479519556513705,
      "loss": 0.1855,
      "step": 6140
    },
    {
      "epoch": 0.37881121034801357,
      "grad_norm": 0.42086920142173767,
      "learning_rate": 0.00017475413201929987,
      "loss": 0.183,
      "step": 6150
    },
    {
      "epoch": 0.3794271635355713,
      "grad_norm": 0.32416003942489624,
      "learning_rate": 0.0001747130684734627,
      "loss": 0.1858,
      "step": 6160
    },
    {
      "epoch": 0.380043116723129,
      "grad_norm": 0.4102582037448883,
      "learning_rate": 0.0001746720049276255,
      "loss": 0.1834,
      "step": 6170
    },
    {
      "epoch": 0.38065906991068676,
      "grad_norm": 0.22849667072296143,
      "learning_rate": 0.00017463094138178833,
      "loss": 0.1817,
      "step": 6180
    },
    {
      "epoch": 0.3812750230982445,
      "grad_norm": 0.35486361384391785,
      "learning_rate": 0.00017458987783595112,
      "loss": 0.1854,
      "step": 6190
    },
    {
      "epoch": 0.38189097628580226,
      "grad_norm": 0.2714192569255829,
      "learning_rate": 0.00017454881429011397,
      "loss": 0.187,
      "step": 6200
    },
    {
      "epoch": 0.38250692947336,
      "grad_norm": 0.2678023874759674,
      "learning_rate": 0.00017450775074427676,
      "loss": 0.1877,
      "step": 6210
    },
    {
      "epoch": 0.38312288266091776,
      "grad_norm": 0.2952859103679657,
      "learning_rate": 0.00017446668719843958,
      "loss": 0.1834,
      "step": 6220
    },
    {
      "epoch": 0.3837388358484755,
      "grad_norm": 0.3404701054096222,
      "learning_rate": 0.00017442562365260243,
      "loss": 0.1818,
      "step": 6230
    },
    {
      "epoch": 0.38435478903603326,
      "grad_norm": 0.2284456342458725,
      "learning_rate": 0.00017438456010676522,
      "loss": 0.1839,
      "step": 6240
    },
    {
      "epoch": 0.384970742223591,
      "grad_norm": 0.2436991035938263,
      "learning_rate": 0.00017434349656092804,
      "loss": 0.1855,
      "step": 6250
    },
    {
      "epoch": 0.38558669541114876,
      "grad_norm": 0.2406805157661438,
      "learning_rate": 0.00017430243301509086,
      "loss": 0.1876,
      "step": 6260
    },
    {
      "epoch": 0.3862026485987065,
      "grad_norm": 0.29660049080848694,
      "learning_rate": 0.00017426136946925368,
      "loss": 0.1871,
      "step": 6270
    },
    {
      "epoch": 0.38681860178626426,
      "grad_norm": 0.2611672282218933,
      "learning_rate": 0.0001742203059234165,
      "loss": 0.1876,
      "step": 6280
    },
    {
      "epoch": 0.387434554973822,
      "grad_norm": 0.2656401991844177,
      "learning_rate": 0.00017417924237757932,
      "loss": 0.1848,
      "step": 6290
    },
    {
      "epoch": 0.38805050816137976,
      "grad_norm": 0.3000311851501465,
      "learning_rate": 0.00017413817883174212,
      "loss": 0.189,
      "step": 6300
    },
    {
      "epoch": 0.38866646134893745,
      "grad_norm": 0.20713375508785248,
      "learning_rate": 0.00017409711528590496,
      "loss": 0.1841,
      "step": 6310
    },
    {
      "epoch": 0.3892824145364952,
      "grad_norm": 0.2672063112258911,
      "learning_rate": 0.00017405605174006776,
      "loss": 0.1857,
      "step": 6320
    },
    {
      "epoch": 0.38989836772405295,
      "grad_norm": 0.26341888308525085,
      "learning_rate": 0.00017401498819423058,
      "loss": 0.1864,
      "step": 6330
    },
    {
      "epoch": 0.3905143209116107,
      "grad_norm": 0.46235525608062744,
      "learning_rate": 0.0001739739246483934,
      "loss": 0.1816,
      "step": 6340
    },
    {
      "epoch": 0.39113027409916845,
      "grad_norm": 0.29179030656814575,
      "learning_rate": 0.00017393286110255622,
      "loss": 0.1854,
      "step": 6350
    },
    {
      "epoch": 0.3917462272867262,
      "grad_norm": 0.2573920488357544,
      "learning_rate": 0.00017389179755671904,
      "loss": 0.1865,
      "step": 6360
    },
    {
      "epoch": 0.39236218047428395,
      "grad_norm": 0.2423313707113266,
      "learning_rate": 0.00017385073401088186,
      "loss": 0.1851,
      "step": 6370
    },
    {
      "epoch": 0.3929781336618417,
      "grad_norm": 0.18718084692955017,
      "learning_rate": 0.00017380967046504465,
      "loss": 0.1815,
      "step": 6380
    },
    {
      "epoch": 0.39359408684939945,
      "grad_norm": 0.26824960112571716,
      "learning_rate": 0.0001737686069192075,
      "loss": 0.1878,
      "step": 6390
    },
    {
      "epoch": 0.3942100400369572,
      "grad_norm": 0.24344247579574585,
      "learning_rate": 0.0001737275433733703,
      "loss": 0.1885,
      "step": 6400
    },
    {
      "epoch": 0.39482599322451495,
      "grad_norm": 0.270506888628006,
      "learning_rate": 0.0001736864798275331,
      "loss": 0.1834,
      "step": 6410
    },
    {
      "epoch": 0.3954419464120727,
      "grad_norm": 0.37958860397338867,
      "learning_rate": 0.00017364541628169593,
      "loss": 0.1884,
      "step": 6420
    },
    {
      "epoch": 0.39605789959963045,
      "grad_norm": 0.21138784289360046,
      "learning_rate": 0.00017360435273585875,
      "loss": 0.1867,
      "step": 6430
    },
    {
      "epoch": 0.3966738527871882,
      "grad_norm": 0.23473277688026428,
      "learning_rate": 0.00017356328919002157,
      "loss": 0.185,
      "step": 6440
    },
    {
      "epoch": 0.39728980597474595,
      "grad_norm": 0.3065831661224365,
      "learning_rate": 0.0001735222256441844,
      "loss": 0.1869,
      "step": 6450
    },
    {
      "epoch": 0.39790575916230364,
      "grad_norm": 0.28803420066833496,
      "learning_rate": 0.00017348116209834718,
      "loss": 0.1856,
      "step": 6460
    },
    {
      "epoch": 0.3985217123498614,
      "grad_norm": 0.203858882188797,
      "learning_rate": 0.00017344009855251003,
      "loss": 0.1865,
      "step": 6470
    },
    {
      "epoch": 0.39913766553741914,
      "grad_norm": 0.240889310836792,
      "learning_rate": 0.00017339903500667282,
      "loss": 0.1848,
      "step": 6480
    },
    {
      "epoch": 0.3997536187249769,
      "grad_norm": 0.26365020871162415,
      "learning_rate": 0.00017335797146083564,
      "loss": 0.1862,
      "step": 6490
    },
    {
      "epoch": 0.40036957191253464,
      "grad_norm": 0.22241026163101196,
      "learning_rate": 0.00017331690791499846,
      "loss": 0.1855,
      "step": 6500
    },
    {
      "epoch": 0.4009855251000924,
      "grad_norm": 0.33325785398483276,
      "learning_rate": 0.00017327584436916128,
      "loss": 0.1857,
      "step": 6510
    },
    {
      "epoch": 0.40160147828765014,
      "grad_norm": 0.7169989347457886,
      "learning_rate": 0.0001732347808233241,
      "loss": 0.1863,
      "step": 6520
    },
    {
      "epoch": 0.4022174314752079,
      "grad_norm": 0.48580917716026306,
      "learning_rate": 0.00017319371727748692,
      "loss": 0.1832,
      "step": 6530
    },
    {
      "epoch": 0.40283338466276564,
      "grad_norm": 0.2229834496974945,
      "learning_rate": 0.00017315265373164972,
      "loss": 0.1846,
      "step": 6540
    },
    {
      "epoch": 0.4034493378503234,
      "grad_norm": 0.22380897402763367,
      "learning_rate": 0.00017311159018581256,
      "loss": 0.184,
      "step": 6550
    },
    {
      "epoch": 0.40406529103788114,
      "grad_norm": 0.19989190995693207,
      "learning_rate": 0.00017307052663997536,
      "loss": 0.1786,
      "step": 6560
    },
    {
      "epoch": 0.4046812442254389,
      "grad_norm": 0.25622397661209106,
      "learning_rate": 0.00017302946309413818,
      "loss": 0.1854,
      "step": 6570
    },
    {
      "epoch": 0.40529719741299663,
      "grad_norm": 0.2370581328868866,
      "learning_rate": 0.000172988399548301,
      "loss": 0.1859,
      "step": 6580
    },
    {
      "epoch": 0.4059131506005544,
      "grad_norm": 0.2681806683540344,
      "learning_rate": 0.00017294733600246382,
      "loss": 0.1845,
      "step": 6590
    },
    {
      "epoch": 0.4065291037881121,
      "grad_norm": 0.34392085671424866,
      "learning_rate": 0.00017290627245662664,
      "loss": 0.1868,
      "step": 6600
    },
    {
      "epoch": 0.40714505697566983,
      "grad_norm": 0.23880212008953094,
      "learning_rate": 0.00017286520891078946,
      "loss": 0.187,
      "step": 6610
    },
    {
      "epoch": 0.4077610101632276,
      "grad_norm": 0.29196593165397644,
      "learning_rate": 0.00017282414536495225,
      "loss": 0.1819,
      "step": 6620
    },
    {
      "epoch": 0.4083769633507853,
      "grad_norm": 0.5203890800476074,
      "learning_rate": 0.0001727830818191151,
      "loss": 0.188,
      "step": 6630
    },
    {
      "epoch": 0.4089929165383431,
      "grad_norm": 0.28480711579322815,
      "learning_rate": 0.00017274201827327792,
      "loss": 0.1835,
      "step": 6640
    },
    {
      "epoch": 0.4096088697259008,
      "grad_norm": 0.269024133682251,
      "learning_rate": 0.0001727009547274407,
      "loss": 0.1851,
      "step": 6650
    },
    {
      "epoch": 0.4102248229134586,
      "grad_norm": 0.2366727739572525,
      "learning_rate": 0.00017265989118160356,
      "loss": 0.188,
      "step": 6660
    },
    {
      "epoch": 0.4108407761010163,
      "grad_norm": 0.24277758598327637,
      "learning_rate": 0.00017261882763576635,
      "loss": 0.1832,
      "step": 6670
    },
    {
      "epoch": 0.4114567292885741,
      "grad_norm": 0.2734469473361969,
      "learning_rate": 0.00017257776408992917,
      "loss": 0.1861,
      "step": 6680
    },
    {
      "epoch": 0.4120726824761318,
      "grad_norm": 0.2738751769065857,
      "learning_rate": 0.000172536700544092,
      "loss": 0.1866,
      "step": 6690
    },
    {
      "epoch": 0.4126886356636896,
      "grad_norm": 0.23033685982227325,
      "learning_rate": 0.0001724956369982548,
      "loss": 0.1826,
      "step": 6700
    },
    {
      "epoch": 0.4133045888512473,
      "grad_norm": 0.2870630621910095,
      "learning_rate": 0.00017245457345241763,
      "loss": 0.1837,
      "step": 6710
    },
    {
      "epoch": 0.4139205420388051,
      "grad_norm": 0.20478272438049316,
      "learning_rate": 0.00017241350990658045,
      "loss": 0.1796,
      "step": 6720
    },
    {
      "epoch": 0.4145364952263628,
      "grad_norm": 0.4000225067138672,
      "learning_rate": 0.00017237244636074324,
      "loss": 0.1829,
      "step": 6730
    },
    {
      "epoch": 0.4151524484139205,
      "grad_norm": 0.264971524477005,
      "learning_rate": 0.0001723313828149061,
      "loss": 0.1861,
      "step": 6740
    },
    {
      "epoch": 0.41576840160147827,
      "grad_norm": 0.2934792935848236,
      "learning_rate": 0.00017229031926906888,
      "loss": 0.1858,
      "step": 6750
    },
    {
      "epoch": 0.416384354789036,
      "grad_norm": 0.4439096450805664,
      "learning_rate": 0.0001722492557232317,
      "loss": 0.1874,
      "step": 6760
    },
    {
      "epoch": 0.41700030797659376,
      "grad_norm": 0.30870530009269714,
      "learning_rate": 0.00017220819217739452,
      "loss": 0.186,
      "step": 6770
    },
    {
      "epoch": 0.4176162611641515,
      "grad_norm": 0.39021793007850647,
      "learning_rate": 0.00017216712863155734,
      "loss": 0.1833,
      "step": 6780
    },
    {
      "epoch": 0.41823221435170926,
      "grad_norm": 0.25644534826278687,
      "learning_rate": 0.00017212606508572016,
      "loss": 0.1851,
      "step": 6790
    },
    {
      "epoch": 0.418848167539267,
      "grad_norm": 0.20770740509033203,
      "learning_rate": 0.00017208500153988298,
      "loss": 0.1876,
      "step": 6800
    },
    {
      "epoch": 0.41946412072682476,
      "grad_norm": 0.3357515037059784,
      "learning_rate": 0.00017204393799404578,
      "loss": 0.1849,
      "step": 6810
    },
    {
      "epoch": 0.4200800739143825,
      "grad_norm": 0.28309497237205505,
      "learning_rate": 0.00017200287444820862,
      "loss": 0.1827,
      "step": 6820
    },
    {
      "epoch": 0.42069602710194026,
      "grad_norm": 0.2030247300863266,
      "learning_rate": 0.00017196181090237142,
      "loss": 0.1836,
      "step": 6830
    },
    {
      "epoch": 0.421311980289498,
      "grad_norm": 0.20708835124969482,
      "learning_rate": 0.00017192074735653426,
      "loss": 0.1806,
      "step": 6840
    },
    {
      "epoch": 0.42192793347705576,
      "grad_norm": 0.41583141684532166,
      "learning_rate": 0.00017187968381069706,
      "loss": 0.1848,
      "step": 6850
    },
    {
      "epoch": 0.4225438866646135,
      "grad_norm": 0.2780628204345703,
      "learning_rate": 0.00017183862026485988,
      "loss": 0.1813,
      "step": 6860
    },
    {
      "epoch": 0.42315983985217126,
      "grad_norm": 0.2305426001548767,
      "learning_rate": 0.0001717975567190227,
      "loss": 0.1872,
      "step": 6870
    },
    {
      "epoch": 0.42377579303972895,
      "grad_norm": 0.3095856010913849,
      "learning_rate": 0.00017175649317318552,
      "loss": 0.1912,
      "step": 6880
    },
    {
      "epoch": 0.4243917462272867,
      "grad_norm": 0.2929254174232483,
      "learning_rate": 0.0001717154296273483,
      "loss": 0.1849,
      "step": 6890
    },
    {
      "epoch": 0.42500769941484445,
      "grad_norm": 0.3009433150291443,
      "learning_rate": 0.00017167436608151116,
      "loss": 0.1812,
      "step": 6900
    },
    {
      "epoch": 0.4256236526024022,
      "grad_norm": 0.3525885343551636,
      "learning_rate": 0.00017163330253567395,
      "loss": 0.1863,
      "step": 6910
    },
    {
      "epoch": 0.42623960578995995,
      "grad_norm": 0.2329970896244049,
      "learning_rate": 0.0001715922389898368,
      "loss": 0.1846,
      "step": 6920
    },
    {
      "epoch": 0.4268555589775177,
      "grad_norm": 0.24003370106220245,
      "learning_rate": 0.0001715511754439996,
      "loss": 0.1849,
      "step": 6930
    },
    {
      "epoch": 0.42747151216507545,
      "grad_norm": 0.18531043827533722,
      "learning_rate": 0.0001715101118981624,
      "loss": 0.1843,
      "step": 6940
    },
    {
      "epoch": 0.4280874653526332,
      "grad_norm": 0.26676666736602783,
      "learning_rate": 0.00017146904835232523,
      "loss": 0.183,
      "step": 6950
    },
    {
      "epoch": 0.42870341854019095,
      "grad_norm": 0.26523756980895996,
      "learning_rate": 0.00017142798480648805,
      "loss": 0.1886,
      "step": 6960
    },
    {
      "epoch": 0.4293193717277487,
      "grad_norm": 0.3321187198162079,
      "learning_rate": 0.00017138692126065084,
      "loss": 0.1817,
      "step": 6970
    },
    {
      "epoch": 0.42993532491530645,
      "grad_norm": 0.359475314617157,
      "learning_rate": 0.0001713458577148137,
      "loss": 0.1881,
      "step": 6980
    },
    {
      "epoch": 0.4305512781028642,
      "grad_norm": 0.30331510305404663,
      "learning_rate": 0.00017130479416897648,
      "loss": 0.1862,
      "step": 6990
    },
    {
      "epoch": 0.43116723129042195,
      "grad_norm": 0.39106783270835876,
      "learning_rate": 0.00017126783697772304,
      "loss": 0.1907,
      "step": 7000
    },
    {
      "epoch": 0.4317831844779797,
      "grad_norm": 0.26303648948669434,
      "learning_rate": 0.00017122677343188586,
      "loss": 0.1838,
      "step": 7010
    },
    {
      "epoch": 0.4323991376655374,
      "grad_norm": 0.26945239305496216,
      "learning_rate": 0.00017118570988604868,
      "loss": 0.1831,
      "step": 7020
    },
    {
      "epoch": 0.43301509085309514,
      "grad_norm": 0.3368217647075653,
      "learning_rate": 0.00017114464634021147,
      "loss": 0.1823,
      "step": 7030
    },
    {
      "epoch": 0.4336310440406529,
      "grad_norm": 0.38210329413414,
      "learning_rate": 0.00017110358279437432,
      "loss": 0.1872,
      "step": 7040
    },
    {
      "epoch": 0.43424699722821064,
      "grad_norm": 0.2151840180158615,
      "learning_rate": 0.0001710625192485371,
      "loss": 0.1823,
      "step": 7050
    },
    {
      "epoch": 0.4348629504157684,
      "grad_norm": 0.2554590106010437,
      "learning_rate": 0.00017102145570269993,
      "loss": 0.183,
      "step": 7060
    },
    {
      "epoch": 0.43547890360332614,
      "grad_norm": 0.6605768799781799,
      "learning_rate": 0.00017098039215686275,
      "loss": 0.1878,
      "step": 7070
    },
    {
      "epoch": 0.4360948567908839,
      "grad_norm": 0.274428129196167,
      "learning_rate": 0.00017093932861102557,
      "loss": 0.1829,
      "step": 7080
    },
    {
      "epoch": 0.43671080997844164,
      "grad_norm": 0.23417295515537262,
      "learning_rate": 0.0001708982650651884,
      "loss": 0.182,
      "step": 7090
    },
    {
      "epoch": 0.4373267631659994,
      "grad_norm": 0.276339054107666,
      "learning_rate": 0.0001708572015193512,
      "loss": 0.1842,
      "step": 7100
    },
    {
      "epoch": 0.43794271635355714,
      "grad_norm": 0.23742105066776276,
      "learning_rate": 0.000170816137973514,
      "loss": 0.1875,
      "step": 7110
    },
    {
      "epoch": 0.4385586695411149,
      "grad_norm": 0.300851434469223,
      "learning_rate": 0.00017077507442767685,
      "loss": 0.1841,
      "step": 7120
    },
    {
      "epoch": 0.43917462272867264,
      "grad_norm": 0.26732105016708374,
      "learning_rate": 0.00017073401088183964,
      "loss": 0.1807,
      "step": 7130
    },
    {
      "epoch": 0.4397905759162304,
      "grad_norm": 0.45068997144699097,
      "learning_rate": 0.00017069294733600246,
      "loss": 0.1878,
      "step": 7140
    },
    {
      "epoch": 0.44040652910378814,
      "grad_norm": 0.25629502534866333,
      "learning_rate": 0.00017065188379016528,
      "loss": 0.1857,
      "step": 7150
    },
    {
      "epoch": 0.4410224822913459,
      "grad_norm": 0.27869418263435364,
      "learning_rate": 0.0001706108202443281,
      "loss": 0.1857,
      "step": 7160
    },
    {
      "epoch": 0.4416384354789036,
      "grad_norm": 0.2182522565126419,
      "learning_rate": 0.00017056975669849092,
      "loss": 0.1853,
      "step": 7170
    },
    {
      "epoch": 0.44225438866646133,
      "grad_norm": 0.3144987225532532,
      "learning_rate": 0.00017052869315265374,
      "loss": 0.1819,
      "step": 7180
    },
    {
      "epoch": 0.4428703418540191,
      "grad_norm": 0.27051395177841187,
      "learning_rate": 0.00017048762960681653,
      "loss": 0.1863,
      "step": 7190
    },
    {
      "epoch": 0.44348629504157683,
      "grad_norm": 0.19459235668182373,
      "learning_rate": 0.00017044656606097938,
      "loss": 0.1841,
      "step": 7200
    },
    {
      "epoch": 0.4441022482291346,
      "grad_norm": 0.3050008714199066,
      "learning_rate": 0.0001704055025151422,
      "loss": 0.1845,
      "step": 7210
    },
    {
      "epoch": 0.4447182014166923,
      "grad_norm": 0.31507572531700134,
      "learning_rate": 0.000170364438969305,
      "loss": 0.1804,
      "step": 7220
    },
    {
      "epoch": 0.4453341546042501,
      "grad_norm": 0.33686012029647827,
      "learning_rate": 0.00017032337542346784,
      "loss": 0.1817,
      "step": 7230
    },
    {
      "epoch": 0.4459501077918078,
      "grad_norm": 0.1877531260251999,
      "learning_rate": 0.00017028231187763063,
      "loss": 0.1822,
      "step": 7240
    },
    {
      "epoch": 0.4465660609793656,
      "grad_norm": 0.24893638491630554,
      "learning_rate": 0.00017024124833179345,
      "loss": 0.1829,
      "step": 7250
    },
    {
      "epoch": 0.4471820141669233,
      "grad_norm": 0.24480198323726654,
      "learning_rate": 0.00017020018478595627,
      "loss": 0.1817,
      "step": 7260
    },
    {
      "epoch": 0.4477979673544811,
      "grad_norm": 0.21348170936107635,
      "learning_rate": 0.0001701591212401191,
      "loss": 0.1854,
      "step": 7270
    },
    {
      "epoch": 0.4484139205420388,
      "grad_norm": 0.24290642142295837,
      "learning_rate": 0.00017011805769428191,
      "loss": 0.1817,
      "step": 7280
    },
    {
      "epoch": 0.4490298737295966,
      "grad_norm": 0.28926756978034973,
      "learning_rate": 0.00017007699414844473,
      "loss": 0.1836,
      "step": 7290
    },
    {
      "epoch": 0.4496458269171543,
      "grad_norm": 0.23399904370307922,
      "learning_rate": 0.00017003593060260753,
      "loss": 0.1828,
      "step": 7300
    },
    {
      "epoch": 0.450261780104712,
      "grad_norm": 0.24861553311347961,
      "learning_rate": 0.00016999486705677037,
      "loss": 0.1817,
      "step": 7310
    },
    {
      "epoch": 0.45087773329226977,
      "grad_norm": 0.2620607614517212,
      "learning_rate": 0.00016995380351093317,
      "loss": 0.1827,
      "step": 7320
    },
    {
      "epoch": 0.4514936864798275,
      "grad_norm": 0.27399778366088867,
      "learning_rate": 0.00016991273996509601,
      "loss": 0.1854,
      "step": 7330
    },
    {
      "epoch": 0.45210963966738527,
      "grad_norm": 0.25627148151397705,
      "learning_rate": 0.0001698716764192588,
      "loss": 0.1828,
      "step": 7340
    },
    {
      "epoch": 0.452725592854943,
      "grad_norm": 0.34748584032058716,
      "learning_rate": 0.00016983061287342163,
      "loss": 0.183,
      "step": 7350
    },
    {
      "epoch": 0.45334154604250076,
      "grad_norm": 0.26501786708831787,
      "learning_rate": 0.00016978954932758445,
      "loss": 0.1836,
      "step": 7360
    },
    {
      "epoch": 0.4539574992300585,
      "grad_norm": 0.29304203391075134,
      "learning_rate": 0.00016974848578174727,
      "loss": 0.1829,
      "step": 7370
    },
    {
      "epoch": 0.45457345241761626,
      "grad_norm": 0.2327248752117157,
      "learning_rate": 0.00016970742223591006,
      "loss": 0.1813,
      "step": 7380
    },
    {
      "epoch": 0.455189405605174,
      "grad_norm": 0.9017842411994934,
      "learning_rate": 0.0001696663586900729,
      "loss": 0.1885,
      "step": 7390
    },
    {
      "epoch": 0.45580535879273176,
      "grad_norm": 0.21317043900489807,
      "learning_rate": 0.0001696252951442357,
      "loss": 0.1847,
      "step": 7400
    },
    {
      "epoch": 0.4564213119802895,
      "grad_norm": 0.2514781653881073,
      "learning_rate": 0.00016958423159839855,
      "loss": 0.1797,
      "step": 7410
    },
    {
      "epoch": 0.45703726516784726,
      "grad_norm": 0.3257662355899811,
      "learning_rate": 0.00016954316805256134,
      "loss": 0.1896,
      "step": 7420
    },
    {
      "epoch": 0.457653218355405,
      "grad_norm": 0.2324981540441513,
      "learning_rate": 0.00016950210450672416,
      "loss": 0.1839,
      "step": 7430
    },
    {
      "epoch": 0.45826917154296276,
      "grad_norm": 0.20278076827526093,
      "learning_rate": 0.00016946104096088698,
      "loss": 0.1825,
      "step": 7440
    },
    {
      "epoch": 0.45888512473052046,
      "grad_norm": 0.22646595537662506,
      "learning_rate": 0.0001694199774150498,
      "loss": 0.1795,
      "step": 7450
    },
    {
      "epoch": 0.4595010779180782,
      "grad_norm": 0.20788848400115967,
      "learning_rate": 0.0001693789138692126,
      "loss": 0.1842,
      "step": 7460
    },
    {
      "epoch": 0.46011703110563595,
      "grad_norm": 0.1836870312690735,
      "learning_rate": 0.00016933785032337544,
      "loss": 0.1825,
      "step": 7470
    },
    {
      "epoch": 0.4607329842931937,
      "grad_norm": 0.288883775472641,
      "learning_rate": 0.00016929678677753823,
      "loss": 0.1823,
      "step": 7480
    },
    {
      "epoch": 0.46134893748075145,
      "grad_norm": 0.26130029559135437,
      "learning_rate": 0.00016925572323170108,
      "loss": 0.1832,
      "step": 7490
    },
    {
      "epoch": 0.4619648906683092,
      "grad_norm": 0.2442411184310913,
      "learning_rate": 0.00016921465968586387,
      "loss": 0.1891,
      "step": 7500
    },
    {
      "epoch": 0.46258084385586695,
      "grad_norm": 0.35934510827064514,
      "learning_rate": 0.0001691735961400267,
      "loss": 0.1855,
      "step": 7510
    },
    {
      "epoch": 0.4631967970434247,
      "grad_norm": 0.18654555082321167,
      "learning_rate": 0.00016913253259418951,
      "loss": 0.186,
      "step": 7520
    },
    {
      "epoch": 0.46381275023098245,
      "grad_norm": 0.32183751463890076,
      "learning_rate": 0.00016909146904835233,
      "loss": 0.1829,
      "step": 7530
    },
    {
      "epoch": 0.4644287034185402,
      "grad_norm": 0.2955923080444336,
      "learning_rate": 0.00016905040550251515,
      "loss": 0.182,
      "step": 7540
    },
    {
      "epoch": 0.46504465660609795,
      "grad_norm": 0.19115877151489258,
      "learning_rate": 0.00016900934195667797,
      "loss": 0.1857,
      "step": 7550
    },
    {
      "epoch": 0.4656606097936557,
      "grad_norm": 0.24294883012771606,
      "learning_rate": 0.00016896827841084077,
      "loss": 0.1823,
      "step": 7560
    },
    {
      "epoch": 0.46627656298121345,
      "grad_norm": 0.24194678664207458,
      "learning_rate": 0.00016892721486500361,
      "loss": 0.1808,
      "step": 7570
    },
    {
      "epoch": 0.4668925161687712,
      "grad_norm": 0.2596176564693451,
      "learning_rate": 0.0001688861513191664,
      "loss": 0.1825,
      "step": 7580
    },
    {
      "epoch": 0.4675084693563289,
      "grad_norm": 0.5443108081817627,
      "learning_rate": 0.00016884508777332923,
      "loss": 0.1863,
      "step": 7590
    },
    {
      "epoch": 0.46812442254388664,
      "grad_norm": 0.23816193640232086,
      "learning_rate": 0.00016880402422749205,
      "loss": 0.1831,
      "step": 7600
    },
    {
      "epoch": 0.4687403757314444,
      "grad_norm": 0.4621751010417938,
      "learning_rate": 0.00016876296068165487,
      "loss": 0.1846,
      "step": 7610
    },
    {
      "epoch": 0.46935632891900214,
      "grad_norm": 0.2249469757080078,
      "learning_rate": 0.0001687218971358177,
      "loss": 0.1851,
      "step": 7620
    },
    {
      "epoch": 0.4699722821065599,
      "grad_norm": 0.22957397997379303,
      "learning_rate": 0.0001686808335899805,
      "loss": 0.1814,
      "step": 7630
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.21007034182548523,
      "learning_rate": 0.00016863977004414333,
      "loss": 0.1868,
      "step": 7640
    },
    {
      "epoch": 0.4712041884816754,
      "grad_norm": 0.3483479619026184,
      "learning_rate": 0.00016859870649830615,
      "loss": 0.183,
      "step": 7650
    },
    {
      "epoch": 0.47182014166923314,
      "grad_norm": 0.32230517268180847,
      "learning_rate": 0.00016855764295246897,
      "loss": 0.186,
      "step": 7660
    },
    {
      "epoch": 0.4724360948567909,
      "grad_norm": 0.23749352991580963,
      "learning_rate": 0.00016851657940663176,
      "loss": 0.1849,
      "step": 7670
    },
    {
      "epoch": 0.47305204804434864,
      "grad_norm": 0.25006017088890076,
      "learning_rate": 0.0001684755158607946,
      "loss": 0.1835,
      "step": 7680
    },
    {
      "epoch": 0.4736680012319064,
      "grad_norm": 0.2393067479133606,
      "learning_rate": 0.0001684344523149574,
      "loss": 0.1859,
      "step": 7690
    },
    {
      "epoch": 0.47428395441946414,
      "grad_norm": 0.2537294030189514,
      "learning_rate": 0.00016839338876912022,
      "loss": 0.1845,
      "step": 7700
    },
    {
      "epoch": 0.4748999076070219,
      "grad_norm": 0.18434764444828033,
      "learning_rate": 0.00016835232522328304,
      "loss": 0.1804,
      "step": 7710
    },
    {
      "epoch": 0.47551586079457964,
      "grad_norm": 0.5034879446029663,
      "learning_rate": 0.00016831126167744586,
      "loss": 0.1832,
      "step": 7720
    },
    {
      "epoch": 0.47613181398213733,
      "grad_norm": 0.24722608923912048,
      "learning_rate": 0.00016827019813160868,
      "loss": 0.182,
      "step": 7730
    },
    {
      "epoch": 0.4767477671696951,
      "grad_norm": 0.20563361048698425,
      "learning_rate": 0.0001682291345857715,
      "loss": 0.1821,
      "step": 7740
    },
    {
      "epoch": 0.47736372035725283,
      "grad_norm": 0.28182077407836914,
      "learning_rate": 0.0001681880710399343,
      "loss": 0.1845,
      "step": 7750
    },
    {
      "epoch": 0.4779796735448106,
      "grad_norm": 0.23344044387340546,
      "learning_rate": 0.00016814700749409714,
      "loss": 0.1846,
      "step": 7760
    },
    {
      "epoch": 0.47859562673236833,
      "grad_norm": 0.20055194199085236,
      "learning_rate": 0.00016810594394825993,
      "loss": 0.1844,
      "step": 7770
    },
    {
      "epoch": 0.4792115799199261,
      "grad_norm": 0.32937419414520264,
      "learning_rate": 0.00016806488040242275,
      "loss": 0.1834,
      "step": 7780
    },
    {
      "epoch": 0.47982753310748383,
      "grad_norm": 0.2766362428665161,
      "learning_rate": 0.00016802381685658557,
      "loss": 0.1839,
      "step": 7790
    },
    {
      "epoch": 0.4804434862950416,
      "grad_norm": 0.2295732945203781,
      "learning_rate": 0.0001679827533107484,
      "loss": 0.1854,
      "step": 7800
    },
    {
      "epoch": 0.4810594394825993,
      "grad_norm": 0.17816168069839478,
      "learning_rate": 0.00016794168976491121,
      "loss": 0.1834,
      "step": 7810
    },
    {
      "epoch": 0.4816753926701571,
      "grad_norm": 0.34157806634902954,
      "learning_rate": 0.00016790062621907403,
      "loss": 0.1876,
      "step": 7820
    },
    {
      "epoch": 0.4822913458577148,
      "grad_norm": 0.41263246536254883,
      "learning_rate": 0.00016785956267323683,
      "loss": 0.1858,
      "step": 7830
    },
    {
      "epoch": 0.4829072990452726,
      "grad_norm": 0.23159712553024292,
      "learning_rate": 0.00016781849912739967,
      "loss": 0.185,
      "step": 7840
    },
    {
      "epoch": 0.4835232522328303,
      "grad_norm": 0.24926061928272247,
      "learning_rate": 0.00016777743558156247,
      "loss": 0.1795,
      "step": 7850
    },
    {
      "epoch": 0.4841392054203881,
      "grad_norm": 0.31401434540748596,
      "learning_rate": 0.0001677363720357253,
      "loss": 0.1811,
      "step": 7860
    },
    {
      "epoch": 0.48475515860794577,
      "grad_norm": 0.22676534950733185,
      "learning_rate": 0.0001676953084898881,
      "loss": 0.1827,
      "step": 7870
    },
    {
      "epoch": 0.4853711117955035,
      "grad_norm": 0.21525050699710846,
      "learning_rate": 0.00016765424494405093,
      "loss": 0.1822,
      "step": 7880
    },
    {
      "epoch": 0.48598706498306127,
      "grad_norm": 0.22967475652694702,
      "learning_rate": 0.00016761318139821375,
      "loss": 0.1829,
      "step": 7890
    },
    {
      "epoch": 0.486603018170619,
      "grad_norm": 0.24708901345729828,
      "learning_rate": 0.00016757211785237657,
      "loss": 0.1858,
      "step": 7900
    },
    {
      "epoch": 0.48721897135817677,
      "grad_norm": 0.18945841491222382,
      "learning_rate": 0.00016753105430653936,
      "loss": 0.187,
      "step": 7910
    },
    {
      "epoch": 0.4878349245457345,
      "grad_norm": 0.20787259936332703,
      "learning_rate": 0.0001674899907607022,
      "loss": 0.1812,
      "step": 7920
    },
    {
      "epoch": 0.48845087773329227,
      "grad_norm": 0.24597522616386414,
      "learning_rate": 0.000167448927214865,
      "loss": 0.1837,
      "step": 7930
    },
    {
      "epoch": 0.48906683092085,
      "grad_norm": 0.2755401134490967,
      "learning_rate": 0.00016740786366902782,
      "loss": 0.1824,
      "step": 7940
    },
    {
      "epoch": 0.48968278410840776,
      "grad_norm": 0.24930013716220856,
      "learning_rate": 0.00016736680012319064,
      "loss": 0.1804,
      "step": 7950
    },
    {
      "epoch": 0.4902987372959655,
      "grad_norm": 0.3214817941188812,
      "learning_rate": 0.00016732573657735346,
      "loss": 0.1834,
      "step": 7960
    },
    {
      "epoch": 0.49091469048352326,
      "grad_norm": 0.25444769859313965,
      "learning_rate": 0.00016728467303151628,
      "loss": 0.1841,
      "step": 7970
    },
    {
      "epoch": 0.491530643671081,
      "grad_norm": 0.4875706136226654,
      "learning_rate": 0.0001672436094856791,
      "loss": 0.1844,
      "step": 7980
    },
    {
      "epoch": 0.49214659685863876,
      "grad_norm": 0.24839237332344055,
      "learning_rate": 0.0001672025459398419,
      "loss": 0.1832,
      "step": 7990
    },
    {
      "epoch": 0.4927625500461965,
      "grad_norm": 0.19058012962341309,
      "learning_rate": 0.00016716148239400474,
      "loss": 0.1796,
      "step": 8000
    },
    {
      "epoch": 0.49337850323375426,
      "grad_norm": 0.2810022532939911,
      "learning_rate": 0.00016712041884816753,
      "loss": 0.1832,
      "step": 8010
    },
    {
      "epoch": 0.49399445642131196,
      "grad_norm": 0.21890898048877716,
      "learning_rate": 0.00016707935530233035,
      "loss": 0.1839,
      "step": 8020
    },
    {
      "epoch": 0.4946104096088697,
      "grad_norm": 0.20863553881645203,
      "learning_rate": 0.00016703829175649317,
      "loss": 0.1833,
      "step": 8030
    },
    {
      "epoch": 0.49522636279642746,
      "grad_norm": 0.29498183727264404,
      "learning_rate": 0.000166997228210656,
      "loss": 0.1837,
      "step": 8040
    },
    {
      "epoch": 0.4958423159839852,
      "grad_norm": 0.31859540939331055,
      "learning_rate": 0.00016695616466481881,
      "loss": 0.1827,
      "step": 8050
    },
    {
      "epoch": 0.49645826917154295,
      "grad_norm": 0.22306369245052338,
      "learning_rate": 0.00016691510111898163,
      "loss": 0.1841,
      "step": 8060
    },
    {
      "epoch": 0.4970742223591007,
      "grad_norm": 0.23054248094558716,
      "learning_rate": 0.00016687403757314445,
      "loss": 0.1826,
      "step": 8070
    },
    {
      "epoch": 0.49769017554665845,
      "grad_norm": 0.20478928089141846,
      "learning_rate": 0.00016683297402730727,
      "loss": 0.181,
      "step": 8080
    },
    {
      "epoch": 0.4983061287342162,
      "grad_norm": 0.21811725199222565,
      "learning_rate": 0.0001667919104814701,
      "loss": 0.1851,
      "step": 8090
    },
    {
      "epoch": 0.49892208192177395,
      "grad_norm": 0.24999675154685974,
      "learning_rate": 0.0001667508469356329,
      "loss": 0.1788,
      "step": 8100
    },
    {
      "epoch": 0.4995380351093317,
      "grad_norm": 0.23441225290298462,
      "learning_rate": 0.00016670978338979573,
      "loss": 0.1841,
      "step": 8110
    },
    {
      "epoch": 0.5001539882968894,
      "grad_norm": 0.20621860027313232,
      "learning_rate": 0.00016666871984395853,
      "loss": 0.1812,
      "step": 8120
    },
    {
      "epoch": 0.5007699414844472,
      "grad_norm": 0.30409273505210876,
      "learning_rate": 0.00016662765629812135,
      "loss": 0.1888,
      "step": 8130
    },
    {
      "epoch": 0.5013858946720049,
      "grad_norm": 0.21660307049751282,
      "learning_rate": 0.00016658659275228417,
      "loss": 0.1856,
      "step": 8140
    },
    {
      "epoch": 0.5020018478595627,
      "grad_norm": 0.21759307384490967,
      "learning_rate": 0.000166545529206447,
      "loss": 0.1833,
      "step": 8150
    },
    {
      "epoch": 0.5026178010471204,
      "grad_norm": 0.26686611771583557,
      "learning_rate": 0.0001665044656606098,
      "loss": 0.185,
      "step": 8160
    },
    {
      "epoch": 0.5032337542346782,
      "grad_norm": 0.22441153228282928,
      "learning_rate": 0.00016646340211477263,
      "loss": 0.1809,
      "step": 8170
    },
    {
      "epoch": 0.5038497074222359,
      "grad_norm": 0.26133161783218384,
      "learning_rate": 0.00016642233856893542,
      "loss": 0.1823,
      "step": 8180
    },
    {
      "epoch": 0.5044656606097937,
      "grad_norm": 0.2810998260974884,
      "learning_rate": 0.00016638127502309827,
      "loss": 0.1867,
      "step": 8190
    },
    {
      "epoch": 0.5050816137973514,
      "grad_norm": 0.25087621808052063,
      "learning_rate": 0.00016634021147726106,
      "loss": 0.1792,
      "step": 8200
    },
    {
      "epoch": 0.5056975669849092,
      "grad_norm": 0.19634757936000824,
      "learning_rate": 0.00016629914793142388,
      "loss": 0.1828,
      "step": 8210
    },
    {
      "epoch": 0.5063135201724669,
      "grad_norm": 0.20151863992214203,
      "learning_rate": 0.0001662580843855867,
      "loss": 0.1844,
      "step": 8220
    },
    {
      "epoch": 0.5069294733600246,
      "grad_norm": 0.20709821581840515,
      "learning_rate": 0.00016621702083974952,
      "loss": 0.1788,
      "step": 8230
    },
    {
      "epoch": 0.5075454265475824,
      "grad_norm": 0.7027472853660583,
      "learning_rate": 0.00016617595729391234,
      "loss": 0.1835,
      "step": 8240
    },
    {
      "epoch": 0.5081613797351401,
      "grad_norm": 0.23784835636615753,
      "learning_rate": 0.00016613489374807516,
      "loss": 0.1836,
      "step": 8250
    },
    {
      "epoch": 0.5087773329226979,
      "grad_norm": 0.1785411685705185,
      "learning_rate": 0.00016609383020223795,
      "loss": 0.1847,
      "step": 8260
    },
    {
      "epoch": 0.5093932861102556,
      "grad_norm": 0.26940426230430603,
      "learning_rate": 0.0001660527666564008,
      "loss": 0.1799,
      "step": 8270
    },
    {
      "epoch": 0.5100092392978134,
      "grad_norm": 0.22220318019390106,
      "learning_rate": 0.0001660117031105636,
      "loss": 0.1798,
      "step": 8280
    },
    {
      "epoch": 0.5106251924853711,
      "grad_norm": 0.2973710298538208,
      "learning_rate": 0.0001659706395647264,
      "loss": 0.1844,
      "step": 8290
    },
    {
      "epoch": 0.5112411456729289,
      "grad_norm": 0.28732985258102417,
      "learning_rate": 0.00016592957601888923,
      "loss": 0.1857,
      "step": 8300
    },
    {
      "epoch": 0.5118570988604866,
      "grad_norm": 0.2253437340259552,
      "learning_rate": 0.00016588851247305205,
      "loss": 0.1834,
      "step": 8310
    },
    {
      "epoch": 0.5124730520480444,
      "grad_norm": 0.21760457754135132,
      "learning_rate": 0.00016584744892721487,
      "loss": 0.1775,
      "step": 8320
    },
    {
      "epoch": 0.5130890052356021,
      "grad_norm": 0.24844695627689362,
      "learning_rate": 0.0001658063853813777,
      "loss": 0.1806,
      "step": 8330
    },
    {
      "epoch": 0.5137049584231599,
      "grad_norm": 0.2803311049938202,
      "learning_rate": 0.00016576532183554049,
      "loss": 0.184,
      "step": 8340
    },
    {
      "epoch": 0.5143209116107176,
      "grad_norm": 0.25742876529693604,
      "learning_rate": 0.00016572425828970333,
      "loss": 0.1799,
      "step": 8350
    },
    {
      "epoch": 0.5149368647982754,
      "grad_norm": 0.30021965503692627,
      "learning_rate": 0.00016568319474386613,
      "loss": 0.184,
      "step": 8360
    },
    {
      "epoch": 0.5155528179858331,
      "grad_norm": 0.23853318393230438,
      "learning_rate": 0.00016564213119802897,
      "loss": 0.1843,
      "step": 8370
    },
    {
      "epoch": 0.5161687711733908,
      "grad_norm": 0.26535817980766296,
      "learning_rate": 0.00016560106765219177,
      "loss": 0.1825,
      "step": 8380
    },
    {
      "epoch": 0.5167847243609486,
      "grad_norm": 0.34886932373046875,
      "learning_rate": 0.0001655600041063546,
      "loss": 0.1856,
      "step": 8390
    },
    {
      "epoch": 0.5174006775485063,
      "grad_norm": 0.3292428255081177,
      "learning_rate": 0.0001655189405605174,
      "loss": 0.1931,
      "step": 8400
    },
    {
      "epoch": 0.5180166307360641,
      "grad_norm": 0.1869911253452301,
      "learning_rate": 0.00016547787701468023,
      "loss": 0.1811,
      "step": 8410
    },
    {
      "epoch": 0.5186325839236218,
      "grad_norm": 0.21664106845855713,
      "learning_rate": 0.00016543681346884302,
      "loss": 0.1848,
      "step": 8420
    },
    {
      "epoch": 0.5192485371111796,
      "grad_norm": 0.5367712378501892,
      "learning_rate": 0.00016539574992300587,
      "loss": 0.1842,
      "step": 8430
    },
    {
      "epoch": 0.5198644902987373,
      "grad_norm": 0.25237953662872314,
      "learning_rate": 0.00016535468637716866,
      "loss": 0.1836,
      "step": 8440
    },
    {
      "epoch": 0.5204804434862951,
      "grad_norm": 0.2664877772331238,
      "learning_rate": 0.0001653136228313315,
      "loss": 0.1854,
      "step": 8450
    },
    {
      "epoch": 0.5210963966738528,
      "grad_norm": 0.2832716405391693,
      "learning_rate": 0.0001652725592854943,
      "loss": 0.183,
      "step": 8460
    },
    {
      "epoch": 0.5217123498614106,
      "grad_norm": 0.24476215243339539,
      "learning_rate": 0.00016523149573965712,
      "loss": 0.1802,
      "step": 8470
    },
    {
      "epoch": 0.5223283030489683,
      "grad_norm": 0.2814907133579254,
      "learning_rate": 0.00016519043219381997,
      "loss": 0.1827,
      "step": 8480
    },
    {
      "epoch": 0.5229442562365261,
      "grad_norm": 0.35650238394737244,
      "learning_rate": 0.00016514936864798276,
      "loss": 0.183,
      "step": 8490
    },
    {
      "epoch": 0.5235602094240838,
      "grad_norm": 0.22752542793750763,
      "learning_rate": 0.00016510830510214558,
      "loss": 0.1828,
      "step": 8500
    },
    {
      "epoch": 0.5241761626116415,
      "grad_norm": 1.0619288682937622,
      "learning_rate": 0.0001650672415563084,
      "loss": 0.1844,
      "step": 8510
    },
    {
      "epoch": 0.5247921157991993,
      "grad_norm": 0.24769452214241028,
      "learning_rate": 0.00016502617801047122,
      "loss": 0.1822,
      "step": 8520
    },
    {
      "epoch": 0.525408068986757,
      "grad_norm": 0.2028767168521881,
      "learning_rate": 0.00016498511446463404,
      "loss": 0.1799,
      "step": 8530
    },
    {
      "epoch": 0.5260240221743148,
      "grad_norm": 0.2735450267791748,
      "learning_rate": 0.00016494405091879686,
      "loss": 0.1886,
      "step": 8540
    },
    {
      "epoch": 0.5266399753618725,
      "grad_norm": 0.21598704159259796,
      "learning_rate": 0.00016490298737295965,
      "loss": 0.1845,
      "step": 8550
    },
    {
      "epoch": 0.5272559285494303,
      "grad_norm": 0.2965559959411621,
      "learning_rate": 0.0001648619238271225,
      "loss": 0.1828,
      "step": 8560
    },
    {
      "epoch": 0.527871881736988,
      "grad_norm": 0.2927386462688446,
      "learning_rate": 0.0001648208602812853,
      "loss": 0.1856,
      "step": 8570
    },
    {
      "epoch": 0.5284878349245458,
      "grad_norm": 0.2513594627380371,
      "learning_rate": 0.0001647797967354481,
      "loss": 0.1835,
      "step": 8580
    },
    {
      "epoch": 0.5291037881121035,
      "grad_norm": 0.2681635320186615,
      "learning_rate": 0.00016473873318961093,
      "loss": 0.1823,
      "step": 8590
    },
    {
      "epoch": 0.5297197412996613,
      "grad_norm": 0.22533497214317322,
      "learning_rate": 0.00016469766964377375,
      "loss": 0.1822,
      "step": 8600
    },
    {
      "epoch": 0.530335694487219,
      "grad_norm": 0.22815203666687012,
      "learning_rate": 0.00016465660609793657,
      "loss": 0.1829,
      "step": 8610
    },
    {
      "epoch": 0.5309516476747768,
      "grad_norm": 0.2411622852087021,
      "learning_rate": 0.0001646155425520994,
      "loss": 0.1819,
      "step": 8620
    },
    {
      "epoch": 0.5315676008623345,
      "grad_norm": 0.1802995204925537,
      "learning_rate": 0.00016457447900626219,
      "loss": 0.1799,
      "step": 8630
    },
    {
      "epoch": 0.5321835540498923,
      "grad_norm": 0.22336119413375854,
      "learning_rate": 0.00016453341546042503,
      "loss": 0.1797,
      "step": 8640
    },
    {
      "epoch": 0.53279950723745,
      "grad_norm": 0.27948781847953796,
      "learning_rate": 0.00016449235191458783,
      "loss": 0.187,
      "step": 8650
    },
    {
      "epoch": 0.5334154604250076,
      "grad_norm": 0.18450795114040375,
      "learning_rate": 0.00016445128836875065,
      "loss": 0.1841,
      "step": 8660
    },
    {
      "epoch": 0.5340314136125655,
      "grad_norm": 0.19301681220531464,
      "learning_rate": 0.00016441022482291347,
      "loss": 0.1825,
      "step": 8670
    },
    {
      "epoch": 0.5346473668001231,
      "grad_norm": 0.21260276436805725,
      "learning_rate": 0.00016436916127707629,
      "loss": 0.1829,
      "step": 8680
    },
    {
      "epoch": 0.535263319987681,
      "grad_norm": 0.30771738290786743,
      "learning_rate": 0.0001643280977312391,
      "loss": 0.1875,
      "step": 8690
    },
    {
      "epoch": 0.5358792731752386,
      "grad_norm": 0.20415827631950378,
      "learning_rate": 0.00016428703418540193,
      "loss": 0.1818,
      "step": 8700
    },
    {
      "epoch": 0.5364952263627965,
      "grad_norm": 0.22971944510936737,
      "learning_rate": 0.00016424597063956472,
      "loss": 0.1836,
      "step": 8710
    },
    {
      "epoch": 0.5371111795503541,
      "grad_norm": 0.21112170815467834,
      "learning_rate": 0.00016420490709372757,
      "loss": 0.1816,
      "step": 8720
    },
    {
      "epoch": 0.537727132737912,
      "grad_norm": 0.22295455634593964,
      "learning_rate": 0.00016416384354789036,
      "loss": 0.179,
      "step": 8730
    },
    {
      "epoch": 0.5383430859254696,
      "grad_norm": 0.25543853640556335,
      "learning_rate": 0.00016412278000205318,
      "loss": 0.1811,
      "step": 8740
    },
    {
      "epoch": 0.5389590391130274,
      "grad_norm": 0.2072879523038864,
      "learning_rate": 0.000164081716456216,
      "loss": 0.1831,
      "step": 8750
    },
    {
      "epoch": 0.5395749923005851,
      "grad_norm": 0.20303748548030853,
      "learning_rate": 0.00016404065291037882,
      "loss": 0.1816,
      "step": 8760
    },
    {
      "epoch": 0.540190945488143,
      "grad_norm": 0.28430989384651184,
      "learning_rate": 0.00016399958936454164,
      "loss": 0.1852,
      "step": 8770
    },
    {
      "epoch": 0.5408068986757006,
      "grad_norm": 0.22097527980804443,
      "learning_rate": 0.00016395852581870446,
      "loss": 0.1796,
      "step": 8780
    },
    {
      "epoch": 0.5414228518632584,
      "grad_norm": 0.19715286791324615,
      "learning_rate": 0.00016391746227286725,
      "loss": 0.1792,
      "step": 8790
    },
    {
      "epoch": 0.5420388050508161,
      "grad_norm": 0.1964791864156723,
      "learning_rate": 0.0001638763987270301,
      "loss": 0.1826,
      "step": 8800
    },
    {
      "epoch": 0.5426547582383738,
      "grad_norm": 0.21240410208702087,
      "learning_rate": 0.0001638353351811929,
      "loss": 0.1848,
      "step": 8810
    },
    {
      "epoch": 0.5432707114259316,
      "grad_norm": 0.22179704904556274,
      "learning_rate": 0.0001637942716353557,
      "loss": 0.1838,
      "step": 8820
    },
    {
      "epoch": 0.5438866646134893,
      "grad_norm": 0.2144203931093216,
      "learning_rate": 0.00016375320808951853,
      "loss": 0.1838,
      "step": 8830
    },
    {
      "epoch": 0.5445026178010471,
      "grad_norm": 0.2139778435230255,
      "learning_rate": 0.00016371214454368135,
      "loss": 0.1859,
      "step": 8840
    },
    {
      "epoch": 0.5451185709886048,
      "grad_norm": 0.21261164546012878,
      "learning_rate": 0.00016367108099784417,
      "loss": 0.1844,
      "step": 8850
    },
    {
      "epoch": 0.5457345241761626,
      "grad_norm": 0.20830902457237244,
      "learning_rate": 0.000163630017452007,
      "loss": 0.18,
      "step": 8860
    },
    {
      "epoch": 0.5463504773637203,
      "grad_norm": 0.23622499406337738,
      "learning_rate": 0.00016358895390616979,
      "loss": 0.1823,
      "step": 8870
    },
    {
      "epoch": 0.5469664305512781,
      "grad_norm": 0.47706371545791626,
      "learning_rate": 0.00016354789036033263,
      "loss": 0.1834,
      "step": 8880
    },
    {
      "epoch": 0.5475823837388358,
      "grad_norm": 0.17928099632263184,
      "learning_rate": 0.00016350682681449543,
      "loss": 0.1799,
      "step": 8890
    },
    {
      "epoch": 0.5481983369263936,
      "grad_norm": 0.23545078933238983,
      "learning_rate": 0.00016346576326865825,
      "loss": 0.1828,
      "step": 8900
    },
    {
      "epoch": 0.5488142901139513,
      "grad_norm": 0.22393186390399933,
      "learning_rate": 0.0001634246997228211,
      "loss": 0.1822,
      "step": 8910
    },
    {
      "epoch": 0.5494302433015091,
      "grad_norm": 0.2014436572790146,
      "learning_rate": 0.00016338363617698389,
      "loss": 0.1828,
      "step": 8920
    },
    {
      "epoch": 0.5500461964890668,
      "grad_norm": 0.2075520008802414,
      "learning_rate": 0.0001633425726311467,
      "loss": 0.1815,
      "step": 8930
    },
    {
      "epoch": 0.5506621496766245,
      "grad_norm": 0.26926189661026,
      "learning_rate": 0.00016330150908530953,
      "loss": 0.1843,
      "step": 8940
    },
    {
      "epoch": 0.5512781028641823,
      "grad_norm": 0.30062609910964966,
      "learning_rate": 0.00016326044553947235,
      "loss": 0.1851,
      "step": 8950
    },
    {
      "epoch": 0.55189405605174,
      "grad_norm": 0.20029067993164062,
      "learning_rate": 0.00016321938199363517,
      "loss": 0.1815,
      "step": 8960
    },
    {
      "epoch": 0.5525100092392978,
      "grad_norm": 0.434834748506546,
      "learning_rate": 0.00016317831844779799,
      "loss": 0.1815,
      "step": 8970
    },
    {
      "epoch": 0.5531259624268555,
      "grad_norm": 0.1871042251586914,
      "learning_rate": 0.00016313725490196078,
      "loss": 0.1818,
      "step": 8980
    },
    {
      "epoch": 0.5537419156144133,
      "grad_norm": 0.2561739981174469,
      "learning_rate": 0.00016309619135612363,
      "loss": 0.1846,
      "step": 8990
    },
    {
      "epoch": 0.554357868801971,
      "grad_norm": 0.18857353925704956,
      "learning_rate": 0.00016305512781028642,
      "loss": 0.1846,
      "step": 9000
    },
    {
      "epoch": 0.5549738219895288,
      "grad_norm": 0.2745925784111023,
      "learning_rate": 0.00016301406426444924,
      "loss": 0.1827,
      "step": 9010
    },
    {
      "epoch": 0.5555897751770865,
      "grad_norm": 0.18449628353118896,
      "learning_rate": 0.00016297300071861206,
      "loss": 0.1816,
      "step": 9020
    },
    {
      "epoch": 0.5562057283646443,
      "grad_norm": 0.23035329580307007,
      "learning_rate": 0.00016293193717277488,
      "loss": 0.181,
      "step": 9030
    },
    {
      "epoch": 0.556821681552202,
      "grad_norm": 0.18725228309631348,
      "learning_rate": 0.0001628908736269377,
      "loss": 0.182,
      "step": 9040
    },
    {
      "epoch": 0.5574376347397598,
      "grad_norm": 0.21313802897930145,
      "learning_rate": 0.00016284981008110052,
      "loss": 0.1808,
      "step": 9050
    },
    {
      "epoch": 0.5580535879273175,
      "grad_norm": 0.16341544687747955,
      "learning_rate": 0.0001628087465352633,
      "loss": 0.1807,
      "step": 9060
    },
    {
      "epoch": 0.5586695411148753,
      "grad_norm": 0.2982432544231415,
      "learning_rate": 0.00016276768298942616,
      "loss": 0.181,
      "step": 9070
    },
    {
      "epoch": 0.559285494302433,
      "grad_norm": 0.26214340329170227,
      "learning_rate": 0.00016273072579817268,
      "loss": 0.182,
      "step": 9080
    },
    {
      "epoch": 0.5599014474899907,
      "grad_norm": 0.19031360745429993,
      "learning_rate": 0.0001626896622523355,
      "loss": 0.1819,
      "step": 9090
    },
    {
      "epoch": 0.5605174006775485,
      "grad_norm": 0.19470496475696564,
      "learning_rate": 0.00016264859870649832,
      "loss": 0.1786,
      "step": 9100
    },
    {
      "epoch": 0.5611333538651062,
      "grad_norm": 0.19198089838027954,
      "learning_rate": 0.00016260753516066114,
      "loss": 0.1871,
      "step": 9110
    },
    {
      "epoch": 0.561749307052664,
      "grad_norm": 0.22939465939998627,
      "learning_rate": 0.00016256647161482394,
      "loss": 0.1836,
      "step": 9120
    },
    {
      "epoch": 0.5623652602402217,
      "grad_norm": 0.21142372488975525,
      "learning_rate": 0.00016252540806898678,
      "loss": 0.1809,
      "step": 9130
    },
    {
      "epoch": 0.5629812134277795,
      "grad_norm": 0.30041173100471497,
      "learning_rate": 0.00016248434452314958,
      "loss": 0.1789,
      "step": 9140
    },
    {
      "epoch": 0.5635971666153372,
      "grad_norm": 0.2927030026912689,
      "learning_rate": 0.0001624432809773124,
      "loss": 0.185,
      "step": 9150
    },
    {
      "epoch": 0.564213119802895,
      "grad_norm": 0.1934765875339508,
      "learning_rate": 0.00016240221743147522,
      "loss": 0.1803,
      "step": 9160
    },
    {
      "epoch": 0.5648290729904527,
      "grad_norm": 0.23709280788898468,
      "learning_rate": 0.00016236115388563804,
      "loss": 0.1817,
      "step": 9170
    },
    {
      "epoch": 0.5654450261780105,
      "grad_norm": 0.2003851979970932,
      "learning_rate": 0.00016232009033980086,
      "loss": 0.1821,
      "step": 9180
    },
    {
      "epoch": 0.5660609793655682,
      "grad_norm": 0.20875971019268036,
      "learning_rate": 0.00016227902679396368,
      "loss": 0.1813,
      "step": 9190
    },
    {
      "epoch": 0.566676932553126,
      "grad_norm": 0.22864580154418945,
      "learning_rate": 0.00016223796324812647,
      "loss": 0.1807,
      "step": 9200
    },
    {
      "epoch": 0.5672928857406837,
      "grad_norm": 0.20016421377658844,
      "learning_rate": 0.00016219689970228932,
      "loss": 0.1848,
      "step": 9210
    },
    {
      "epoch": 0.5679088389282414,
      "grad_norm": 0.23760463297367096,
      "learning_rate": 0.0001621558361564521,
      "loss": 0.1834,
      "step": 9220
    },
    {
      "epoch": 0.5685247921157992,
      "grad_norm": 0.18444398045539856,
      "learning_rate": 0.00016211477261061493,
      "loss": 0.1814,
      "step": 9230
    },
    {
      "epoch": 0.5691407453033569,
      "grad_norm": 0.24739579856395721,
      "learning_rate": 0.00016207370906477775,
      "loss": 0.1837,
      "step": 9240
    },
    {
      "epoch": 0.5697566984909147,
      "grad_norm": 0.23306100070476532,
      "learning_rate": 0.00016203264551894057,
      "loss": 0.1814,
      "step": 9250
    },
    {
      "epoch": 0.5703726516784724,
      "grad_norm": 0.21343456208705902,
      "learning_rate": 0.0001619915819731034,
      "loss": 0.1801,
      "step": 9260
    },
    {
      "epoch": 0.5709886048660302,
      "grad_norm": 0.19048094749450684,
      "learning_rate": 0.0001619505184272662,
      "loss": 0.1839,
      "step": 9270
    },
    {
      "epoch": 0.5716045580535879,
      "grad_norm": 0.19257977604866028,
      "learning_rate": 0.000161909454881429,
      "loss": 0.1816,
      "step": 9280
    },
    {
      "epoch": 0.5722205112411457,
      "grad_norm": 0.17134471237659454,
      "learning_rate": 0.00016186839133559185,
      "loss": 0.1871,
      "step": 9290
    },
    {
      "epoch": 0.5728364644287034,
      "grad_norm": 0.21501465141773224,
      "learning_rate": 0.00016182732778975464,
      "loss": 0.1817,
      "step": 9300
    },
    {
      "epoch": 0.5734524176162612,
      "grad_norm": 0.19870848953723907,
      "learning_rate": 0.00016178626424391746,
      "loss": 0.1799,
      "step": 9310
    },
    {
      "epoch": 0.5740683708038189,
      "grad_norm": 0.2574750781059265,
      "learning_rate": 0.00016174520069808028,
      "loss": 0.1847,
      "step": 9320
    },
    {
      "epoch": 0.5746843239913767,
      "grad_norm": 0.19882164895534515,
      "learning_rate": 0.0001617041371522431,
      "loss": 0.1848,
      "step": 9330
    },
    {
      "epoch": 0.5753002771789344,
      "grad_norm": 0.20044076442718506,
      "learning_rate": 0.00016166307360640592,
      "loss": 0.1805,
      "step": 9340
    },
    {
      "epoch": 0.5759162303664922,
      "grad_norm": 0.22813139855861664,
      "learning_rate": 0.00016162201006056874,
      "loss": 0.183,
      "step": 9350
    },
    {
      "epoch": 0.5765321835540499,
      "grad_norm": 0.20167124271392822,
      "learning_rate": 0.00016158094651473154,
      "loss": 0.1825,
      "step": 9360
    },
    {
      "epoch": 0.5771481367416076,
      "grad_norm": 0.2707885503768921,
      "learning_rate": 0.00016153988296889438,
      "loss": 0.183,
      "step": 9370
    },
    {
      "epoch": 0.5777640899291654,
      "grad_norm": 0.19798092544078827,
      "learning_rate": 0.00016149881942305718,
      "loss": 0.1839,
      "step": 9380
    },
    {
      "epoch": 0.5783800431167231,
      "grad_norm": 0.19029419124126434,
      "learning_rate": 0.00016145775587722,
      "loss": 0.1811,
      "step": 9390
    },
    {
      "epoch": 0.5789959963042809,
      "grad_norm": 0.216192364692688,
      "learning_rate": 0.00016141669233138282,
      "loss": 0.1801,
      "step": 9400
    },
    {
      "epoch": 0.5796119494918386,
      "grad_norm": 0.17022046446800232,
      "learning_rate": 0.00016137562878554564,
      "loss": 0.1821,
      "step": 9410
    },
    {
      "epoch": 0.5802279026793964,
      "grad_norm": 0.25521644949913025,
      "learning_rate": 0.00016133456523970846,
      "loss": 0.179,
      "step": 9420
    },
    {
      "epoch": 0.5808438558669541,
      "grad_norm": 0.2612954080104828,
      "learning_rate": 0.00016129350169387128,
      "loss": 0.1809,
      "step": 9430
    },
    {
      "epoch": 0.5814598090545119,
      "grad_norm": 0.2596854269504547,
      "learning_rate": 0.00016125243814803407,
      "loss": 0.183,
      "step": 9440
    },
    {
      "epoch": 0.5820757622420696,
      "grad_norm": 0.2233349084854126,
      "learning_rate": 0.00016121137460219692,
      "loss": 0.185,
      "step": 9450
    },
    {
      "epoch": 0.5826917154296274,
      "grad_norm": 0.216300830245018,
      "learning_rate": 0.00016117031105635974,
      "loss": 0.1837,
      "step": 9460
    },
    {
      "epoch": 0.5833076686171851,
      "grad_norm": 0.19614464044570923,
      "learning_rate": 0.00016112924751052253,
      "loss": 0.1823,
      "step": 9470
    },
    {
      "epoch": 0.5839236218047429,
      "grad_norm": 0.19709955155849457,
      "learning_rate": 0.00016108818396468538,
      "loss": 0.1835,
      "step": 9480
    },
    {
      "epoch": 0.5845395749923006,
      "grad_norm": 0.25470638275146484,
      "learning_rate": 0.00016104712041884817,
      "loss": 0.1787,
      "step": 9490
    },
    {
      "epoch": 0.5851555281798583,
      "grad_norm": 0.21896076202392578,
      "learning_rate": 0.000161006056873011,
      "loss": 0.1838,
      "step": 9500
    },
    {
      "epoch": 0.5857714813674161,
      "grad_norm": 0.39329463243484497,
      "learning_rate": 0.0001609649933271738,
      "loss": 0.184,
      "step": 9510
    },
    {
      "epoch": 0.5863874345549738,
      "grad_norm": 0.22371657192707062,
      "learning_rate": 0.00016092392978133663,
      "loss": 0.1866,
      "step": 9520
    },
    {
      "epoch": 0.5870033877425316,
      "grad_norm": 0.2116122990846634,
      "learning_rate": 0.00016088286623549945,
      "loss": 0.1838,
      "step": 9530
    },
    {
      "epoch": 0.5876193409300893,
      "grad_norm": 0.21243375539779663,
      "learning_rate": 0.00016084180268966227,
      "loss": 0.1869,
      "step": 9540
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.24232181906700134,
      "learning_rate": 0.00016080073914382506,
      "loss": 0.1811,
      "step": 9550
    },
    {
      "epoch": 0.5888512473052048,
      "grad_norm": 0.23954541981220245,
      "learning_rate": 0.0001607596755979879,
      "loss": 0.1839,
      "step": 9560
    },
    {
      "epoch": 0.5894672004927626,
      "grad_norm": 0.22468993067741394,
      "learning_rate": 0.0001607186120521507,
      "loss": 0.1833,
      "step": 9570
    },
    {
      "epoch": 0.5900831536803203,
      "grad_norm": 0.20030517876148224,
      "learning_rate": 0.00016067754850631352,
      "loss": 0.182,
      "step": 9580
    },
    {
      "epoch": 0.5906991068678781,
      "grad_norm": 0.18860748410224915,
      "learning_rate": 0.00016063648496047634,
      "loss": 0.1822,
      "step": 9590
    },
    {
      "epoch": 0.5913150600554358,
      "grad_norm": 0.1535651534795761,
      "learning_rate": 0.00016059542141463916,
      "loss": 0.1823,
      "step": 9600
    },
    {
      "epoch": 0.5919310132429936,
      "grad_norm": 0.18846936523914337,
      "learning_rate": 0.00016055435786880198,
      "loss": 0.1832,
      "step": 9610
    },
    {
      "epoch": 0.5925469664305513,
      "grad_norm": 0.19524461030960083,
      "learning_rate": 0.0001605132943229648,
      "loss": 0.1834,
      "step": 9620
    },
    {
      "epoch": 0.5931629196181091,
      "grad_norm": 0.2661827504634857,
      "learning_rate": 0.0001604722307771276,
      "loss": 0.1823,
      "step": 9630
    },
    {
      "epoch": 0.5937788728056668,
      "grad_norm": 0.2822362184524536,
      "learning_rate": 0.00016043116723129044,
      "loss": 0.1819,
      "step": 9640
    },
    {
      "epoch": 0.5943948259932245,
      "grad_norm": 0.2787647247314453,
      "learning_rate": 0.00016039010368545324,
      "loss": 0.1825,
      "step": 9650
    },
    {
      "epoch": 0.5950107791807823,
      "grad_norm": 0.18488726019859314,
      "learning_rate": 0.00016034904013961606,
      "loss": 0.1825,
      "step": 9660
    },
    {
      "epoch": 0.59562673236834,
      "grad_norm": 0.19624856114387512,
      "learning_rate": 0.00016030797659377888,
      "loss": 0.1799,
      "step": 9670
    },
    {
      "epoch": 0.5962426855558978,
      "grad_norm": 0.18447017669677734,
      "learning_rate": 0.0001602669130479417,
      "loss": 0.1824,
      "step": 9680
    },
    {
      "epoch": 0.5968586387434555,
      "grad_norm": 0.22948932647705078,
      "learning_rate": 0.00016022584950210452,
      "loss": 0.1812,
      "step": 9690
    },
    {
      "epoch": 0.5974745919310133,
      "grad_norm": 0.1833389699459076,
      "learning_rate": 0.00016018478595626734,
      "loss": 0.1831,
      "step": 9700
    },
    {
      "epoch": 0.598090545118571,
      "grad_norm": 0.19267064332962036,
      "learning_rate": 0.00016014372241043013,
      "loss": 0.1843,
      "step": 9710
    },
    {
      "epoch": 0.5987064983061288,
      "grad_norm": 0.23636868596076965,
      "learning_rate": 0.00016010265886459298,
      "loss": 0.1783,
      "step": 9720
    },
    {
      "epoch": 0.5993224514936865,
      "grad_norm": 0.21321451663970947,
      "learning_rate": 0.00016006159531875577,
      "loss": 0.1819,
      "step": 9730
    },
    {
      "epoch": 0.5999384046812443,
      "grad_norm": 0.24948588013648987,
      "learning_rate": 0.0001600205317729186,
      "loss": 0.1798,
      "step": 9740
    },
    {
      "epoch": 0.600554357868802,
      "grad_norm": 0.2728317677974701,
      "learning_rate": 0.0001599794682270814,
      "loss": 0.1844,
      "step": 9750
    },
    {
      "epoch": 0.6011703110563598,
      "grad_norm": 0.22483500838279724,
      "learning_rate": 0.00015993840468124423,
      "loss": 0.1812,
      "step": 9760
    },
    {
      "epoch": 0.6017862642439175,
      "grad_norm": 0.20179328322410583,
      "learning_rate": 0.00015989734113540705,
      "loss": 0.1796,
      "step": 9770
    },
    {
      "epoch": 0.6024022174314753,
      "grad_norm": 0.22031770646572113,
      "learning_rate": 0.00015985627758956987,
      "loss": 0.1827,
      "step": 9780
    },
    {
      "epoch": 0.603018170619033,
      "grad_norm": 0.2865353524684906,
      "learning_rate": 0.00015981521404373266,
      "loss": 0.1804,
      "step": 9790
    },
    {
      "epoch": 0.6036341238065907,
      "grad_norm": 0.30702072381973267,
      "learning_rate": 0.0001597741504978955,
      "loss": 0.1801,
      "step": 9800
    },
    {
      "epoch": 0.6042500769941485,
      "grad_norm": 0.1899416148662567,
      "learning_rate": 0.0001597330869520583,
      "loss": 0.186,
      "step": 9810
    },
    {
      "epoch": 0.6048660301817061,
      "grad_norm": 0.24114370346069336,
      "learning_rate": 0.00015969202340622112,
      "loss": 0.1823,
      "step": 9820
    },
    {
      "epoch": 0.605481983369264,
      "grad_norm": 0.25081944465637207,
      "learning_rate": 0.00015965095986038394,
      "loss": 0.1827,
      "step": 9830
    },
    {
      "epoch": 0.6060979365568216,
      "grad_norm": 0.22668658196926117,
      "learning_rate": 0.00015960989631454676,
      "loss": 0.1861,
      "step": 9840
    },
    {
      "epoch": 0.6067138897443795,
      "grad_norm": 0.1955459862947464,
      "learning_rate": 0.00015956883276870958,
      "loss": 0.1839,
      "step": 9850
    },
    {
      "epoch": 0.6073298429319371,
      "grad_norm": 0.2161148339509964,
      "learning_rate": 0.0001595277692228724,
      "loss": 0.1839,
      "step": 9860
    },
    {
      "epoch": 0.607945796119495,
      "grad_norm": 0.20465430617332458,
      "learning_rate": 0.0001594867056770352,
      "loss": 0.1792,
      "step": 9870
    },
    {
      "epoch": 0.6085617493070526,
      "grad_norm": 0.2156558334827423,
      "learning_rate": 0.00015944564213119804,
      "loss": 0.1808,
      "step": 9880
    },
    {
      "epoch": 0.6091777024946105,
      "grad_norm": 0.1950664520263672,
      "learning_rate": 0.00015940457858536086,
      "loss": 0.1819,
      "step": 9890
    },
    {
      "epoch": 0.6097936556821681,
      "grad_norm": 0.16128939390182495,
      "learning_rate": 0.00015936351503952368,
      "loss": 0.1829,
      "step": 9900
    },
    {
      "epoch": 0.610409608869726,
      "grad_norm": 0.17613749206066132,
      "learning_rate": 0.0001593224514936865,
      "loss": 0.1835,
      "step": 9910
    },
    {
      "epoch": 0.6110255620572836,
      "grad_norm": 0.22738942503929138,
      "learning_rate": 0.0001592813879478493,
      "loss": 0.1816,
      "step": 9920
    },
    {
      "epoch": 0.6116415152448413,
      "grad_norm": 0.19916637241840363,
      "learning_rate": 0.00015924032440201214,
      "loss": 0.1809,
      "step": 9930
    },
    {
      "epoch": 0.6122574684323991,
      "grad_norm": 0.19352009892463684,
      "learning_rate": 0.00015919926085617494,
      "loss": 0.1812,
      "step": 9940
    },
    {
      "epoch": 0.6128734216199568,
      "grad_norm": 0.20954668521881104,
      "learning_rate": 0.00015915819731033776,
      "loss": 0.1814,
      "step": 9950
    },
    {
      "epoch": 0.6134893748075146,
      "grad_norm": 0.4995720386505127,
      "learning_rate": 0.00015911713376450058,
      "loss": 0.1839,
      "step": 9960
    },
    {
      "epoch": 0.6141053279950723,
      "grad_norm": 0.23039290308952332,
      "learning_rate": 0.0001590760702186634,
      "loss": 0.1825,
      "step": 9970
    },
    {
      "epoch": 0.6147212811826301,
      "grad_norm": 0.2230043262243271,
      "learning_rate": 0.00015903500667282622,
      "loss": 0.1816,
      "step": 9980
    },
    {
      "epoch": 0.6153372343701878,
      "grad_norm": 0.20072679221630096,
      "learning_rate": 0.00015899394312698904,
      "loss": 0.1846,
      "step": 9990
    },
    {
      "epoch": 0.6159531875577456,
      "grad_norm": 0.18326357007026672,
      "learning_rate": 0.00015895287958115183,
      "loss": 0.1789,
      "step": 10000
    },
    {
      "epoch": 0.6165691407453033,
      "grad_norm": 0.4900577664375305,
      "learning_rate": 0.00015891181603531468,
      "loss": 0.1844,
      "step": 10010
    },
    {
      "epoch": 0.6171850939328611,
      "grad_norm": 0.2780033051967621,
      "learning_rate": 0.00015887075248947747,
      "loss": 0.1835,
      "step": 10020
    },
    {
      "epoch": 0.6178010471204188,
      "grad_norm": 0.2037983536720276,
      "learning_rate": 0.0001588296889436403,
      "loss": 0.1829,
      "step": 10030
    },
    {
      "epoch": 0.6184170003079766,
      "grad_norm": 0.1915431022644043,
      "learning_rate": 0.0001587886253978031,
      "loss": 0.1806,
      "step": 10040
    },
    {
      "epoch": 0.6190329534955343,
      "grad_norm": 0.20211297273635864,
      "learning_rate": 0.00015874756185196593,
      "loss": 0.1827,
      "step": 10050
    },
    {
      "epoch": 0.6196489066830921,
      "grad_norm": 0.6880672574043274,
      "learning_rate": 0.00015870649830612875,
      "loss": 0.1822,
      "step": 10060
    },
    {
      "epoch": 0.6202648598706498,
      "grad_norm": 0.17559212446212769,
      "learning_rate": 0.00015866543476029157,
      "loss": 0.1825,
      "step": 10070
    },
    {
      "epoch": 0.6208808130582075,
      "grad_norm": 0.17147521674633026,
      "learning_rate": 0.00015862437121445436,
      "loss": 0.1808,
      "step": 10080
    },
    {
      "epoch": 0.6214967662457653,
      "grad_norm": 0.21777449548244476,
      "learning_rate": 0.0001585833076686172,
      "loss": 0.185,
      "step": 10090
    },
    {
      "epoch": 0.622112719433323,
      "grad_norm": 0.23325516283512115,
      "learning_rate": 0.00015854224412278,
      "loss": 0.1819,
      "step": 10100
    },
    {
      "epoch": 0.6227286726208808,
      "grad_norm": 0.21575844287872314,
      "learning_rate": 0.00015850118057694282,
      "loss": 0.1823,
      "step": 10110
    },
    {
      "epoch": 0.6233446258084385,
      "grad_norm": 0.17823030054569244,
      "learning_rate": 0.00015846011703110564,
      "loss": 0.1856,
      "step": 10120
    },
    {
      "epoch": 0.6239605789959963,
      "grad_norm": 0.22567634284496307,
      "learning_rate": 0.00015841905348526846,
      "loss": 0.183,
      "step": 10130
    },
    {
      "epoch": 0.624576532183554,
      "grad_norm": 0.32740676403045654,
      "learning_rate": 0.00015837798993943128,
      "loss": 0.1826,
      "step": 10140
    },
    {
      "epoch": 0.6251924853711118,
      "grad_norm": 0.19291947782039642,
      "learning_rate": 0.0001583369263935941,
      "loss": 0.1808,
      "step": 10150
    },
    {
      "epoch": 0.6258084385586695,
      "grad_norm": 0.21081511676311493,
      "learning_rate": 0.0001582958628477569,
      "loss": 0.1833,
      "step": 10160
    },
    {
      "epoch": 0.6264243917462273,
      "grad_norm": 0.2097819745540619,
      "learning_rate": 0.00015825479930191974,
      "loss": 0.182,
      "step": 10170
    },
    {
      "epoch": 0.627040344933785,
      "grad_norm": 0.38503047823905945,
      "learning_rate": 0.00015821373575608254,
      "loss": 0.1821,
      "step": 10180
    },
    {
      "epoch": 0.6276562981213428,
      "grad_norm": 0.927247941493988,
      "learning_rate": 0.00015817267221024536,
      "loss": 0.1861,
      "step": 10190
    },
    {
      "epoch": 0.6282722513089005,
      "grad_norm": 0.21620114147663116,
      "learning_rate": 0.00015813160866440818,
      "loss": 0.1817,
      "step": 10200
    },
    {
      "epoch": 0.6288882044964582,
      "grad_norm": 0.18986861407756805,
      "learning_rate": 0.000158090545118571,
      "loss": 0.185,
      "step": 10210
    },
    {
      "epoch": 0.629504157684016,
      "grad_norm": 0.24212323129177094,
      "learning_rate": 0.00015804948157273382,
      "loss": 0.181,
      "step": 10220
    },
    {
      "epoch": 0.6301201108715737,
      "grad_norm": 0.2437216341495514,
      "learning_rate": 0.00015800841802689664,
      "loss": 0.1837,
      "step": 10230
    },
    {
      "epoch": 0.6307360640591315,
      "grad_norm": 0.22285901010036469,
      "learning_rate": 0.00015796735448105943,
      "loss": 0.1821,
      "step": 10240
    },
    {
      "epoch": 0.6313520172466892,
      "grad_norm": 0.28927186131477356,
      "learning_rate": 0.00015792629093522228,
      "loss": 0.1823,
      "step": 10250
    },
    {
      "epoch": 0.631967970434247,
      "grad_norm": 0.1605677455663681,
      "learning_rate": 0.00015788522738938507,
      "loss": 0.18,
      "step": 10260
    },
    {
      "epoch": 0.6325839236218047,
      "grad_norm": 0.23171009123325348,
      "learning_rate": 0.0001578441638435479,
      "loss": 0.1787,
      "step": 10270
    },
    {
      "epoch": 0.6331998768093625,
      "grad_norm": 0.20939964056015015,
      "learning_rate": 0.0001578031002977107,
      "loss": 0.1826,
      "step": 10280
    },
    {
      "epoch": 0.6338158299969202,
      "grad_norm": 0.27883532643318176,
      "learning_rate": 0.00015776203675187353,
      "loss": 0.1844,
      "step": 10290
    },
    {
      "epoch": 0.634431783184478,
      "grad_norm": 0.2030610740184784,
      "learning_rate": 0.00015772097320603635,
      "loss": 0.1803,
      "step": 10300
    },
    {
      "epoch": 0.6350477363720357,
      "grad_norm": 0.19879905879497528,
      "learning_rate": 0.00015767990966019917,
      "loss": 0.1823,
      "step": 10310
    },
    {
      "epoch": 0.6356636895595935,
      "grad_norm": 0.2858665883541107,
      "learning_rate": 0.000157638846114362,
      "loss": 0.1835,
      "step": 10320
    },
    {
      "epoch": 0.6362796427471512,
      "grad_norm": 0.1993829905986786,
      "learning_rate": 0.0001575977825685248,
      "loss": 0.1825,
      "step": 10330
    },
    {
      "epoch": 0.636895595934709,
      "grad_norm": 0.17650558054447174,
      "learning_rate": 0.00015755671902268763,
      "loss": 0.1832,
      "step": 10340
    },
    {
      "epoch": 0.6375115491222667,
      "grad_norm": 0.19008469581604004,
      "learning_rate": 0.00015751565547685042,
      "loss": 0.1829,
      "step": 10350
    },
    {
      "epoch": 0.6381275023098244,
      "grad_norm": NaN,
      "learning_rate": 0.00015747459193101327,
      "loss": 0.185,
      "step": 10360
    },
    {
      "epoch": 0.6387434554973822,
      "grad_norm": 0.1733834445476532,
      "learning_rate": 0.0001574417410943435,
      "loss": 0.1882,
      "step": 10370
    },
    {
      "epoch": 0.6393594086849399,
      "grad_norm": 0.25620922446250916,
      "learning_rate": 0.00015740067754850632,
      "loss": 0.1828,
      "step": 10380
    },
    {
      "epoch": 0.6399753618724977,
      "grad_norm": 0.420564204454422,
      "learning_rate": 0.00015735961400266914,
      "loss": 0.1798,
      "step": 10390
    },
    {
      "epoch": 0.6405913150600554,
      "grad_norm": 0.23298878967761993,
      "learning_rate": 0.00015731855045683196,
      "loss": 0.184,
      "step": 10400
    },
    {
      "epoch": 0.6412072682476132,
      "grad_norm": 0.49853816628456116,
      "learning_rate": 0.00015727748691099478,
      "loss": 0.1799,
      "step": 10410
    },
    {
      "epoch": 0.6418232214351709,
      "grad_norm": 0.2984532117843628,
      "learning_rate": 0.00015723642336515757,
      "loss": 0.1819,
      "step": 10420
    },
    {
      "epoch": 0.6424391746227287,
      "grad_norm": 0.22745291888713837,
      "learning_rate": 0.00015719535981932042,
      "loss": 0.1825,
      "step": 10430
    },
    {
      "epoch": 0.6430551278102864,
      "grad_norm": 0.3129672110080719,
      "learning_rate": 0.0001571542962734832,
      "loss": 0.1819,
      "step": 10440
    },
    {
      "epoch": 0.6436710809978442,
      "grad_norm": 0.19271834194660187,
      "learning_rate": 0.00015711323272764603,
      "loss": 0.1821,
      "step": 10450
    },
    {
      "epoch": 0.6442870341854019,
      "grad_norm": 0.18308134377002716,
      "learning_rate": 0.00015707216918180885,
      "loss": 0.181,
      "step": 10460
    },
    {
      "epoch": 0.6449029873729597,
      "grad_norm": 0.26763030886650085,
      "learning_rate": 0.00015703110563597167,
      "loss": 0.1853,
      "step": 10470
    },
    {
      "epoch": 0.6455189405605174,
      "grad_norm": 0.3666136562824249,
      "learning_rate": 0.0001569900420901345,
      "loss": 0.1827,
      "step": 10480
    },
    {
      "epoch": 0.6461348937480752,
      "grad_norm": 0.24901455640792847,
      "learning_rate": 0.0001569489785442973,
      "loss": 0.1788,
      "step": 10490
    },
    {
      "epoch": 0.6467508469356329,
      "grad_norm": 0.2125525325536728,
      "learning_rate": 0.0001569079149984601,
      "loss": 0.1789,
      "step": 10500
    },
    {
      "epoch": 0.6473668001231906,
      "grad_norm": 0.22150644659996033,
      "learning_rate": 0.00015686685145262295,
      "loss": 0.1854,
      "step": 10510
    },
    {
      "epoch": 0.6479827533107484,
      "grad_norm": 0.3045874834060669,
      "learning_rate": 0.00015682578790678574,
      "loss": 0.1834,
      "step": 10520
    },
    {
      "epoch": 0.6485987064983061,
      "grad_norm": 0.20922474563121796,
      "learning_rate": 0.00015678472436094856,
      "loss": 0.1829,
      "step": 10530
    },
    {
      "epoch": 0.6492146596858639,
      "grad_norm": 0.3127603232860565,
      "learning_rate": 0.00015674366081511138,
      "loss": 0.1828,
      "step": 10540
    },
    {
      "epoch": 0.6498306128734216,
      "grad_norm": 0.253676176071167,
      "learning_rate": 0.0001567025972692742,
      "loss": 0.1816,
      "step": 10550
    },
    {
      "epoch": 0.6504465660609794,
      "grad_norm": 0.23820477724075317,
      "learning_rate": 0.00015666153372343702,
      "loss": 0.1822,
      "step": 10560
    },
    {
      "epoch": 0.6510625192485371,
      "grad_norm": 0.18356133997440338,
      "learning_rate": 0.00015662047017759984,
      "loss": 0.1793,
      "step": 10570
    },
    {
      "epoch": 0.6516784724360949,
      "grad_norm": 0.2773541808128357,
      "learning_rate": 0.00015657940663176266,
      "loss": 0.1855,
      "step": 10580
    },
    {
      "epoch": 0.6522944256236526,
      "grad_norm": 0.24481208622455597,
      "learning_rate": 0.00015653834308592548,
      "loss": 0.1816,
      "step": 10590
    },
    {
      "epoch": 0.6529103788112104,
      "grad_norm": 0.20442821085453033,
      "learning_rate": 0.0001564972795400883,
      "loss": 0.1801,
      "step": 10600
    },
    {
      "epoch": 0.6535263319987681,
      "grad_norm": 0.2052670568227768,
      "learning_rate": 0.0001564562159942511,
      "loss": 0.1802,
      "step": 10610
    },
    {
      "epoch": 0.6541422851863259,
      "grad_norm": 0.19294089078903198,
      "learning_rate": 0.00015641515244841394,
      "loss": 0.1775,
      "step": 10620
    },
    {
      "epoch": 0.6547582383738836,
      "grad_norm": 0.2006596475839615,
      "learning_rate": 0.00015637408890257674,
      "loss": 0.1813,
      "step": 10630
    },
    {
      "epoch": 0.6553741915614413,
      "grad_norm": 0.21437250077724457,
      "learning_rate": 0.00015633302535673956,
      "loss": 0.1841,
      "step": 10640
    },
    {
      "epoch": 0.6559901447489991,
      "grad_norm": 0.21553531289100647,
      "learning_rate": 0.00015629196181090238,
      "loss": 0.1815,
      "step": 10650
    },
    {
      "epoch": 0.6566060979365568,
      "grad_norm": 0.18553109467029572,
      "learning_rate": 0.0001562508982650652,
      "loss": 0.1795,
      "step": 10660
    },
    {
      "epoch": 0.6572220511241146,
      "grad_norm": 0.19920437037944794,
      "learning_rate": 0.00015620983471922802,
      "loss": 0.1801,
      "step": 10670
    },
    {
      "epoch": 0.6578380043116723,
      "grad_norm": 0.1763303428888321,
      "learning_rate": 0.00015616877117339084,
      "loss": 0.1803,
      "step": 10680
    },
    {
      "epoch": 0.6584539574992301,
      "grad_norm": 0.21593056619167328,
      "learning_rate": 0.00015612770762755363,
      "loss": 0.1786,
      "step": 10690
    },
    {
      "epoch": 0.6590699106867878,
      "grad_norm": 0.1845034956932068,
      "learning_rate": 0.00015608664408171648,
      "loss": 0.1834,
      "step": 10700
    },
    {
      "epoch": 0.6596858638743456,
      "grad_norm": 0.23552897572517395,
      "learning_rate": 0.00015604558053587927,
      "loss": 0.1833,
      "step": 10710
    },
    {
      "epoch": 0.6603018170619033,
      "grad_norm": 0.17202353477478027,
      "learning_rate": 0.0001560045169900421,
      "loss": 0.1776,
      "step": 10720
    },
    {
      "epoch": 0.6609177702494611,
      "grad_norm": 0.17025701701641083,
      "learning_rate": 0.0001559634534442049,
      "loss": 0.1798,
      "step": 10730
    },
    {
      "epoch": 0.6615337234370188,
      "grad_norm": 0.1836506426334381,
      "learning_rate": 0.00015592238989836773,
      "loss": 0.1824,
      "step": 10740
    },
    {
      "epoch": 0.6621496766245766,
      "grad_norm": 0.2160397320985794,
      "learning_rate": 0.00015588132635253055,
      "loss": 0.1848,
      "step": 10750
    },
    {
      "epoch": 0.6627656298121343,
      "grad_norm": 0.2103939950466156,
      "learning_rate": 0.00015584026280669337,
      "loss": 0.1828,
      "step": 10760
    },
    {
      "epoch": 0.6633815829996921,
      "grad_norm": 0.187197744846344,
      "learning_rate": 0.00015579919926085616,
      "loss": 0.1788,
      "step": 10770
    },
    {
      "epoch": 0.6639975361872498,
      "grad_norm": 0.1932511180639267,
      "learning_rate": 0.000155758135715019,
      "loss": 0.1821,
      "step": 10780
    },
    {
      "epoch": 0.6646134893748075,
      "grad_norm": 0.19673319160938263,
      "learning_rate": 0.0001557170721691818,
      "loss": 0.1823,
      "step": 10790
    },
    {
      "epoch": 0.6652294425623653,
      "grad_norm": 0.16648970544338226,
      "learning_rate": 0.00015567600862334465,
      "loss": 0.1818,
      "step": 10800
    },
    {
      "epoch": 0.665845395749923,
      "grad_norm": 0.19104719161987305,
      "learning_rate": 0.00015563494507750744,
      "loss": 0.1785,
      "step": 10810
    },
    {
      "epoch": 0.6664613489374808,
      "grad_norm": 0.28218308091163635,
      "learning_rate": 0.00015559388153167026,
      "loss": 0.1819,
      "step": 10820
    },
    {
      "epoch": 0.6670773021250385,
      "grad_norm": 0.16789716482162476,
      "learning_rate": 0.00015555281798583308,
      "loss": 0.1801,
      "step": 10830
    },
    {
      "epoch": 0.6676932553125963,
      "grad_norm": 0.17514052987098694,
      "learning_rate": 0.0001555117544399959,
      "loss": 0.1828,
      "step": 10840
    },
    {
      "epoch": 0.668309208500154,
      "grad_norm": 0.22433412075042725,
      "learning_rate": 0.0001554706908941587,
      "loss": 0.1795,
      "step": 10850
    },
    {
      "epoch": 0.6689251616877118,
      "grad_norm": 0.16524966061115265,
      "learning_rate": 0.00015542962734832154,
      "loss": 0.1796,
      "step": 10860
    },
    {
      "epoch": 0.6695411148752695,
      "grad_norm": 0.20324933528900146,
      "learning_rate": 0.00015538856380248434,
      "loss": 0.1814,
      "step": 10870
    },
    {
      "epoch": 0.6701570680628273,
      "grad_norm": 0.21505330502986908,
      "learning_rate": 0.00015534750025664718,
      "loss": 0.181,
      "step": 10880
    },
    {
      "epoch": 0.670773021250385,
      "grad_norm": 0.20012213289737701,
      "learning_rate": 0.00015530643671080998,
      "loss": 0.1824,
      "step": 10890
    },
    {
      "epoch": 0.6713889744379428,
      "grad_norm": 0.17774128913879395,
      "learning_rate": 0.0001552653731649728,
      "loss": 0.1792,
      "step": 10900
    },
    {
      "epoch": 0.6720049276255005,
      "grad_norm": 0.24081659317016602,
      "learning_rate": 0.00015522430961913562,
      "loss": 0.1835,
      "step": 10910
    },
    {
      "epoch": 0.6726208808130582,
      "grad_norm": 0.18368074297904968,
      "learning_rate": 0.00015518324607329844,
      "loss": 0.1822,
      "step": 10920
    },
    {
      "epoch": 0.673236834000616,
      "grad_norm": 0.17630305886268616,
      "learning_rate": 0.00015514218252746123,
      "loss": 0.1827,
      "step": 10930
    },
    {
      "epoch": 0.6738527871881737,
      "grad_norm": 0.3136030435562134,
      "learning_rate": 0.00015510111898162408,
      "loss": 0.1802,
      "step": 10940
    },
    {
      "epoch": 0.6744687403757315,
      "grad_norm": 0.18693509697914124,
      "learning_rate": 0.00015506005543578687,
      "loss": 0.1818,
      "step": 10950
    },
    {
      "epoch": 0.6750846935632892,
      "grad_norm": 0.27292659878730774,
      "learning_rate": 0.00015501899188994972,
      "loss": 0.1802,
      "step": 10960
    },
    {
      "epoch": 0.675700646750847,
      "grad_norm": 0.17115220427513123,
      "learning_rate": 0.0001549779283441125,
      "loss": 0.1832,
      "step": 10970
    },
    {
      "epoch": 0.6763165999384047,
      "grad_norm": 0.6108199954032898,
      "learning_rate": 0.00015493686479827533,
      "loss": 0.1794,
      "step": 10980
    },
    {
      "epoch": 0.6769325531259625,
      "grad_norm": 0.17915481328964233,
      "learning_rate": 0.00015489580125243818,
      "loss": 0.1804,
      "step": 10990
    },
    {
      "epoch": 0.6775485063135201,
      "grad_norm": 0.24156518280506134,
      "learning_rate": 0.00015485473770660097,
      "loss": 0.1816,
      "step": 11000
    },
    {
      "epoch": 0.678164459501078,
      "grad_norm": 0.19517162442207336,
      "learning_rate": 0.0001548136741607638,
      "loss": 0.1778,
      "step": 11010
    },
    {
      "epoch": 0.6787804126886356,
      "grad_norm": 0.20287340879440308,
      "learning_rate": 0.0001547726106149266,
      "loss": 0.1824,
      "step": 11020
    },
    {
      "epoch": 0.6793963658761935,
      "grad_norm": 0.1695035845041275,
      "learning_rate": 0.00015473154706908943,
      "loss": 0.1793,
      "step": 11030
    },
    {
      "epoch": 0.6800123190637511,
      "grad_norm": 0.1785811334848404,
      "learning_rate": 0.00015469048352325225,
      "loss": 0.1799,
      "step": 11040
    },
    {
      "epoch": 0.680628272251309,
      "grad_norm": 0.20855115354061127,
      "learning_rate": 0.00015464941997741507,
      "loss": 0.1794,
      "step": 11050
    },
    {
      "epoch": 0.6812442254388666,
      "grad_norm": 0.18743351101875305,
      "learning_rate": 0.00015460835643157786,
      "loss": 0.1808,
      "step": 11060
    },
    {
      "epoch": 0.6818601786264243,
      "grad_norm": 0.19130262732505798,
      "learning_rate": 0.0001545672928857407,
      "loss": 0.1811,
      "step": 11070
    },
    {
      "epoch": 0.6824761318139821,
      "grad_norm": 0.18966886401176453,
      "learning_rate": 0.0001545262293399035,
      "loss": 0.1781,
      "step": 11080
    },
    {
      "epoch": 0.6830920850015398,
      "grad_norm": 0.18106746673583984,
      "learning_rate": 0.00015448516579406632,
      "loss": 0.1826,
      "step": 11090
    },
    {
      "epoch": 0.6837080381890976,
      "grad_norm": 0.17855297029018402,
      "learning_rate": 0.00015444410224822914,
      "loss": 0.1803,
      "step": 11100
    },
    {
      "epoch": 0.6843239913766553,
      "grad_norm": 0.1522487998008728,
      "learning_rate": 0.00015440303870239196,
      "loss": 0.1815,
      "step": 11110
    },
    {
      "epoch": 0.6849399445642131,
      "grad_norm": 0.1865096539258957,
      "learning_rate": 0.00015436197515655478,
      "loss": 0.18,
      "step": 11120
    },
    {
      "epoch": 0.6855558977517708,
      "grad_norm": 0.23578493297100067,
      "learning_rate": 0.0001543209116107176,
      "loss": 0.1807,
      "step": 11130
    },
    {
      "epoch": 0.6861718509393286,
      "grad_norm": 0.20463739335536957,
      "learning_rate": 0.0001542798480648804,
      "loss": 0.1798,
      "step": 11140
    },
    {
      "epoch": 0.6867878041268863,
      "grad_norm": 0.1973796933889389,
      "learning_rate": 0.00015423878451904324,
      "loss": 0.1778,
      "step": 11150
    },
    {
      "epoch": 0.6874037573144441,
      "grad_norm": 0.2343488335609436,
      "learning_rate": 0.00015419772097320604,
      "loss": 0.181,
      "step": 11160
    },
    {
      "epoch": 0.6880197105020018,
      "grad_norm": 0.2236507534980774,
      "learning_rate": 0.0001541607637819526,
      "loss": 0.1812,
      "step": 11170
    },
    {
      "epoch": 0.6886356636895596,
      "grad_norm": 0.22627703845500946,
      "learning_rate": 0.00015411970023611538,
      "loss": 0.1829,
      "step": 11180
    },
    {
      "epoch": 0.6892516168771173,
      "grad_norm": 0.18618722259998322,
      "learning_rate": 0.00015407863669027823,
      "loss": 0.1814,
      "step": 11190
    },
    {
      "epoch": 0.6898675700646751,
      "grad_norm": 0.16243307292461395,
      "learning_rate": 0.00015403757314444102,
      "loss": 0.1799,
      "step": 11200
    },
    {
      "epoch": 0.6904835232522328,
      "grad_norm": 0.20716944336891174,
      "learning_rate": 0.00015399650959860384,
      "loss": 0.1804,
      "step": 11210
    },
    {
      "epoch": 0.6910994764397905,
      "grad_norm": 0.20831207931041718,
      "learning_rate": 0.00015395544605276666,
      "loss": 0.1829,
      "step": 11220
    },
    {
      "epoch": 0.6917154296273483,
      "grad_norm": 0.802889347076416,
      "learning_rate": 0.00015391438250692948,
      "loss": 0.1835,
      "step": 11230
    },
    {
      "epoch": 0.692331382814906,
      "grad_norm": 0.2573651671409607,
      "learning_rate": 0.0001538733189610923,
      "loss": 0.1795,
      "step": 11240
    },
    {
      "epoch": 0.6929473360024638,
      "grad_norm": 0.1898389309644699,
      "learning_rate": 0.00015383225541525512,
      "loss": 0.1781,
      "step": 11250
    },
    {
      "epoch": 0.6935632891900215,
      "grad_norm": 0.2476179003715515,
      "learning_rate": 0.00015379119186941791,
      "loss": 0.1791,
      "step": 11260
    },
    {
      "epoch": 0.6941792423775793,
      "grad_norm": 0.2104976624250412,
      "learning_rate": 0.00015375012832358076,
      "loss": 0.181,
      "step": 11270
    },
    {
      "epoch": 0.694795195565137,
      "grad_norm": 0.16455663740634918,
      "learning_rate": 0.00015370906477774355,
      "loss": 0.182,
      "step": 11280
    },
    {
      "epoch": 0.6954111487526948,
      "grad_norm": 0.209132581949234,
      "learning_rate": 0.0001536680012319064,
      "loss": 0.1827,
      "step": 11290
    },
    {
      "epoch": 0.6960271019402525,
      "grad_norm": 0.17703916132450104,
      "learning_rate": 0.0001536269376860692,
      "loss": 0.1808,
      "step": 11300
    },
    {
      "epoch": 0.6966430551278103,
      "grad_norm": 0.18659815192222595,
      "learning_rate": 0.00015358587414023202,
      "loss": 0.1809,
      "step": 11310
    },
    {
      "epoch": 0.697259008315368,
      "grad_norm": 0.20038117468357086,
      "learning_rate": 0.00015354481059439484,
      "loss": 0.1795,
      "step": 11320
    },
    {
      "epoch": 0.6978749615029258,
      "grad_norm": 0.22174926102161407,
      "learning_rate": 0.00015350374704855766,
      "loss": 0.1837,
      "step": 11330
    },
    {
      "epoch": 0.6984909146904835,
      "grad_norm": 0.19795463979244232,
      "learning_rate": 0.00015346268350272045,
      "loss": 0.1842,
      "step": 11340
    },
    {
      "epoch": 0.6991068678780412,
      "grad_norm": 0.19880223274230957,
      "learning_rate": 0.0001534216199568833,
      "loss": 0.18,
      "step": 11350
    },
    {
      "epoch": 0.699722821065599,
      "grad_norm": 0.27694159746170044,
      "learning_rate": 0.0001533805564110461,
      "loss": 0.184,
      "step": 11360
    },
    {
      "epoch": 0.7003387742531567,
      "grad_norm": 0.21061909198760986,
      "learning_rate": 0.00015333949286520894,
      "loss": 0.1811,
      "step": 11370
    },
    {
      "epoch": 0.7009547274407145,
      "grad_norm": 0.22876673936843872,
      "learning_rate": 0.00015329842931937173,
      "loss": 0.1816,
      "step": 11380
    },
    {
      "epoch": 0.7015706806282722,
      "grad_norm": 0.17245957255363464,
      "learning_rate": 0.00015325736577353455,
      "loss": 0.1778,
      "step": 11390
    },
    {
      "epoch": 0.70218663381583,
      "grad_norm": 0.1812313199043274,
      "learning_rate": 0.00015321630222769737,
      "loss": 0.1797,
      "step": 11400
    },
    {
      "epoch": 0.7028025870033877,
      "grad_norm": 0.1979357749223709,
      "learning_rate": 0.0001531752386818602,
      "loss": 0.1826,
      "step": 11410
    },
    {
      "epoch": 0.7034185401909455,
      "grad_norm": 0.2024073749780655,
      "learning_rate": 0.00015313417513602298,
      "loss": 0.1892,
      "step": 11420
    },
    {
      "epoch": 0.7040344933785032,
      "grad_norm": 0.2791023850440979,
      "learning_rate": 0.00015309311159018583,
      "loss": 0.1805,
      "step": 11430
    },
    {
      "epoch": 0.704650446566061,
      "grad_norm": 0.27381041646003723,
      "learning_rate": 0.00015305204804434862,
      "loss": 0.1803,
      "step": 11440
    },
    {
      "epoch": 0.7052663997536187,
      "grad_norm": 0.2340434044599533,
      "learning_rate": 0.00015301098449851147,
      "loss": 0.1813,
      "step": 11450
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.29429709911346436,
      "learning_rate": 0.00015296992095267426,
      "loss": 0.1811,
      "step": 11460
    },
    {
      "epoch": 0.7064983061287342,
      "grad_norm": 0.1931002289056778,
      "learning_rate": 0.00015292885740683708,
      "loss": 0.1824,
      "step": 11470
    },
    {
      "epoch": 0.707114259316292,
      "grad_norm": 0.23960013687610626,
      "learning_rate": 0.0001528877938609999,
      "loss": 0.1835,
      "step": 11480
    },
    {
      "epoch": 0.7077302125038497,
      "grad_norm": 0.1979387104511261,
      "learning_rate": 0.00015284673031516272,
      "loss": 0.1786,
      "step": 11490
    },
    {
      "epoch": 0.7083461656914074,
      "grad_norm": 0.21765221655368805,
      "learning_rate": 0.00015280566676932554,
      "loss": 0.1799,
      "step": 11500
    },
    {
      "epoch": 0.7089621188789652,
      "grad_norm": 0.22057002782821655,
      "learning_rate": 0.00015276460322348836,
      "loss": 0.1828,
      "step": 11510
    },
    {
      "epoch": 0.7095780720665229,
      "grad_norm": 0.25958120822906494,
      "learning_rate": 0.00015272353967765115,
      "loss": 0.1808,
      "step": 11520
    },
    {
      "epoch": 0.7101940252540807,
      "grad_norm": 0.2081751674413681,
      "learning_rate": 0.000152682476131814,
      "loss": 0.1808,
      "step": 11530
    },
    {
      "epoch": 0.7108099784416384,
      "grad_norm": 0.2551412582397461,
      "learning_rate": 0.00015264141258597682,
      "loss": 0.182,
      "step": 11540
    },
    {
      "epoch": 0.7114259316291962,
      "grad_norm": 0.2310367375612259,
      "learning_rate": 0.00015260034904013961,
      "loss": 0.1823,
      "step": 11550
    },
    {
      "epoch": 0.7120418848167539,
      "grad_norm": 0.19909453392028809,
      "learning_rate": 0.00015255928549430246,
      "loss": 0.1817,
      "step": 11560
    },
    {
      "epoch": 0.7126578380043117,
      "grad_norm": 0.19596612453460693,
      "learning_rate": 0.00015251822194846525,
      "loss": 0.1811,
      "step": 11570
    },
    {
      "epoch": 0.7132737911918694,
      "grad_norm": 0.21628756821155548,
      "learning_rate": 0.00015247715840262807,
      "loss": 0.182,
      "step": 11580
    },
    {
      "epoch": 0.7138897443794272,
      "grad_norm": 0.23292601108551025,
      "learning_rate": 0.0001524360948567909,
      "loss": 0.1805,
      "step": 11590
    },
    {
      "epoch": 0.7145056975669849,
      "grad_norm": 1.4275306463241577,
      "learning_rate": 0.00015239503131095371,
      "loss": 0.1806,
      "step": 11600
    },
    {
      "epoch": 0.7151216507545427,
      "grad_norm": 0.2232910394668579,
      "learning_rate": 0.00015235396776511653,
      "loss": 0.182,
      "step": 11610
    },
    {
      "epoch": 0.7157376039421004,
      "grad_norm": 0.22302591800689697,
      "learning_rate": 0.00015231290421927935,
      "loss": 0.1808,
      "step": 11620
    },
    {
      "epoch": 0.7163535571296581,
      "grad_norm": 0.19983695447444916,
      "learning_rate": 0.00015227184067344215,
      "loss": 0.1853,
      "step": 11630
    },
    {
      "epoch": 0.7169695103172159,
      "grad_norm": 0.20225875079631805,
      "learning_rate": 0.000152230777127605,
      "loss": 0.1817,
      "step": 11640
    },
    {
      "epoch": 0.7175854635047736,
      "grad_norm": 0.17573176324367523,
      "learning_rate": 0.0001521897135817678,
      "loss": 0.1819,
      "step": 11650
    },
    {
      "epoch": 0.7182014166923314,
      "grad_norm": 0.21202242374420166,
      "learning_rate": 0.0001521486500359306,
      "loss": 0.1794,
      "step": 11660
    },
    {
      "epoch": 0.7188173698798891,
      "grad_norm": 0.1764669269323349,
      "learning_rate": 0.00015210758649009343,
      "loss": 0.1786,
      "step": 11670
    },
    {
      "epoch": 0.7194333230674469,
      "grad_norm": 0.16460567712783813,
      "learning_rate": 0.00015206652294425625,
      "loss": 0.1804,
      "step": 11680
    },
    {
      "epoch": 0.7200492762550046,
      "grad_norm": 0.20001238584518433,
      "learning_rate": 0.00015202545939841907,
      "loss": 0.181,
      "step": 11690
    },
    {
      "epoch": 0.7206652294425624,
      "grad_norm": 0.3429597020149231,
      "learning_rate": 0.0001519843958525819,
      "loss": 0.1818,
      "step": 11700
    },
    {
      "epoch": 0.7212811826301201,
      "grad_norm": 0.2322743833065033,
      "learning_rate": 0.00015194333230674468,
      "loss": 0.1784,
      "step": 11710
    },
    {
      "epoch": 0.7218971358176779,
      "grad_norm": 0.19419163465499878,
      "learning_rate": 0.00015190226876090753,
      "loss": 0.1798,
      "step": 11720
    },
    {
      "epoch": 0.7225130890052356,
      "grad_norm": 0.17411601543426514,
      "learning_rate": 0.00015186120521507032,
      "loss": 0.1783,
      "step": 11730
    },
    {
      "epoch": 0.7231290421927934,
      "grad_norm": 0.2007296085357666,
      "learning_rate": 0.00015182014166923314,
      "loss": 0.1783,
      "step": 11740
    },
    {
      "epoch": 0.7237449953803511,
      "grad_norm": 0.19021475315093994,
      "learning_rate": 0.00015177907812339596,
      "loss": 0.1812,
      "step": 11750
    },
    {
      "epoch": 0.7243609485679089,
      "grad_norm": 0.18971458077430725,
      "learning_rate": 0.00015173801457755878,
      "loss": 0.1812,
      "step": 11760
    },
    {
      "epoch": 0.7249769017554666,
      "grad_norm": 0.20380085706710815,
      "learning_rate": 0.0001516969510317216,
      "loss": 0.1828,
      "step": 11770
    },
    {
      "epoch": 0.7255928549430243,
      "grad_norm": 0.16814124584197998,
      "learning_rate": 0.00015165588748588442,
      "loss": 0.1797,
      "step": 11780
    },
    {
      "epoch": 0.7262088081305821,
      "grad_norm": 0.2049761265516281,
      "learning_rate": 0.00015161482394004721,
      "loss": 0.1817,
      "step": 11790
    },
    {
      "epoch": 0.7268247613181398,
      "grad_norm": 0.15912151336669922,
      "learning_rate": 0.00015157376039421006,
      "loss": 0.1827,
      "step": 11800
    },
    {
      "epoch": 0.7274407145056976,
      "grad_norm": 0.17908209562301636,
      "learning_rate": 0.00015153269684837285,
      "loss": 0.1782,
      "step": 11810
    },
    {
      "epoch": 0.7280566676932553,
      "grad_norm": 0.17285823822021484,
      "learning_rate": 0.00015149163330253567,
      "loss": 0.186,
      "step": 11820
    },
    {
      "epoch": 0.7286726208808131,
      "grad_norm": 0.2141961306333542,
      "learning_rate": 0.0001514505697566985,
      "loss": 0.1863,
      "step": 11830
    },
    {
      "epoch": 0.7292885740683708,
      "grad_norm": 0.31765860319137573,
      "learning_rate": 0.00015140950621086131,
      "loss": 0.1802,
      "step": 11840
    },
    {
      "epoch": 0.7299045272559286,
      "grad_norm": 0.20320643484592438,
      "learning_rate": 0.00015136844266502413,
      "loss": 0.1805,
      "step": 11850
    },
    {
      "epoch": 0.7305204804434863,
      "grad_norm": 0.24229174852371216,
      "learning_rate": 0.00015132737911918695,
      "loss": 0.18,
      "step": 11860
    },
    {
      "epoch": 0.7311364336310441,
      "grad_norm": 0.38201436400413513,
      "learning_rate": 0.00015128631557334975,
      "loss": 0.1805,
      "step": 11870
    },
    {
      "epoch": 0.7317523868186018,
      "grad_norm": 0.17474569380283356,
      "learning_rate": 0.0001512452520275126,
      "loss": 0.1777,
      "step": 11880
    },
    {
      "epoch": 0.7323683400061596,
      "grad_norm": 0.2965695261955261,
      "learning_rate": 0.0001512041884816754,
      "loss": 0.1787,
      "step": 11890
    },
    {
      "epoch": 0.7329842931937173,
      "grad_norm": 0.17213989794254303,
      "learning_rate": 0.0001511631249358382,
      "loss": 0.1794,
      "step": 11900
    },
    {
      "epoch": 0.7336002463812751,
      "grad_norm": 0.21728378534317017,
      "learning_rate": 0.00015112206139000103,
      "loss": 0.1821,
      "step": 11910
    },
    {
      "epoch": 0.7342161995688328,
      "grad_norm": 0.1496492475271225,
      "learning_rate": 0.00015108099784416385,
      "loss": 0.1794,
      "step": 11920
    },
    {
      "epoch": 0.7348321527563905,
      "grad_norm": 0.2949126064777374,
      "learning_rate": 0.00015103993429832667,
      "loss": 0.1823,
      "step": 11930
    },
    {
      "epoch": 0.7354481059439483,
      "grad_norm": 0.1990330070257187,
      "learning_rate": 0.0001509988707524895,
      "loss": 0.1778,
      "step": 11940
    },
    {
      "epoch": 0.736064059131506,
      "grad_norm": 0.17605499923229218,
      "learning_rate": 0.00015095780720665228,
      "loss": 0.1807,
      "step": 11950
    },
    {
      "epoch": 0.7366800123190638,
      "grad_norm": 0.164397731423378,
      "learning_rate": 0.00015091674366081513,
      "loss": 0.181,
      "step": 11960
    },
    {
      "epoch": 0.7372959655066215,
      "grad_norm": 0.2555002272129059,
      "learning_rate": 0.00015087568011497795,
      "loss": 0.18,
      "step": 11970
    },
    {
      "epoch": 0.7379119186941793,
      "grad_norm": 0.19047096371650696,
      "learning_rate": 0.00015083461656914074,
      "loss": 0.179,
      "step": 11980
    },
    {
      "epoch": 0.738527871881737,
      "grad_norm": 0.18099550902843475,
      "learning_rate": 0.0001507935530233036,
      "loss": 0.1814,
      "step": 11990
    },
    {
      "epoch": 0.7391438250692948,
      "grad_norm": 0.3181511461734772,
      "learning_rate": 0.00015075248947746638,
      "loss": 0.1798,
      "step": 12000
    },
    {
      "epoch": 0.7397597782568525,
      "grad_norm": 0.17797479033470154,
      "learning_rate": 0.0001507114259316292,
      "loss": 0.1805,
      "step": 12010
    },
    {
      "epoch": 0.7403757314444103,
      "grad_norm": 0.18168100714683533,
      "learning_rate": 0.00015067036238579202,
      "loss": 0.1807,
      "step": 12020
    },
    {
      "epoch": 0.740991684631968,
      "grad_norm": 0.20030026137828827,
      "learning_rate": 0.00015062929883995484,
      "loss": 0.1817,
      "step": 12030
    },
    {
      "epoch": 0.7416076378195258,
      "grad_norm": 0.21205390989780426,
      "learning_rate": 0.00015058823529411766,
      "loss": 0.18,
      "step": 12040
    },
    {
      "epoch": 0.7422235910070835,
      "grad_norm": 0.16573965549468994,
      "learning_rate": 0.00015054717174828048,
      "loss": 0.1795,
      "step": 12050
    },
    {
      "epoch": 0.7428395441946412,
      "grad_norm": 0.17588716745376587,
      "learning_rate": 0.00015050610820244327,
      "loss": 0.1809,
      "step": 12060
    },
    {
      "epoch": 0.743455497382199,
      "grad_norm": 0.1926421970129013,
      "learning_rate": 0.00015046504465660612,
      "loss": 0.1823,
      "step": 12070
    },
    {
      "epoch": 0.7440714505697567,
      "grad_norm": 0.5370577573776245,
      "learning_rate": 0.00015042398111076891,
      "loss": 0.1794,
      "step": 12080
    },
    {
      "epoch": 0.7446874037573145,
      "grad_norm": 0.19478857517242432,
      "learning_rate": 0.00015038291756493173,
      "loss": 0.1797,
      "step": 12090
    },
    {
      "epoch": 0.7453033569448722,
      "grad_norm": 0.20137155055999756,
      "learning_rate": 0.00015034185401909455,
      "loss": 0.1796,
      "step": 12100
    },
    {
      "epoch": 0.74591931013243,
      "grad_norm": 0.19050779938697815,
      "learning_rate": 0.00015030079047325737,
      "loss": 0.1811,
      "step": 12110
    },
    {
      "epoch": 0.7465352633199877,
      "grad_norm": 0.23783810436725616,
      "learning_rate": 0.0001502597269274202,
      "loss": 0.1802,
      "step": 12120
    },
    {
      "epoch": 0.7471512165075455,
      "grad_norm": 0.20321343839168549,
      "learning_rate": 0.00015021866338158301,
      "loss": 0.1825,
      "step": 12130
    },
    {
      "epoch": 0.7477671696951032,
      "grad_norm": 0.2034393548965454,
      "learning_rate": 0.0001501775998357458,
      "loss": 0.1807,
      "step": 12140
    },
    {
      "epoch": 0.748383122882661,
      "grad_norm": 0.2079571783542633,
      "learning_rate": 0.00015013653628990865,
      "loss": 0.183,
      "step": 12150
    },
    {
      "epoch": 0.7489990760702187,
      "grad_norm": 0.181956484913826,
      "learning_rate": 0.00015009547274407145,
      "loss": 0.1799,
      "step": 12160
    },
    {
      "epoch": 0.7496150292577765,
      "grad_norm": 0.2159246802330017,
      "learning_rate": 0.00015005440919823427,
      "loss": 0.179,
      "step": 12170
    },
    {
      "epoch": 0.7502309824453341,
      "grad_norm": 0.1865285187959671,
      "learning_rate": 0.0001500133456523971,
      "loss": 0.1794,
      "step": 12180
    },
    {
      "epoch": 0.750846935632892,
      "grad_norm": 0.2181926667690277,
      "learning_rate": 0.0001499722821065599,
      "loss": 0.1809,
      "step": 12190
    },
    {
      "epoch": 0.7514628888204496,
      "grad_norm": 0.18248505890369415,
      "learning_rate": 0.00014993121856072273,
      "loss": 0.178,
      "step": 12200
    },
    {
      "epoch": 0.7520788420080073,
      "grad_norm": 0.20312732458114624,
      "learning_rate": 0.00014989015501488555,
      "loss": 0.1791,
      "step": 12210
    },
    {
      "epoch": 0.7526947951955651,
      "grad_norm": 0.19657552242279053,
      "learning_rate": 0.00014984909146904834,
      "loss": 0.1806,
      "step": 12220
    },
    {
      "epoch": 0.7533107483831228,
      "grad_norm": 0.18347294628620148,
      "learning_rate": 0.0001498080279232112,
      "loss": 0.1831,
      "step": 12230
    },
    {
      "epoch": 0.7539267015706806,
      "grad_norm": 0.21136434376239777,
      "learning_rate": 0.00014976696437737398,
      "loss": 0.1823,
      "step": 12240
    },
    {
      "epoch": 0.7545426547582383,
      "grad_norm": 0.18516314029693604,
      "learning_rate": 0.0001497259008315368,
      "loss": 0.1823,
      "step": 12250
    },
    {
      "epoch": 0.7551586079457961,
      "grad_norm": 0.1839970499277115,
      "learning_rate": 0.00014968483728569962,
      "loss": 0.1821,
      "step": 12260
    },
    {
      "epoch": 0.7557745611333538,
      "grad_norm": 0.22607260942459106,
      "learning_rate": 0.00014964377373986244,
      "loss": 0.1777,
      "step": 12270
    },
    {
      "epoch": 0.7563905143209116,
      "grad_norm": 0.19028733670711517,
      "learning_rate": 0.00014960271019402526,
      "loss": 0.1802,
      "step": 12280
    },
    {
      "epoch": 0.7570064675084693,
      "grad_norm": 0.23103925585746765,
      "learning_rate": 0.00014956164664818808,
      "loss": 0.1781,
      "step": 12290
    },
    {
      "epoch": 0.7576224206960271,
      "grad_norm": 0.22791065275669098,
      "learning_rate": 0.00014952058310235087,
      "loss": 0.1815,
      "step": 12300
    },
    {
      "epoch": 0.7582383738835848,
      "grad_norm": 0.15241922438144684,
      "learning_rate": 0.00014947951955651372,
      "loss": 0.1802,
      "step": 12310
    },
    {
      "epoch": 0.7588543270711426,
      "grad_norm": 0.2440609186887741,
      "learning_rate": 0.00014943845601067651,
      "loss": 0.1824,
      "step": 12320
    },
    {
      "epoch": 0.7594702802587003,
      "grad_norm": 0.17281551659107208,
      "learning_rate": 0.00014939739246483936,
      "loss": 0.1821,
      "step": 12330
    },
    {
      "epoch": 0.760086233446258,
      "grad_norm": 0.16969986259937286,
      "learning_rate": 0.00014935632891900215,
      "loss": 0.1801,
      "step": 12340
    },
    {
      "epoch": 0.7607021866338158,
      "grad_norm": 0.2355639636516571,
      "learning_rate": 0.00014931526537316497,
      "loss": 0.1807,
      "step": 12350
    },
    {
      "epoch": 0.7613181398213735,
      "grad_norm": 0.18122592568397522,
      "learning_rate": 0.0001492742018273278,
      "loss": 0.1801,
      "step": 12360
    },
    {
      "epoch": 0.7619340930089313,
      "grad_norm": 0.18156710267066956,
      "learning_rate": 0.00014923313828149061,
      "loss": 0.1777,
      "step": 12370
    },
    {
      "epoch": 0.762550046196489,
      "grad_norm": 0.18926513195037842,
      "learning_rate": 0.0001491920747356534,
      "loss": 0.1796,
      "step": 12380
    },
    {
      "epoch": 0.7631659993840468,
      "grad_norm": 0.22345831990242004,
      "learning_rate": 0.00014915101118981625,
      "loss": 0.1802,
      "step": 12390
    },
    {
      "epoch": 0.7637819525716045,
      "grad_norm": 0.17846374213695526,
      "learning_rate": 0.00014910994764397907,
      "loss": 0.1794,
      "step": 12400
    },
    {
      "epoch": 0.7643979057591623,
      "grad_norm": 0.41972073912620544,
      "learning_rate": 0.0001490688840981419,
      "loss": 0.1792,
      "step": 12410
    },
    {
      "epoch": 0.76501385894672,
      "grad_norm": 0.16397374868392944,
      "learning_rate": 0.00014902782055230471,
      "loss": 0.1788,
      "step": 12420
    },
    {
      "epoch": 0.7656298121342778,
      "grad_norm": 0.2147793024778366,
      "learning_rate": 0.0001489867570064675,
      "loss": 0.1826,
      "step": 12430
    },
    {
      "epoch": 0.7662457653218355,
      "grad_norm": 0.17048323154449463,
      "learning_rate": 0.00014894569346063035,
      "loss": 0.1824,
      "step": 12440
    },
    {
      "epoch": 0.7668617185093933,
      "grad_norm": 0.2163565456867218,
      "learning_rate": 0.00014890462991479315,
      "loss": 0.1809,
      "step": 12450
    },
    {
      "epoch": 0.767477671696951,
      "grad_norm": 0.19878458976745605,
      "learning_rate": 0.00014886356636895597,
      "loss": 0.1817,
      "step": 12460
    },
    {
      "epoch": 0.7680936248845088,
      "grad_norm": 0.1972336769104004,
      "learning_rate": 0.0001488225028231188,
      "loss": 0.1781,
      "step": 12470
    },
    {
      "epoch": 0.7687095780720665,
      "grad_norm": 0.23900336027145386,
      "learning_rate": 0.0001487814392772816,
      "loss": 0.1813,
      "step": 12480
    },
    {
      "epoch": 0.7693255312596242,
      "grad_norm": 0.31894537806510925,
      "learning_rate": 0.00014874037573144443,
      "loss": 0.1815,
      "step": 12490
    },
    {
      "epoch": 0.769941484447182,
      "grad_norm": 0.17418056726455688,
      "learning_rate": 0.00014869931218560725,
      "loss": 0.1832,
      "step": 12500
    },
    {
      "epoch": 0.7705574376347397,
      "grad_norm": 0.1632666289806366,
      "learning_rate": 0.00014865824863977004,
      "loss": 0.1818,
      "step": 12510
    },
    {
      "epoch": 0.7711733908222975,
      "grad_norm": 0.19915103912353516,
      "learning_rate": 0.0001486171850939329,
      "loss": 0.1801,
      "step": 12520
    },
    {
      "epoch": 0.7717893440098552,
      "grad_norm": 0.17440563440322876,
      "learning_rate": 0.00014857612154809568,
      "loss": 0.181,
      "step": 12530
    },
    {
      "epoch": 0.772405297197413,
      "grad_norm": 0.15048080682754517,
      "learning_rate": 0.0001485350580022585,
      "loss": 0.1796,
      "step": 12540
    },
    {
      "epoch": 0.7730212503849707,
      "grad_norm": 0.17449626326560974,
      "learning_rate": 0.00014849399445642132,
      "loss": 0.1839,
      "step": 12550
    },
    {
      "epoch": 0.7736372035725285,
      "grad_norm": 0.1645558625459671,
      "learning_rate": 0.00014845293091058414,
      "loss": 0.1798,
      "step": 12560
    },
    {
      "epoch": 0.7742531567600862,
      "grad_norm": 0.15223731100559235,
      "learning_rate": 0.00014841186736474696,
      "loss": 0.1791,
      "step": 12570
    },
    {
      "epoch": 0.774869109947644,
      "grad_norm": 0.1673937290906906,
      "learning_rate": 0.00014837080381890978,
      "loss": 0.1797,
      "step": 12580
    },
    {
      "epoch": 0.7754850631352017,
      "grad_norm": 0.15472204983234406,
      "learning_rate": 0.00014832974027307257,
      "loss": 0.1782,
      "step": 12590
    },
    {
      "epoch": 0.7761010163227595,
      "grad_norm": 0.1913839876651764,
      "learning_rate": 0.00014828867672723542,
      "loss": 0.1814,
      "step": 12600
    },
    {
      "epoch": 0.7767169695103172,
      "grad_norm": 0.46840840578079224,
      "learning_rate": 0.0001482476131813982,
      "loss": 0.1815,
      "step": 12610
    },
    {
      "epoch": 0.7773329226978749,
      "grad_norm": 0.1768893003463745,
      "learning_rate": 0.00014820654963556103,
      "loss": 0.1825,
      "step": 12620
    },
    {
      "epoch": 0.7779488758854327,
      "grad_norm": 0.17150236666202545,
      "learning_rate": 0.00014816548608972385,
      "loss": 0.1816,
      "step": 12630
    },
    {
      "epoch": 0.7785648290729904,
      "grad_norm": 0.2326890528202057,
      "learning_rate": 0.00014812442254388667,
      "loss": 0.1812,
      "step": 12640
    },
    {
      "epoch": 0.7791807822605482,
      "grad_norm": 0.21490272879600525,
      "learning_rate": 0.0001480833589980495,
      "loss": 0.1776,
      "step": 12650
    },
    {
      "epoch": 0.7797967354481059,
      "grad_norm": 0.1697288453578949,
      "learning_rate": 0.00014804229545221231,
      "loss": 0.1805,
      "step": 12660
    },
    {
      "epoch": 0.7804126886356637,
      "grad_norm": 0.26418596506118774,
      "learning_rate": 0.0001480012319063751,
      "loss": 0.1821,
      "step": 12670
    },
    {
      "epoch": 0.7810286418232214,
      "grad_norm": 0.19637155532836914,
      "learning_rate": 0.00014796016836053795,
      "loss": 0.1812,
      "step": 12680
    },
    {
      "epoch": 0.7816445950107792,
      "grad_norm": 0.1968417912721634,
      "learning_rate": 0.00014791910481470075,
      "loss": 0.1797,
      "step": 12690
    },
    {
      "epoch": 0.7822605481983369,
      "grad_norm": 0.4731045961380005,
      "learning_rate": 0.00014787804126886357,
      "loss": 0.1837,
      "step": 12700
    },
    {
      "epoch": 0.7828765013858947,
      "grad_norm": 0.18761329352855682,
      "learning_rate": 0.0001478369777230264,
      "loss": 0.1838,
      "step": 12710
    },
    {
      "epoch": 0.7834924545734524,
      "grad_norm": 0.17438161373138428,
      "learning_rate": 0.0001477959141771892,
      "loss": 0.1787,
      "step": 12720
    },
    {
      "epoch": 0.7841084077610102,
      "grad_norm": 0.20874297618865967,
      "learning_rate": 0.00014775485063135203,
      "loss": 0.1786,
      "step": 12730
    },
    {
      "epoch": 0.7847243609485679,
      "grad_norm": 0.1866089254617691,
      "learning_rate": 0.00014771378708551485,
      "loss": 0.1816,
      "step": 12740
    },
    {
      "epoch": 0.7853403141361257,
      "grad_norm": 0.2180858850479126,
      "learning_rate": 0.00014767272353967764,
      "loss": 0.1813,
      "step": 12750
    },
    {
      "epoch": 0.7859562673236834,
      "grad_norm": 0.2112637609243393,
      "learning_rate": 0.0001476316599938405,
      "loss": 0.1798,
      "step": 12760
    },
    {
      "epoch": 0.7865722205112411,
      "grad_norm": 0.2053980529308319,
      "learning_rate": 0.00014759059644800328,
      "loss": 0.1811,
      "step": 12770
    },
    {
      "epoch": 0.7871881736987989,
      "grad_norm": 0.23757457733154297,
      "learning_rate": 0.0001475495329021661,
      "loss": 0.1784,
      "step": 12780
    },
    {
      "epoch": 0.7878041268863566,
      "grad_norm": 0.16338060796260834,
      "learning_rate": 0.00014750846935632892,
      "loss": 0.1807,
      "step": 12790
    },
    {
      "epoch": 0.7884200800739144,
      "grad_norm": 0.2192905843257904,
      "learning_rate": 0.00014746740581049174,
      "loss": 0.1825,
      "step": 12800
    },
    {
      "epoch": 0.7890360332614721,
      "grad_norm": 0.16415491700172424,
      "learning_rate": 0.00014742634226465456,
      "loss": 0.1813,
      "step": 12810
    },
    {
      "epoch": 0.7896519864490299,
      "grad_norm": 0.1865435391664505,
      "learning_rate": 0.00014738527871881738,
      "loss": 0.1792,
      "step": 12820
    },
    {
      "epoch": 0.7902679396365876,
      "grad_norm": 0.1804725080728531,
      "learning_rate": 0.0001473442151729802,
      "loss": 0.1786,
      "step": 12830
    },
    {
      "epoch": 0.7908838928241454,
      "grad_norm": 0.17503057420253754,
      "learning_rate": 0.00014730315162714302,
      "loss": 0.1811,
      "step": 12840
    },
    {
      "epoch": 0.7914998460117031,
      "grad_norm": 0.1600930094718933,
      "learning_rate": 0.00014726208808130584,
      "loss": 0.1822,
      "step": 12850
    },
    {
      "epoch": 0.7921157991992609,
      "grad_norm": 0.21138763427734375,
      "learning_rate": 0.00014722102453546863,
      "loss": 0.1799,
      "step": 12860
    },
    {
      "epoch": 0.7927317523868186,
      "grad_norm": 0.5586596727371216,
      "learning_rate": 0.00014717996098963148,
      "loss": 0.1801,
      "step": 12870
    },
    {
      "epoch": 0.7933477055743764,
      "grad_norm": 0.20450827479362488,
      "learning_rate": 0.00014713889744379427,
      "loss": 0.1793,
      "step": 12880
    },
    {
      "epoch": 0.7939636587619341,
      "grad_norm": 0.267076700925827,
      "learning_rate": 0.0001470978338979571,
      "loss": 0.1786,
      "step": 12890
    },
    {
      "epoch": 0.7945796119494919,
      "grad_norm": 0.17546813189983368,
      "learning_rate": 0.0001470567703521199,
      "loss": 0.182,
      "step": 12900
    },
    {
      "epoch": 0.7951955651370496,
      "grad_norm": 0.2050791084766388,
      "learning_rate": 0.00014701570680628273,
      "loss": 0.1841,
      "step": 12910
    },
    {
      "epoch": 0.7958115183246073,
      "grad_norm": 0.1479199081659317,
      "learning_rate": 0.00014697464326044555,
      "loss": 0.1795,
      "step": 12920
    },
    {
      "epoch": 0.7964274715121651,
      "grad_norm": 0.17131230235099792,
      "learning_rate": 0.00014693357971460837,
      "loss": 0.1777,
      "step": 12930
    },
    {
      "epoch": 0.7970434246997228,
      "grad_norm": 0.17512056231498718,
      "learning_rate": 0.00014689251616877117,
      "loss": 0.1804,
      "step": 12940
    },
    {
      "epoch": 0.7976593778872806,
      "grad_norm": 0.4112050235271454,
      "learning_rate": 0.000146851452622934,
      "loss": 0.1831,
      "step": 12950
    },
    {
      "epoch": 0.7982753310748383,
      "grad_norm": 0.17545601725578308,
      "learning_rate": 0.0001468103890770968,
      "loss": 0.1814,
      "step": 12960
    },
    {
      "epoch": 0.7988912842623961,
      "grad_norm": 0.173090398311615,
      "learning_rate": 0.00014676932553125963,
      "loss": 0.1799,
      "step": 12970
    },
    {
      "epoch": 0.7995072374499538,
      "grad_norm": 0.21519865095615387,
      "learning_rate": 0.00014672826198542245,
      "loss": 0.1826,
      "step": 12980
    },
    {
      "epoch": 0.8001231906375116,
      "grad_norm": 0.1705140620470047,
      "learning_rate": 0.00014668719843958527,
      "loss": 0.1813,
      "step": 12990
    },
    {
      "epoch": 0.8007391438250693,
      "grad_norm": 0.24006207287311554,
      "learning_rate": 0.00014664613489374809,
      "loss": 0.1802,
      "step": 13000
    },
    {
      "epoch": 0.8013550970126271,
      "grad_norm": 0.2084791362285614,
      "learning_rate": 0.0001466050713479109,
      "loss": 0.1803,
      "step": 13010
    },
    {
      "epoch": 0.8019710502001848,
      "grad_norm": 0.1785328984260559,
      "learning_rate": 0.0001465640078020737,
      "loss": 0.1818,
      "step": 13020
    },
    {
      "epoch": 0.8025870033877426,
      "grad_norm": 0.4503539204597473,
      "learning_rate": 0.00014652294425623655,
      "loss": 0.181,
      "step": 13030
    },
    {
      "epoch": 0.8032029565753003,
      "grad_norm": 0.2502898871898651,
      "learning_rate": 0.00014648188071039934,
      "loss": 0.1802,
      "step": 13040
    },
    {
      "epoch": 0.803818909762858,
      "grad_norm": 0.18704479932785034,
      "learning_rate": 0.00014644081716456216,
      "loss": 0.1807,
      "step": 13050
    },
    {
      "epoch": 0.8044348629504158,
      "grad_norm": 0.18992719054222107,
      "learning_rate": 0.00014639975361872498,
      "loss": 0.1821,
      "step": 13060
    },
    {
      "epoch": 0.8050508161379735,
      "grad_norm": 0.18455518782138824,
      "learning_rate": 0.0001463586900728878,
      "loss": 0.1791,
      "step": 13070
    },
    {
      "epoch": 0.8056667693255313,
      "grad_norm": 0.22897017002105713,
      "learning_rate": 0.00014631762652705062,
      "loss": 0.182,
      "step": 13080
    },
    {
      "epoch": 0.806282722513089,
      "grad_norm": 0.18445736169815063,
      "learning_rate": 0.00014627656298121344,
      "loss": 0.1843,
      "step": 13090
    },
    {
      "epoch": 0.8068986757006468,
      "grad_norm": 0.2801896035671234,
      "learning_rate": 0.00014623549943537623,
      "loss": 0.1797,
      "step": 13100
    },
    {
      "epoch": 0.8075146288882045,
      "grad_norm": 0.17341165244579315,
      "learning_rate": 0.00014619443588953908,
      "loss": 0.1803,
      "step": 13110
    },
    {
      "epoch": 0.8081305820757623,
      "grad_norm": 0.24839286506175995,
      "learning_rate": 0.00014615337234370187,
      "loss": 0.1815,
      "step": 13120
    },
    {
      "epoch": 0.80874653526332,
      "grad_norm": 0.17905673384666443,
      "learning_rate": 0.0001461123087978647,
      "loss": 0.1829,
      "step": 13130
    },
    {
      "epoch": 0.8093624884508778,
      "grad_norm": 0.16935373842716217,
      "learning_rate": 0.0001460712452520275,
      "loss": 0.1793,
      "step": 13140
    },
    {
      "epoch": 0.8099784416384355,
      "grad_norm": 0.20007891952991486,
      "learning_rate": 0.00014603018170619033,
      "loss": 0.1801,
      "step": 13150
    },
    {
      "epoch": 0.8105943948259933,
      "grad_norm": 0.1739015430212021,
      "learning_rate": 0.00014598911816035315,
      "loss": 0.1806,
      "step": 13160
    },
    {
      "epoch": 0.811210348013551,
      "grad_norm": 0.1794348955154419,
      "learning_rate": 0.00014594805461451597,
      "loss": 0.1762,
      "step": 13170
    },
    {
      "epoch": 0.8118263012011088,
      "grad_norm": 0.19082723557949066,
      "learning_rate": 0.00014590699106867877,
      "loss": 0.1805,
      "step": 13180
    },
    {
      "epoch": 0.8124422543886665,
      "grad_norm": 0.18748298287391663,
      "learning_rate": 0.0001458659275228416,
      "loss": 0.1797,
      "step": 13190
    },
    {
      "epoch": 0.8130582075762242,
      "grad_norm": 0.2022264450788498,
      "learning_rate": 0.0001458248639770044,
      "loss": 0.1819,
      "step": 13200
    },
    {
      "epoch": 0.813674160763782,
      "grad_norm": 0.18522648513317108,
      "learning_rate": 0.00014578380043116723,
      "loss": 0.1825,
      "step": 13210
    },
    {
      "epoch": 0.8142901139513397,
      "grad_norm": 0.21348781883716583,
      "learning_rate": 0.00014574273688533005,
      "loss": 0.1813,
      "step": 13220
    },
    {
      "epoch": 0.8149060671388975,
      "grad_norm": 0.22679422795772552,
      "learning_rate": 0.00014570167333949287,
      "loss": 0.1822,
      "step": 13230
    },
    {
      "epoch": 0.8155220203264552,
      "grad_norm": 0.15761759877204895,
      "learning_rate": 0.0001456606097936557,
      "loss": 0.1779,
      "step": 13240
    },
    {
      "epoch": 0.816137973514013,
      "grad_norm": 0.235340878367424,
      "learning_rate": 0.0001456195462478185,
      "loss": 0.1813,
      "step": 13250
    },
    {
      "epoch": 0.8167539267015707,
      "grad_norm": 0.16405223309993744,
      "learning_rate": 0.00014557848270198133,
      "loss": 0.1819,
      "step": 13260
    },
    {
      "epoch": 0.8173698798891285,
      "grad_norm": 0.14990125596523285,
      "learning_rate": 0.00014553741915614415,
      "loss": 0.1796,
      "step": 13270
    },
    {
      "epoch": 0.8179858330766862,
      "grad_norm": 0.21189172565937042,
      "learning_rate": 0.00014549635561030697,
      "loss": 0.1808,
      "step": 13280
    },
    {
      "epoch": 0.818601786264244,
      "grad_norm": 0.1763986349105835,
      "learning_rate": 0.00014545529206446976,
      "loss": 0.1812,
      "step": 13290
    },
    {
      "epoch": 0.8192177394518017,
      "grad_norm": 0.1441829949617386,
      "learning_rate": 0.0001454142285186326,
      "loss": 0.18,
      "step": 13300
    },
    {
      "epoch": 0.8198336926393595,
      "grad_norm": 0.20479488372802734,
      "learning_rate": 0.0001453731649727954,
      "loss": 0.1805,
      "step": 13310
    },
    {
      "epoch": 0.8204496458269172,
      "grad_norm": 0.195072740316391,
      "learning_rate": 0.00014533210142695825,
      "loss": 0.1813,
      "step": 13320
    },
    {
      "epoch": 0.8210655990144748,
      "grad_norm": 0.1738193780183792,
      "learning_rate": 0.00014529103788112104,
      "loss": 0.1789,
      "step": 13330
    },
    {
      "epoch": 0.8216815522020327,
      "grad_norm": 0.18965330719947815,
      "learning_rate": 0.00014524997433528386,
      "loss": 0.1811,
      "step": 13340
    },
    {
      "epoch": 0.8222975053895903,
      "grad_norm": 0.16641680896282196,
      "learning_rate": 0.00014520891078944668,
      "loss": 0.1785,
      "step": 13350
    },
    {
      "epoch": 0.8229134585771481,
      "grad_norm": 0.16983114182949066,
      "learning_rate": 0.0001451678472436095,
      "loss": 0.1805,
      "step": 13360
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 0.1475936323404312,
      "learning_rate": 0.00014512678369777232,
      "loss": 0.1842,
      "step": 13370
    },
    {
      "epoch": 0.8241453649522636,
      "grad_norm": 0.23705323040485382,
      "learning_rate": 0.00014508572015193514,
      "loss": 0.182,
      "step": 13380
    },
    {
      "epoch": 0.8247613181398213,
      "grad_norm": 0.28550446033477783,
      "learning_rate": 0.00014504465660609793,
      "loss": 0.1785,
      "step": 13390
    },
    {
      "epoch": 0.8253772713273791,
      "grad_norm": 0.18346232175827026,
      "learning_rate": 0.00014500359306026078,
      "loss": 0.18,
      "step": 13400
    },
    {
      "epoch": 0.8259932245149368,
      "grad_norm": 0.196979358792305,
      "learning_rate": 0.00014496252951442357,
      "loss": 0.1787,
      "step": 13410
    },
    {
      "epoch": 0.8266091777024946,
      "grad_norm": 0.21395739912986755,
      "learning_rate": 0.0001449214659685864,
      "loss": 0.1788,
      "step": 13420
    },
    {
      "epoch": 0.8272251308900523,
      "grad_norm": 0.2541826665401459,
      "learning_rate": 0.0001448804024227492,
      "loss": 0.1814,
      "step": 13430
    },
    {
      "epoch": 0.8278410840776101,
      "grad_norm": 0.21430566906929016,
      "learning_rate": 0.00014483933887691203,
      "loss": 0.1829,
      "step": 13440
    },
    {
      "epoch": 0.8284570372651678,
      "grad_norm": 0.17807747423648834,
      "learning_rate": 0.00014479827533107485,
      "loss": 0.182,
      "step": 13450
    },
    {
      "epoch": 0.8290729904527256,
      "grad_norm": 0.20006226003170013,
      "learning_rate": 0.00014475721178523767,
      "loss": 0.1794,
      "step": 13460
    },
    {
      "epoch": 0.8296889436402833,
      "grad_norm": 0.19362908601760864,
      "learning_rate": 0.00014471614823940047,
      "loss": 0.1789,
      "step": 13470
    },
    {
      "epoch": 0.830304896827841,
      "grad_norm": 0.23136091232299805,
      "learning_rate": 0.0001446750846935633,
      "loss": 0.1806,
      "step": 13480
    },
    {
      "epoch": 0.8309208500153988,
      "grad_norm": 0.22165893018245697,
      "learning_rate": 0.0001446340211477261,
      "loss": 0.1805,
      "step": 13490
    },
    {
      "epoch": 0.8315368032029565,
      "grad_norm": 0.17134395241737366,
      "learning_rate": 0.00014459295760188893,
      "loss": 0.1791,
      "step": 13500
    },
    {
      "epoch": 0.8321527563905143,
      "grad_norm": 0.20528404414653778,
      "learning_rate": 0.00014455189405605175,
      "loss": 0.178,
      "step": 13510
    },
    {
      "epoch": 0.832768709578072,
      "grad_norm": 0.2059556394815445,
      "learning_rate": 0.00014451083051021457,
      "loss": 0.1791,
      "step": 13520
    },
    {
      "epoch": 0.8333846627656298,
      "grad_norm": 0.1884719431400299,
      "learning_rate": 0.00014446976696437739,
      "loss": 0.1768,
      "step": 13530
    },
    {
      "epoch": 0.8340006159531875,
      "grad_norm": 0.24807403981685638,
      "learning_rate": 0.0001444287034185402,
      "loss": 0.1807,
      "step": 13540
    },
    {
      "epoch": 0.8346165691407453,
      "grad_norm": 0.2107633799314499,
      "learning_rate": 0.000144387639872703,
      "loss": 0.1783,
      "step": 13550
    },
    {
      "epoch": 0.835232522328303,
      "grad_norm": 0.17452393472194672,
      "learning_rate": 0.00014434657632686585,
      "loss": 0.1823,
      "step": 13560
    },
    {
      "epoch": 0.8358484755158608,
      "grad_norm": 0.19873425364494324,
      "learning_rate": 0.00014430551278102864,
      "loss": 0.1795,
      "step": 13570
    },
    {
      "epoch": 0.8364644287034185,
      "grad_norm": 0.19337812066078186,
      "learning_rate": 0.00014426444923519146,
      "loss": 0.1797,
      "step": 13580
    },
    {
      "epoch": 0.8370803818909763,
      "grad_norm": 0.1908857673406601,
      "learning_rate": 0.00014422338568935428,
      "loss": 0.1809,
      "step": 13590
    },
    {
      "epoch": 0.837696335078534,
      "grad_norm": 0.21477600932121277,
      "learning_rate": 0.0001441823221435171,
      "loss": 0.1796,
      "step": 13600
    },
    {
      "epoch": 0.8383122882660918,
      "grad_norm": 0.17034895718097687,
      "learning_rate": 0.00014414125859767992,
      "loss": 0.1798,
      "step": 13610
    },
    {
      "epoch": 0.8389282414536495,
      "grad_norm": 0.20020464062690735,
      "learning_rate": 0.00014410019505184274,
      "loss": 0.1803,
      "step": 13620
    },
    {
      "epoch": 0.8395441946412072,
      "grad_norm": 0.16508522629737854,
      "learning_rate": 0.00014405913150600553,
      "loss": 0.1789,
      "step": 13630
    },
    {
      "epoch": 0.840160147828765,
      "grad_norm": 0.218547984957695,
      "learning_rate": 0.00014401806796016838,
      "loss": 0.1843,
      "step": 13640
    },
    {
      "epoch": 0.8407761010163227,
      "grad_norm": 0.16876037418842316,
      "learning_rate": 0.00014397700441433117,
      "loss": 0.1798,
      "step": 13650
    },
    {
      "epoch": 0.8413920542038805,
      "grad_norm": 0.1827094554901123,
      "learning_rate": 0.000143935940868494,
      "loss": 0.183,
      "step": 13660
    },
    {
      "epoch": 0.8420080073914382,
      "grad_norm": 0.17925573885440826,
      "learning_rate": 0.00014389487732265684,
      "loss": 0.1813,
      "step": 13670
    },
    {
      "epoch": 0.842623960578996,
      "grad_norm": 0.19368118047714233,
      "learning_rate": 0.00014385381377681963,
      "loss": 0.1823,
      "step": 13680
    },
    {
      "epoch": 0.8432399137665537,
      "grad_norm": 0.1486286073923111,
      "learning_rate": 0.00014381275023098245,
      "loss": 0.1826,
      "step": 13690
    },
    {
      "epoch": 0.8438558669541115,
      "grad_norm": 0.4460621178150177,
      "learning_rate": 0.00014377168668514527,
      "loss": 0.1852,
      "step": 13700
    },
    {
      "epoch": 0.8444718201416692,
      "grad_norm": 0.16544292867183685,
      "learning_rate": 0.0001437306231393081,
      "loss": 0.1802,
      "step": 13710
    },
    {
      "epoch": 0.845087773329227,
      "grad_norm": 0.16816650331020355,
      "learning_rate": 0.0001436895595934709,
      "loss": 0.1784,
      "step": 13720
    },
    {
      "epoch": 0.8457037265167847,
      "grad_norm": 0.1912568360567093,
      "learning_rate": 0.00014364849604763373,
      "loss": 0.1796,
      "step": 13730
    },
    {
      "epoch": 0.8463196797043425,
      "grad_norm": 0.20465455949306488,
      "learning_rate": 0.00014360743250179653,
      "loss": 0.1836,
      "step": 13740
    },
    {
      "epoch": 0.8469356328919002,
      "grad_norm": 0.24708575010299683,
      "learning_rate": 0.00014356636895595937,
      "loss": 0.1829,
      "step": 13750
    },
    {
      "epoch": 0.8475515860794579,
      "grad_norm": 0.2877665162086487,
      "learning_rate": 0.00014352530541012217,
      "loss": 0.1785,
      "step": 13760
    },
    {
      "epoch": 0.8481675392670157,
      "grad_norm": 0.17136900126934052,
      "learning_rate": 0.00014348424186428499,
      "loss": 0.1799,
      "step": 13770
    },
    {
      "epoch": 0.8487834924545734,
      "grad_norm": 0.16969172656536102,
      "learning_rate": 0.0001434431783184478,
      "loss": 0.1791,
      "step": 13780
    },
    {
      "epoch": 0.8493994456421312,
      "grad_norm": 0.19436238706111908,
      "learning_rate": 0.00014340211477261063,
      "loss": 0.1796,
      "step": 13790
    },
    {
      "epoch": 0.8500153988296889,
      "grad_norm": 0.183698371052742,
      "learning_rate": 0.00014336105122677345,
      "loss": 0.1801,
      "step": 13800
    },
    {
      "epoch": 0.8506313520172467,
      "grad_norm": 0.18364816904067993,
      "learning_rate": 0.00014331998768093627,
      "loss": 0.1807,
      "step": 13810
    },
    {
      "epoch": 0.8512473052048044,
      "grad_norm": 0.24537870287895203,
      "learning_rate": 0.00014327892413509906,
      "loss": 0.1795,
      "step": 13820
    },
    {
      "epoch": 0.8518632583923622,
      "grad_norm": 0.19459380209445953,
      "learning_rate": 0.0001432378605892619,
      "loss": 0.1808,
      "step": 13830
    },
    {
      "epoch": 0.8524792115799199,
      "grad_norm": 0.20168431103229523,
      "learning_rate": 0.0001431967970434247,
      "loss": 0.1803,
      "step": 13840
    },
    {
      "epoch": 0.8530951647674777,
      "grad_norm": 0.171756774187088,
      "learning_rate": 0.00014315573349758752,
      "loss": 0.1802,
      "step": 13850
    },
    {
      "epoch": 0.8537111179550354,
      "grad_norm": 0.28995785117149353,
      "learning_rate": 0.00014311466995175034,
      "loss": 0.1804,
      "step": 13860
    },
    {
      "epoch": 0.8543270711425932,
      "grad_norm": 0.1763589084148407,
      "learning_rate": 0.00014307360640591316,
      "loss": 0.1765,
      "step": 13870
    },
    {
      "epoch": 0.8549430243301509,
      "grad_norm": 0.17224687337875366,
      "learning_rate": 0.00014303254286007598,
      "loss": 0.1787,
      "step": 13880
    },
    {
      "epoch": 0.8555589775177087,
      "grad_norm": 0.17854246497154236,
      "learning_rate": 0.0001429914793142388,
      "loss": 0.1781,
      "step": 13890
    },
    {
      "epoch": 0.8561749307052664,
      "grad_norm": 0.20958685874938965,
      "learning_rate": 0.0001429504157684016,
      "loss": 0.1803,
      "step": 13900
    },
    {
      "epoch": 0.8567908838928241,
      "grad_norm": 0.1894809901714325,
      "learning_rate": 0.00014290935222256444,
      "loss": 0.1837,
      "step": 13910
    },
    {
      "epoch": 0.8574068370803819,
      "grad_norm": 0.16085533797740936,
      "learning_rate": 0.00014286828867672723,
      "loss": 0.1794,
      "step": 13920
    },
    {
      "epoch": 0.8580227902679396,
      "grad_norm": 0.18845060467720032,
      "learning_rate": 0.00014282722513089005,
      "loss": 0.183,
      "step": 13930
    },
    {
      "epoch": 0.8586387434554974,
      "grad_norm": 0.18828363716602325,
      "learning_rate": 0.00014278616158505287,
      "loss": 0.1834,
      "step": 13940
    },
    {
      "epoch": 0.8592546966430551,
      "grad_norm": 0.19196899235248566,
      "learning_rate": 0.0001427450980392157,
      "loss": 0.1785,
      "step": 13950
    },
    {
      "epoch": 0.8598706498306129,
      "grad_norm": 0.22263233363628387,
      "learning_rate": 0.0001427040344933785,
      "loss": 0.1832,
      "step": 13960
    },
    {
      "epoch": 0.8604866030181706,
      "grad_norm": 0.20751333236694336,
      "learning_rate": 0.00014266297094754133,
      "loss": 0.1796,
      "step": 13970
    },
    {
      "epoch": 0.8611025562057284,
      "grad_norm": 0.17334379255771637,
      "learning_rate": 0.00014262190740170412,
      "loss": 0.1788,
      "step": 13980
    },
    {
      "epoch": 0.8617185093932861,
      "grad_norm": 0.20814302563667297,
      "learning_rate": 0.00014258084385586697,
      "loss": 0.1821,
      "step": 13990
    },
    {
      "epoch": 0.8623344625808439,
      "grad_norm": 0.1891656219959259,
      "learning_rate": 0.00014253978031002976,
      "loss": 0.1822,
      "step": 14000
    },
    {
      "epoch": 0.8629504157684016,
      "grad_norm": 0.16251477599143982,
      "learning_rate": 0.00014249871676419258,
      "loss": 0.18,
      "step": 14010
    },
    {
      "epoch": 0.8635663689559594,
      "grad_norm": 0.5846388339996338,
      "learning_rate": 0.0001424576532183554,
      "loss": 0.1802,
      "step": 14020
    },
    {
      "epoch": 0.8641823221435171,
      "grad_norm": 0.2571811079978943,
      "learning_rate": 0.00014241658967251823,
      "loss": 0.1837,
      "step": 14030
    },
    {
      "epoch": 0.8647982753310748,
      "grad_norm": 0.15673132240772247,
      "learning_rate": 0.00014237552612668105,
      "loss": 0.1795,
      "step": 14040
    },
    {
      "epoch": 0.8654142285186326,
      "grad_norm": 0.20030884444713593,
      "learning_rate": 0.00014233446258084387,
      "loss": 0.1791,
      "step": 14050
    },
    {
      "epoch": 0.8660301817061903,
      "grad_norm": 0.16350919008255005,
      "learning_rate": 0.00014229339903500666,
      "loss": 0.1791,
      "step": 14060
    },
    {
      "epoch": 0.8666461348937481,
      "grad_norm": 0.1906711459159851,
      "learning_rate": 0.0001422523354891695,
      "loss": 0.1783,
      "step": 14070
    },
    {
      "epoch": 0.8672620880813058,
      "grad_norm": 0.1859157532453537,
      "learning_rate": 0.0001422112719433323,
      "loss": 0.1808,
      "step": 14080
    },
    {
      "epoch": 0.8678780412688636,
      "grad_norm": 0.17915759980678558,
      "learning_rate": 0.00014217020839749512,
      "loss": 0.1779,
      "step": 14090
    },
    {
      "epoch": 0.8684939944564213,
      "grad_norm": 0.2005125880241394,
      "learning_rate": 0.00014212914485165797,
      "loss": 0.179,
      "step": 14100
    },
    {
      "epoch": 0.8691099476439791,
      "grad_norm": 0.1370718628168106,
      "learning_rate": 0.00014208808130582076,
      "loss": 0.1827,
      "step": 14110
    },
    {
      "epoch": 0.8697259008315368,
      "grad_norm": 0.15509457886219025,
      "learning_rate": 0.00014204701775998358,
      "loss": 0.1774,
      "step": 14120
    },
    {
      "epoch": 0.8703418540190946,
      "grad_norm": 0.15959647297859192,
      "learning_rate": 0.0001420059542141464,
      "loss": 0.1788,
      "step": 14130
    },
    {
      "epoch": 0.8709578072066523,
      "grad_norm": 0.17803679406642914,
      "learning_rate": 0.00014196489066830922,
      "loss": 0.1798,
      "step": 14140
    },
    {
      "epoch": 0.8715737603942101,
      "grad_norm": 0.19181130826473236,
      "learning_rate": 0.00014192382712247204,
      "loss": 0.1828,
      "step": 14150
    },
    {
      "epoch": 0.8721897135817678,
      "grad_norm": 0.1567433774471283,
      "learning_rate": 0.00014188276357663486,
      "loss": 0.1825,
      "step": 14160
    },
    {
      "epoch": 0.8728056667693256,
      "grad_norm": 0.24422882497310638,
      "learning_rate": 0.00014184170003079765,
      "loss": 0.1805,
      "step": 14170
    },
    {
      "epoch": 0.8734216199568833,
      "grad_norm": 0.33165624737739563,
      "learning_rate": 0.0001418006364849605,
      "loss": 0.1807,
      "step": 14180
    },
    {
      "epoch": 0.874037573144441,
      "grad_norm": 0.1470520794391632,
      "learning_rate": 0.0001417595729391233,
      "loss": 0.1805,
      "step": 14190
    },
    {
      "epoch": 0.8746535263319988,
      "grad_norm": 0.18571442365646362,
      "learning_rate": 0.00014171850939328614,
      "loss": 0.1796,
      "step": 14200
    },
    {
      "epoch": 0.8752694795195565,
      "grad_norm": 0.1804031878709793,
      "learning_rate": 0.00014167744584744893,
      "loss": 0.1782,
      "step": 14210
    },
    {
      "epoch": 0.8758854327071143,
      "grad_norm": 0.22361411154270172,
      "learning_rate": 0.00014163638230161175,
      "loss": 0.1802,
      "step": 14220
    },
    {
      "epoch": 0.876501385894672,
      "grad_norm": 0.17732007801532745,
      "learning_rate": 0.00014159531875577457,
      "loss": 0.1791,
      "step": 14230
    },
    {
      "epoch": 0.8771173390822298,
      "grad_norm": 0.2538878619670868,
      "learning_rate": 0.0001415542552099374,
      "loss": 0.1804,
      "step": 14240
    },
    {
      "epoch": 0.8777332922697875,
      "grad_norm": 0.18346206843852997,
      "learning_rate": 0.00014151319166410018,
      "loss": 0.1809,
      "step": 14250
    },
    {
      "epoch": 0.8783492454573453,
      "grad_norm": 0.20077002048492432,
      "learning_rate": 0.00014147212811826303,
      "loss": 0.1761,
      "step": 14260
    },
    {
      "epoch": 0.878965198644903,
      "grad_norm": 0.29238417744636536,
      "learning_rate": 0.00014143106457242582,
      "loss": 0.1804,
      "step": 14270
    },
    {
      "epoch": 0.8795811518324608,
      "grad_norm": 0.1782052367925644,
      "learning_rate": 0.00014139000102658867,
      "loss": 0.177,
      "step": 14280
    },
    {
      "epoch": 0.8801971050200185,
      "grad_norm": 0.1570671647787094,
      "learning_rate": 0.00014134893748075146,
      "loss": 0.1783,
      "step": 14290
    },
    {
      "epoch": 0.8808130582075763,
      "grad_norm": 0.19688287377357483,
      "learning_rate": 0.00014130787393491428,
      "loss": 0.1835,
      "step": 14300
    },
    {
      "epoch": 0.881429011395134,
      "grad_norm": 0.15185041725635529,
      "learning_rate": 0.0001412668103890771,
      "loss": 0.1813,
      "step": 14310
    },
    {
      "epoch": 0.8820449645826918,
      "grad_norm": 0.24759149551391602,
      "learning_rate": 0.00014122574684323992,
      "loss": 0.1789,
      "step": 14320
    },
    {
      "epoch": 0.8826609177702495,
      "grad_norm": 0.193844273686409,
      "learning_rate": 0.00014118468329740272,
      "loss": 0.1793,
      "step": 14330
    },
    {
      "epoch": 0.8832768709578072,
      "grad_norm": 0.1605459302663803,
      "learning_rate": 0.00014114361975156556,
      "loss": 0.1814,
      "step": 14340
    },
    {
      "epoch": 0.883892824145365,
      "grad_norm": 0.17565959692001343,
      "learning_rate": 0.00014110255620572836,
      "loss": 0.18,
      "step": 14350
    },
    {
      "epoch": 0.8845087773329227,
      "grad_norm": 0.21620985865592957,
      "learning_rate": 0.0001410614926598912,
      "loss": 0.181,
      "step": 14360
    },
    {
      "epoch": 0.8851247305204805,
      "grad_norm": 0.16970306634902954,
      "learning_rate": 0.000141020429114054,
      "loss": 0.1807,
      "step": 14370
    },
    {
      "epoch": 0.8857406837080382,
      "grad_norm": 0.16300682723522186,
      "learning_rate": 0.00014097936556821682,
      "loss": 0.1803,
      "step": 14380
    },
    {
      "epoch": 0.886356636895596,
      "grad_norm": 0.18227645754814148,
      "learning_rate": 0.00014093830202237964,
      "loss": 0.1783,
      "step": 14390
    },
    {
      "epoch": 0.8869725900831537,
      "grad_norm": 0.16467273235321045,
      "learning_rate": 0.00014089723847654246,
      "loss": 0.1796,
      "step": 14400
    },
    {
      "epoch": 0.8875885432707115,
      "grad_norm": 0.20503002405166626,
      "learning_rate": 0.00014085617493070528,
      "loss": 0.1809,
      "step": 14410
    },
    {
      "epoch": 0.8882044964582692,
      "grad_norm": 0.16116397082805634,
      "learning_rate": 0.0001408151113848681,
      "loss": 0.1773,
      "step": 14420
    },
    {
      "epoch": 0.888820449645827,
      "grad_norm": 0.1772657036781311,
      "learning_rate": 0.0001407740478390309,
      "loss": 0.1798,
      "step": 14430
    },
    {
      "epoch": 0.8894364028333847,
      "grad_norm": 0.19966207444667816,
      "learning_rate": 0.00014073298429319374,
      "loss": 0.1798,
      "step": 14440
    },
    {
      "epoch": 0.8900523560209425,
      "grad_norm": 0.195496067404747,
      "learning_rate": 0.00014069192074735653,
      "loss": 0.1804,
      "step": 14450
    },
    {
      "epoch": 0.8906683092085002,
      "grad_norm": 0.2108963429927826,
      "learning_rate": 0.00014065085720151935,
      "loss": 0.1806,
      "step": 14460
    },
    {
      "epoch": 0.8912842623960578,
      "grad_norm": 0.18571867048740387,
      "learning_rate": 0.00014060979365568217,
      "loss": 0.1805,
      "step": 14470
    },
    {
      "epoch": 0.8919002155836157,
      "grad_norm": 0.20615755021572113,
      "learning_rate": 0.000140568730109845,
      "loss": 0.18,
      "step": 14480
    },
    {
      "epoch": 0.8925161687711733,
      "grad_norm": 0.22647914290428162,
      "learning_rate": 0.0001405276665640078,
      "loss": 0.181,
      "step": 14490
    },
    {
      "epoch": 0.8931321219587312,
      "grad_norm": 0.24969370663166046,
      "learning_rate": 0.00014048660301817063,
      "loss": 0.1801,
      "step": 14500
    },
    {
      "epoch": 0.8937480751462888,
      "grad_norm": 0.21694663166999817,
      "learning_rate": 0.00014044553947233342,
      "loss": 0.1799,
      "step": 14510
    },
    {
      "epoch": 0.8943640283338467,
      "grad_norm": 0.19665463268756866,
      "learning_rate": 0.00014040447592649627,
      "loss": 0.1799,
      "step": 14520
    },
    {
      "epoch": 0.8949799815214043,
      "grad_norm": 0.1708601862192154,
      "learning_rate": 0.0001403634123806591,
      "loss": 0.1803,
      "step": 14530
    },
    {
      "epoch": 0.8955959347089621,
      "grad_norm": 0.17994754016399384,
      "learning_rate": 0.00014032234883482188,
      "loss": 0.1784,
      "step": 14540
    },
    {
      "epoch": 0.8962118878965198,
      "grad_norm": 0.18353624641895294,
      "learning_rate": 0.00014028128528898473,
      "loss": 0.1798,
      "step": 14550
    },
    {
      "epoch": 0.8968278410840776,
      "grad_norm": 0.16492009162902832,
      "learning_rate": 0.00014024022174314752,
      "loss": 0.1833,
      "step": 14560
    },
    {
      "epoch": 0.8974437942716353,
      "grad_norm": 0.19385185837745667,
      "learning_rate": 0.00014019915819731034,
      "loss": 0.1797,
      "step": 14570
    },
    {
      "epoch": 0.8980597474591931,
      "grad_norm": 0.17462261021137238,
      "learning_rate": 0.00014015809465147316,
      "loss": 0.1839,
      "step": 14580
    },
    {
      "epoch": 0.8986757006467508,
      "grad_norm": 0.29283061623573303,
      "learning_rate": 0.00014011703110563598,
      "loss": 0.1857,
      "step": 14590
    },
    {
      "epoch": 0.8992916538343086,
      "grad_norm": 0.1732633262872696,
      "learning_rate": 0.0001400759675597988,
      "loss": 0.181,
      "step": 14600
    },
    {
      "epoch": 0.8999076070218663,
      "grad_norm": 0.16310952603816986,
      "learning_rate": 0.00014003490401396162,
      "loss": 0.1792,
      "step": 14610
    },
    {
      "epoch": 0.900523560209424,
      "grad_norm": 0.16345447301864624,
      "learning_rate": 0.00013999384046812442,
      "loss": 0.1791,
      "step": 14620
    },
    {
      "epoch": 0.9011395133969818,
      "grad_norm": 0.17652404308319092,
      "learning_rate": 0.00013995277692228726,
      "loss": 0.1778,
      "step": 14630
    },
    {
      "epoch": 0.9017554665845395,
      "grad_norm": 0.16001373529434204,
      "learning_rate": 0.00013991171337645006,
      "loss": 0.1828,
      "step": 14640
    },
    {
      "epoch": 0.9023714197720973,
      "grad_norm": 0.23098430037498474,
      "learning_rate": 0.00013987064983061288,
      "loss": 0.1806,
      "step": 14650
    },
    {
      "epoch": 0.902987372959655,
      "grad_norm": 0.16301409900188446,
      "learning_rate": 0.0001398295862847757,
      "loss": 0.1803,
      "step": 14660
    },
    {
      "epoch": 0.9036033261472128,
      "grad_norm": 0.14551478624343872,
      "learning_rate": 0.00013978852273893852,
      "loss": 0.1794,
      "step": 14670
    },
    {
      "epoch": 0.9042192793347705,
      "grad_norm": 0.14069795608520508,
      "learning_rate": 0.00013974745919310134,
      "loss": 0.1791,
      "step": 14680
    },
    {
      "epoch": 0.9048352325223283,
      "grad_norm": 0.20010894536972046,
      "learning_rate": 0.00013970639564726416,
      "loss": 0.1787,
      "step": 14690
    },
    {
      "epoch": 0.905451185709886,
      "grad_norm": 0.14891500771045685,
      "learning_rate": 0.00013966533210142695,
      "loss": 0.1804,
      "step": 14700
    },
    {
      "epoch": 0.9060671388974438,
      "grad_norm": 0.5612061023712158,
      "learning_rate": 0.0001396242685555898,
      "loss": 0.1799,
      "step": 14710
    },
    {
      "epoch": 0.9066830920850015,
      "grad_norm": 0.1905858963727951,
      "learning_rate": 0.0001395832050097526,
      "loss": 0.1792,
      "step": 14720
    },
    {
      "epoch": 0.9072990452725593,
      "grad_norm": 0.21467609703540802,
      "learning_rate": 0.0001395421414639154,
      "loss": 0.1785,
      "step": 14730
    },
    {
      "epoch": 0.907914998460117,
      "grad_norm": 0.1863163262605667,
      "learning_rate": 0.00013950107791807823,
      "loss": 0.1792,
      "step": 14740
    },
    {
      "epoch": 0.9085309516476747,
      "grad_norm": 0.20520693063735962,
      "learning_rate": 0.00013946001437224105,
      "loss": 0.1816,
      "step": 14750
    },
    {
      "epoch": 0.9091469048352325,
      "grad_norm": 0.188836470246315,
      "learning_rate": 0.00013941895082640387,
      "loss": 0.1802,
      "step": 14760
    },
    {
      "epoch": 0.9097628580227902,
      "grad_norm": 0.21984396874904633,
      "learning_rate": 0.0001393778872805667,
      "loss": 0.1779,
      "step": 14770
    },
    {
      "epoch": 0.910378811210348,
      "grad_norm": 0.2486133873462677,
      "learning_rate": 0.00013933682373472948,
      "loss": 0.1801,
      "step": 14780
    },
    {
      "epoch": 0.9109947643979057,
      "grad_norm": 0.39656636118888855,
      "learning_rate": 0.00013929576018889233,
      "loss": 0.1824,
      "step": 14790
    },
    {
      "epoch": 0.9116107175854635,
      "grad_norm": 0.1774839460849762,
      "learning_rate": 0.00013925469664305512,
      "loss": 0.1826,
      "step": 14800
    },
    {
      "epoch": 0.9122266707730212,
      "grad_norm": 0.22849632799625397,
      "learning_rate": 0.00013921363309721794,
      "loss": 0.1801,
      "step": 14810
    },
    {
      "epoch": 0.912842623960579,
      "grad_norm": 0.16756145656108856,
      "learning_rate": 0.00013917256955138076,
      "loss": 0.1817,
      "step": 14820
    },
    {
      "epoch": 0.9134585771481367,
      "grad_norm": 0.1483394205570221,
      "learning_rate": 0.00013913150600554358,
      "loss": 0.18,
      "step": 14830
    },
    {
      "epoch": 0.9140745303356945,
      "grad_norm": 0.18723554909229279,
      "learning_rate": 0.0001390904424597064,
      "loss": 0.1782,
      "step": 14840
    },
    {
      "epoch": 0.9146904835232522,
      "grad_norm": 0.13920694589614868,
      "learning_rate": 0.00013904937891386922,
      "loss": 0.1841,
      "step": 14850
    },
    {
      "epoch": 0.91530643671081,
      "grad_norm": 0.20706431567668915,
      "learning_rate": 0.00013900831536803202,
      "loss": 0.1789,
      "step": 14860
    },
    {
      "epoch": 0.9159223898983677,
      "grad_norm": 0.18652290105819702,
      "learning_rate": 0.00013896725182219486,
      "loss": 0.1799,
      "step": 14870
    },
    {
      "epoch": 0.9165383430859255,
      "grad_norm": 0.1876748949289322,
      "learning_rate": 0.00013892618827635766,
      "loss": 0.1801,
      "step": 14880
    },
    {
      "epoch": 0.9171542962734832,
      "grad_norm": 0.168307363986969,
      "learning_rate": 0.00013888512473052048,
      "loss": 0.1798,
      "step": 14890
    },
    {
      "epoch": 0.9177702494610409,
      "grad_norm": 0.15696489810943604,
      "learning_rate": 0.0001388440611846833,
      "loss": 0.1806,
      "step": 14900
    },
    {
      "epoch": 0.9183862026485987,
      "grad_norm": 0.1979602873325348,
      "learning_rate": 0.00013880299763884612,
      "loss": 0.1773,
      "step": 14910
    },
    {
      "epoch": 0.9190021558361564,
      "grad_norm": 0.20473916828632355,
      "learning_rate": 0.00013876193409300894,
      "loss": 0.1805,
      "step": 14920
    },
    {
      "epoch": 0.9196181090237142,
      "grad_norm": 0.17361979186534882,
      "learning_rate": 0.00013872087054717176,
      "loss": 0.1792,
      "step": 14930
    },
    {
      "epoch": 0.9202340622112719,
      "grad_norm": 0.16687555611133575,
      "learning_rate": 0.00013867980700133458,
      "loss": 0.1788,
      "step": 14940
    },
    {
      "epoch": 0.9208500153988297,
      "grad_norm": 0.1718490570783615,
      "learning_rate": 0.0001386387434554974,
      "loss": 0.1812,
      "step": 14950
    },
    {
      "epoch": 0.9214659685863874,
      "grad_norm": 0.19034960865974426,
      "learning_rate": 0.00013859767990966022,
      "loss": 0.1814,
      "step": 14960
    },
    {
      "epoch": 0.9220819217739452,
      "grad_norm": 0.16300268471240997,
      "learning_rate": 0.000138556616363823,
      "loss": 0.1775,
      "step": 14970
    },
    {
      "epoch": 0.9226978749615029,
      "grad_norm": 0.2190343290567398,
      "learning_rate": 0.00013851555281798586,
      "loss": 0.1833,
      "step": 14980
    },
    {
      "epoch": 0.9233138281490607,
      "grad_norm": 0.20314474403858185,
      "learning_rate": 0.00013847448927214865,
      "loss": 0.1784,
      "step": 14990
    },
    {
      "epoch": 0.9239297813366184,
      "grad_norm": 0.23683157563209534,
      "learning_rate": 0.00013843342572631147,
      "loss": 0.1781,
      "step": 15000
    },
    {
      "epoch": 0.9245457345241762,
      "grad_norm": 0.1753872185945511,
      "learning_rate": 0.0001383923621804743,
      "loss": 0.1821,
      "step": 15010
    },
    {
      "epoch": 0.9251616877117339,
      "grad_norm": 0.15425221621990204,
      "learning_rate": 0.0001383512986346371,
      "loss": 0.1803,
      "step": 15020
    },
    {
      "epoch": 0.9257776408992916,
      "grad_norm": 0.17720474302768707,
      "learning_rate": 0.00013831023508879993,
      "loss": 0.1783,
      "step": 15030
    },
    {
      "epoch": 0.9263935940868494,
      "grad_norm": 0.2298808991909027,
      "learning_rate": 0.00013826917154296275,
      "loss": 0.1815,
      "step": 15040
    },
    {
      "epoch": 0.9270095472744071,
      "grad_norm": 0.1479436457157135,
      "learning_rate": 0.00013822810799712554,
      "loss": 0.1804,
      "step": 15050
    },
    {
      "epoch": 0.9276255004619649,
      "grad_norm": 0.21089211106300354,
      "learning_rate": 0.0001381870444512884,
      "loss": 0.1805,
      "step": 15060
    },
    {
      "epoch": 0.9282414536495226,
      "grad_norm": 0.18813219666481018,
      "learning_rate": 0.00013814598090545118,
      "loss": 0.1821,
      "step": 15070
    },
    {
      "epoch": 0.9288574068370804,
      "grad_norm": 0.18230129778385162,
      "learning_rate": 0.000138104917359614,
      "loss": 0.1767,
      "step": 15080
    },
    {
      "epoch": 0.9294733600246381,
      "grad_norm": 0.1872679889202118,
      "learning_rate": 0.00013806385381377682,
      "loss": 0.1802,
      "step": 15090
    },
    {
      "epoch": 0.9300893132121959,
      "grad_norm": 0.22724401950836182,
      "learning_rate": 0.00013802279026793964,
      "loss": 0.1784,
      "step": 15100
    },
    {
      "epoch": 0.9307052663997536,
      "grad_norm": 0.19494351744651794,
      "learning_rate": 0.00013798172672210246,
      "loss": 0.1795,
      "step": 15110
    },
    {
      "epoch": 0.9313212195873114,
      "grad_norm": 0.1346655637025833,
      "learning_rate": 0.00013794066317626528,
      "loss": 0.1787,
      "step": 15120
    },
    {
      "epoch": 0.9319371727748691,
      "grad_norm": 0.17862895131111145,
      "learning_rate": 0.00013789959963042808,
      "loss": 0.1791,
      "step": 15130
    },
    {
      "epoch": 0.9325531259624269,
      "grad_norm": 0.1526649445295334,
      "learning_rate": 0.00013785853608459092,
      "loss": 0.1799,
      "step": 15140
    },
    {
      "epoch": 0.9331690791499846,
      "grad_norm": 0.21123401820659637,
      "learning_rate": 0.00013781747253875372,
      "loss": 0.1802,
      "step": 15150
    },
    {
      "epoch": 0.9337850323375424,
      "grad_norm": 0.17973285913467407,
      "learning_rate": 0.00013777640899291654,
      "loss": 0.1775,
      "step": 15160
    },
    {
      "epoch": 0.9344009855251001,
      "grad_norm": 0.21534064412117004,
      "learning_rate": 0.00013773534544707936,
      "loss": 0.1777,
      "step": 15170
    },
    {
      "epoch": 0.9350169387126578,
      "grad_norm": 0.20596206188201904,
      "learning_rate": 0.00013769428190124218,
      "loss": 0.1828,
      "step": 15180
    },
    {
      "epoch": 0.9356328919002156,
      "grad_norm": 0.1781972348690033,
      "learning_rate": 0.000137653218355405,
      "loss": 0.18,
      "step": 15190
    },
    {
      "epoch": 0.9362488450877733,
      "grad_norm": 0.1702168732881546,
      "learning_rate": 0.00013761215480956782,
      "loss": 0.1797,
      "step": 15200
    },
    {
      "epoch": 0.9368647982753311,
      "grad_norm": 0.1516159176826477,
      "learning_rate": 0.0001375710912637306,
      "loss": 0.1777,
      "step": 15210
    },
    {
      "epoch": 0.9374807514628888,
      "grad_norm": 0.1803581565618515,
      "learning_rate": 0.00013753002771789346,
      "loss": 0.1798,
      "step": 15220
    },
    {
      "epoch": 0.9380967046504466,
      "grad_norm": 0.16219009459018707,
      "learning_rate": 0.00013748896417205625,
      "loss": 0.1817,
      "step": 15230
    },
    {
      "epoch": 0.9387126578380043,
      "grad_norm": 0.1711435467004776,
      "learning_rate": 0.0001374479006262191,
      "loss": 0.1781,
      "step": 15240
    },
    {
      "epoch": 0.9393286110255621,
      "grad_norm": 0.1575554609298706,
      "learning_rate": 0.0001374068370803819,
      "loss": 0.1808,
      "step": 15250
    },
    {
      "epoch": 0.9399445642131198,
      "grad_norm": 0.29258736968040466,
      "learning_rate": 0.0001373657735345447,
      "loss": 0.1839,
      "step": 15260
    },
    {
      "epoch": 0.9405605174006776,
      "grad_norm": 0.1708838939666748,
      "learning_rate": 0.00013732470998870753,
      "loss": 0.181,
      "step": 15270
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 0.24146243929862976,
      "learning_rate": 0.00013728364644287035,
      "loss": 0.1812,
      "step": 15280
    },
    {
      "epoch": 0.9417924237757931,
      "grad_norm": 0.19407832622528076,
      "learning_rate": 0.00013724258289703314,
      "loss": 0.1805,
      "step": 15290
    },
    {
      "epoch": 0.9424083769633508,
      "grad_norm": 0.1820274144411087,
      "learning_rate": 0.000137201519351196,
      "loss": 0.1766,
      "step": 15300
    },
    {
      "epoch": 0.9430243301509086,
      "grad_norm": 0.17628397047519684,
      "learning_rate": 0.00013716045580535878,
      "loss": 0.1809,
      "step": 15310
    },
    {
      "epoch": 0.9436402833384663,
      "grad_norm": 0.18571801483631134,
      "learning_rate": 0.00013711939225952163,
      "loss": 0.1778,
      "step": 15320
    },
    {
      "epoch": 0.944256236526024,
      "grad_norm": 0.18947075307369232,
      "learning_rate": 0.00013707832871368442,
      "loss": 0.1802,
      "step": 15330
    },
    {
      "epoch": 0.9448721897135818,
      "grad_norm": 0.16976146399974823,
      "learning_rate": 0.00013703726516784724,
      "loss": 0.1846,
      "step": 15340
    },
    {
      "epoch": 0.9454881429011395,
      "grad_norm": 0.20653553307056427,
      "learning_rate": 0.00013699620162201006,
      "loss": 0.1818,
      "step": 15350
    },
    {
      "epoch": 0.9461040960886973,
      "grad_norm": 0.8803553581237793,
      "learning_rate": 0.00013695513807617288,
      "loss": 0.1798,
      "step": 15360
    },
    {
      "epoch": 0.946720049276255,
      "grad_norm": 0.17746169865131378,
      "learning_rate": 0.0001369140745303357,
      "loss": 0.1782,
      "step": 15370
    },
    {
      "epoch": 0.9473360024638128,
      "grad_norm": 0.16425146162509918,
      "learning_rate": 0.00013687301098449852,
      "loss": 0.1824,
      "step": 15380
    },
    {
      "epoch": 0.9479519556513705,
      "grad_norm": 0.14507193863391876,
      "learning_rate": 0.00013683194743866134,
      "loss": 0.178,
      "step": 15390
    },
    {
      "epoch": 0.9485679088389283,
      "grad_norm": 0.2021486908197403,
      "learning_rate": 0.00013679088389282416,
      "loss": 0.1799,
      "step": 15400
    },
    {
      "epoch": 0.949183862026486,
      "grad_norm": 0.15256619453430176,
      "learning_rate": 0.00013674982034698698,
      "loss": 0.1781,
      "step": 15410
    },
    {
      "epoch": 0.9497998152140438,
      "grad_norm": 0.1845187097787857,
      "learning_rate": 0.00013670875680114978,
      "loss": 0.18,
      "step": 15420
    },
    {
      "epoch": 0.9504157684016015,
      "grad_norm": 0.15598848462104797,
      "learning_rate": 0.00013666769325531262,
      "loss": 0.181,
      "step": 15430
    },
    {
      "epoch": 0.9510317215891593,
      "grad_norm": 0.18392927944660187,
      "learning_rate": 0.00013662662970947542,
      "loss": 0.1835,
      "step": 15440
    },
    {
      "epoch": 0.951647674776717,
      "grad_norm": 0.5338690280914307,
      "learning_rate": 0.00013658556616363824,
      "loss": 0.1797,
      "step": 15450
    },
    {
      "epoch": 0.9522636279642747,
      "grad_norm": 0.37198585271835327,
      "learning_rate": 0.00013654450261780106,
      "loss": 0.178,
      "step": 15460
    },
    {
      "epoch": 0.9528795811518325,
      "grad_norm": 0.17380686104297638,
      "learning_rate": 0.00013650343907196388,
      "loss": 0.18,
      "step": 15470
    },
    {
      "epoch": 0.9534955343393902,
      "grad_norm": 0.6100925207138062,
      "learning_rate": 0.0001364623755261267,
      "loss": 0.1802,
      "step": 15480
    },
    {
      "epoch": 0.954111487526948,
      "grad_norm": 0.20397573709487915,
      "learning_rate": 0.00013642131198028952,
      "loss": 0.1793,
      "step": 15490
    },
    {
      "epoch": 0.9547274407145057,
      "grad_norm": 0.16940279304981232,
      "learning_rate": 0.0001363802484344523,
      "loss": 0.1813,
      "step": 15500
    },
    {
      "epoch": 0.9553433939020635,
      "grad_norm": 0.5324698686599731,
      "learning_rate": 0.00013633918488861516,
      "loss": 0.1866,
      "step": 15510
    },
    {
      "epoch": 0.9559593470896212,
      "grad_norm": 0.1727200299501419,
      "learning_rate": 0.00013629812134277795,
      "loss": 0.1767,
      "step": 15520
    },
    {
      "epoch": 0.956575300277179,
      "grad_norm": 0.18826612830162048,
      "learning_rate": 0.00013625705779694077,
      "loss": 0.1816,
      "step": 15530
    },
    {
      "epoch": 0.9571912534647367,
      "grad_norm": 0.22241580486297607,
      "learning_rate": 0.0001362159942511036,
      "loss": 0.1795,
      "step": 15540
    },
    {
      "epoch": 0.9578072066522945,
      "grad_norm": 0.18210123479366302,
      "learning_rate": 0.0001361749307052664,
      "loss": 0.1837,
      "step": 15550
    },
    {
      "epoch": 0.9584231598398522,
      "grad_norm": 0.17325466871261597,
      "learning_rate": 0.00013613386715942923,
      "loss": 0.1825,
      "step": 15560
    },
    {
      "epoch": 0.95903911302741,
      "grad_norm": 0.20479343831539154,
      "learning_rate": 0.00013609280361359205,
      "loss": 0.1815,
      "step": 15570
    },
    {
      "epoch": 0.9596550662149677,
      "grad_norm": 0.16068917512893677,
      "learning_rate": 0.00013605174006775484,
      "loss": 0.179,
      "step": 15580
    },
    {
      "epoch": 0.9602710194025255,
      "grad_norm": 0.2704722583293915,
      "learning_rate": 0.0001360106765219177,
      "loss": 0.1814,
      "step": 15590
    },
    {
      "epoch": 0.9608869725900832,
      "grad_norm": 1.5123602151870728,
      "learning_rate": 0.00013596961297608048,
      "loss": 0.1796,
      "step": 15600
    },
    {
      "epoch": 0.9615029257776408,
      "grad_norm": 0.17799894511699677,
      "learning_rate": 0.0001359285494302433,
      "loss": 0.1799,
      "step": 15610
    },
    {
      "epoch": 0.9621188789651987,
      "grad_norm": 0.16041775047779083,
      "learning_rate": 0.00013588748588440612,
      "loss": 0.1812,
      "step": 15620
    },
    {
      "epoch": 0.9627348321527563,
      "grad_norm": 0.16988429427146912,
      "learning_rate": 0.00013584642233856894,
      "loss": 0.1834,
      "step": 15630
    },
    {
      "epoch": 0.9633507853403142,
      "grad_norm": 0.1735873818397522,
      "learning_rate": 0.00013580535879273176,
      "loss": 0.18,
      "step": 15640
    },
    {
      "epoch": 0.9639667385278718,
      "grad_norm": 0.8097873330116272,
      "learning_rate": 0.00013576429524689458,
      "loss": 0.1809,
      "step": 15650
    },
    {
      "epoch": 0.9645826917154297,
      "grad_norm": 0.23341308534145355,
      "learning_rate": 0.00013572323170105738,
      "loss": 0.1816,
      "step": 15660
    },
    {
      "epoch": 0.9651986449029873,
      "grad_norm": 0.1870182305574417,
      "learning_rate": 0.00013568216815522022,
      "loss": 0.1791,
      "step": 15670
    },
    {
      "epoch": 0.9658145980905452,
      "grad_norm": 0.17194148898124695,
      "learning_rate": 0.00013564110460938302,
      "loss": 0.1789,
      "step": 15680
    },
    {
      "epoch": 0.9664305512781028,
      "grad_norm": 0.21233756840229034,
      "learning_rate": 0.00013560004106354584,
      "loss": 0.1809,
      "step": 15690
    },
    {
      "epoch": 0.9670465044656607,
      "grad_norm": 0.2125210165977478,
      "learning_rate": 0.00013555897751770866,
      "loss": 0.1817,
      "step": 15700
    },
    {
      "epoch": 0.9676624576532183,
      "grad_norm": 0.19071628153324127,
      "learning_rate": 0.00013551791397187148,
      "loss": 0.1806,
      "step": 15710
    },
    {
      "epoch": 0.9682784108407761,
      "grad_norm": 0.18719561398029327,
      "learning_rate": 0.0001354768504260343,
      "loss": 0.1835,
      "step": 15720
    },
    {
      "epoch": 0.9688943640283338,
      "grad_norm": 0.15388862788677216,
      "learning_rate": 0.00013543578688019712,
      "loss": 0.18,
      "step": 15730
    },
    {
      "epoch": 0.9695103172158915,
      "grad_norm": 0.2033100426197052,
      "learning_rate": 0.0001353947233343599,
      "loss": 0.178,
      "step": 15740
    },
    {
      "epoch": 0.9701262704034493,
      "grad_norm": 0.1650751680135727,
      "learning_rate": 0.00013535365978852276,
      "loss": 0.1778,
      "step": 15750
    },
    {
      "epoch": 0.970742223591007,
      "grad_norm": 0.16806894540786743,
      "learning_rate": 0.00013531259624268555,
      "loss": 0.1799,
      "step": 15760
    },
    {
      "epoch": 0.9713581767785648,
      "grad_norm": 0.17098934948444366,
      "learning_rate": 0.00013527153269684837,
      "loss": 0.1811,
      "step": 15770
    },
    {
      "epoch": 0.9719741299661225,
      "grad_norm": 0.21840593218803406,
      "learning_rate": 0.0001352304691510112,
      "loss": 0.1809,
      "step": 15780
    },
    {
      "epoch": 0.9725900831536803,
      "grad_norm": 0.1511109471321106,
      "learning_rate": 0.000135189405605174,
      "loss": 0.181,
      "step": 15790
    },
    {
      "epoch": 0.973206036341238,
      "grad_norm": 0.1587095707654953,
      "learning_rate": 0.00013514834205933683,
      "loss": 0.1803,
      "step": 15800
    },
    {
      "epoch": 0.9738219895287958,
      "grad_norm": 0.19552282989025116,
      "learning_rate": 0.00013510727851349965,
      "loss": 0.1806,
      "step": 15810
    },
    {
      "epoch": 0.9744379427163535,
      "grad_norm": 0.1952512413263321,
      "learning_rate": 0.00013506621496766247,
      "loss": 0.1794,
      "step": 15820
    },
    {
      "epoch": 0.9750538959039113,
      "grad_norm": 0.17676474153995514,
      "learning_rate": 0.0001350251514218253,
      "loss": 0.178,
      "step": 15830
    },
    {
      "epoch": 0.975669849091469,
      "grad_norm": 0.15174400806427002,
      "learning_rate": 0.0001349840878759881,
      "loss": 0.1771,
      "step": 15840
    },
    {
      "epoch": 0.9762858022790268,
      "grad_norm": 0.1765424907207489,
      "learning_rate": 0.0001349430243301509,
      "loss": 0.1793,
      "step": 15850
    },
    {
      "epoch": 0.9769017554665845,
      "grad_norm": 0.16352026164531708,
      "learning_rate": 0.00013490196078431375,
      "loss": 0.1794,
      "step": 15860
    },
    {
      "epoch": 0.9775177086541423,
      "grad_norm": 0.22154924273490906,
      "learning_rate": 0.00013486089723847654,
      "loss": 0.1803,
      "step": 15870
    },
    {
      "epoch": 0.9781336618417,
      "grad_norm": 0.21690179407596588,
      "learning_rate": 0.00013481983369263936,
      "loss": 0.1789,
      "step": 15880
    },
    {
      "epoch": 0.9787496150292577,
      "grad_norm": 0.18385787308216095,
      "learning_rate": 0.00013477877014680218,
      "loss": 0.178,
      "step": 15890
    },
    {
      "epoch": 0.9793655682168155,
      "grad_norm": 0.1514112651348114,
      "learning_rate": 0.000134737706600965,
      "loss": 0.1798,
      "step": 15900
    },
    {
      "epoch": 0.9799815214043732,
      "grad_norm": 0.17432637512683868,
      "learning_rate": 0.00013469664305512782,
      "loss": 0.1817,
      "step": 15910
    },
    {
      "epoch": 0.980597474591931,
      "grad_norm": 0.18521897494792938,
      "learning_rate": 0.00013465557950929064,
      "loss": 0.1799,
      "step": 15920
    },
    {
      "epoch": 0.9812134277794887,
      "grad_norm": 0.17630039155483246,
      "learning_rate": 0.00013461451596345344,
      "loss": 0.1792,
      "step": 15930
    },
    {
      "epoch": 0.9818293809670465,
      "grad_norm": 0.15895280241966248,
      "learning_rate": 0.00013457345241761628,
      "loss": 0.1784,
      "step": 15940
    },
    {
      "epoch": 0.9824453341546042,
      "grad_norm": 0.1749628186225891,
      "learning_rate": 0.00013453238887177908,
      "loss": 0.1824,
      "step": 15950
    },
    {
      "epoch": 0.983061287342162,
      "grad_norm": 0.19746814668178558,
      "learning_rate": 0.0001344913253259419,
      "loss": 0.1789,
      "step": 15960
    },
    {
      "epoch": 0.9836772405297197,
      "grad_norm": 0.1811821460723877,
      "learning_rate": 0.00013445026178010472,
      "loss": 0.1804,
      "step": 15970
    },
    {
      "epoch": 0.9842931937172775,
      "grad_norm": 0.16869163513183594,
      "learning_rate": 0.00013440919823426754,
      "loss": 0.1795,
      "step": 15980
    },
    {
      "epoch": 0.9849091469048352,
      "grad_norm": 0.16755205392837524,
      "learning_rate": 0.00013436813468843036,
      "loss": 0.1779,
      "step": 15990
    },
    {
      "epoch": 0.985525100092393,
      "grad_norm": 0.3127090632915497,
      "learning_rate": 0.00013432707114259318,
      "loss": 0.1782,
      "step": 16000
    },
    {
      "epoch": 0.9861410532799507,
      "grad_norm": 0.2113489955663681,
      "learning_rate": 0.00013428600759675597,
      "loss": 0.1792,
      "step": 16010
    },
    {
      "epoch": 0.9867570064675085,
      "grad_norm": 0.184927836060524,
      "learning_rate": 0.00013424494405091882,
      "loss": 0.1814,
      "step": 16020
    },
    {
      "epoch": 0.9873729596550662,
      "grad_norm": 0.2050580531358719,
      "learning_rate": 0.0001342038805050816,
      "loss": 0.1817,
      "step": 16030
    },
    {
      "epoch": 0.9879889128426239,
      "grad_norm": 0.12932296097278595,
      "learning_rate": 0.00013416281695924443,
      "loss": 0.1802,
      "step": 16040
    },
    {
      "epoch": 0.9886048660301817,
      "grad_norm": 0.29394710063934326,
      "learning_rate": 0.00013412175341340725,
      "loss": 0.1774,
      "step": 16050
    },
    {
      "epoch": 0.9892208192177394,
      "grad_norm": 0.19372321665287018,
      "learning_rate": 0.00013408068986757007,
      "loss": 0.1795,
      "step": 16060
    },
    {
      "epoch": 0.9898367724052972,
      "grad_norm": 0.16096875071525574,
      "learning_rate": 0.0001340396263217329,
      "loss": 0.1799,
      "step": 16070
    },
    {
      "epoch": 0.9904527255928549,
      "grad_norm": 0.18461580574512482,
      "learning_rate": 0.0001339985627758957,
      "loss": 0.1792,
      "step": 16080
    },
    {
      "epoch": 0.9910686787804127,
      "grad_norm": 0.1634122133255005,
      "learning_rate": 0.0001339574992300585,
      "loss": 0.1772,
      "step": 16090
    },
    {
      "epoch": 0.9916846319679704,
      "grad_norm": 0.1748664826154709,
      "learning_rate": 0.00013391643568422135,
      "loss": 0.1792,
      "step": 16100
    },
    {
      "epoch": 0.9923005851555282,
      "grad_norm": 0.16288968920707703,
      "learning_rate": 0.00013387537213838414,
      "loss": 0.1809,
      "step": 16110
    },
    {
      "epoch": 0.9929165383430859,
      "grad_norm": 0.16876639425754547,
      "learning_rate": 0.00013383430859254696,
      "loss": 0.18,
      "step": 16120
    },
    {
      "epoch": 0.9935324915306437,
      "grad_norm": 0.1686469167470932,
      "learning_rate": 0.00013379324504670978,
      "loss": 0.1815,
      "step": 16130
    },
    {
      "epoch": 0.9941484447182014,
      "grad_norm": 0.17883603274822235,
      "learning_rate": 0.0001337521815008726,
      "loss": 0.1767,
      "step": 16140
    },
    {
      "epoch": 0.9947643979057592,
      "grad_norm": 0.14385448396205902,
      "learning_rate": 0.00013371111795503542,
      "loss": 0.1798,
      "step": 16150
    },
    {
      "epoch": 0.9953803510933169,
      "grad_norm": 0.16903239488601685,
      "learning_rate": 0.00013367005440919824,
      "loss": 0.1828,
      "step": 16160
    },
    {
      "epoch": 0.9959963042808746,
      "grad_norm": 0.20136739313602448,
      "learning_rate": 0.00013362899086336104,
      "loss": 0.1774,
      "step": 16170
    },
    {
      "epoch": 0.9966122574684324,
      "grad_norm": 0.18741358816623688,
      "learning_rate": 0.00013358792731752388,
      "loss": 0.178,
      "step": 16180
    },
    {
      "epoch": 0.9972282106559901,
      "grad_norm": 0.15028506517410278,
      "learning_rate": 0.00013354686377168668,
      "loss": 0.1787,
      "step": 16190
    },
    {
      "epoch": 0.9978441638435479,
      "grad_norm": 0.16845151782035828,
      "learning_rate": 0.0001335058002258495,
      "loss": 0.1801,
      "step": 16200
    },
    {
      "epoch": 0.9984601170311056,
      "grad_norm": 0.1725071519613266,
      "learning_rate": 0.00013346473668001232,
      "loss": 0.1793,
      "step": 16210
    },
    {
      "epoch": 0.9990760702186634,
      "grad_norm": 0.16889315843582153,
      "learning_rate": 0.00013342367313417514,
      "loss": 0.1824,
      "step": 16220
    },
    {
      "epoch": 0.9996920234062211,
      "grad_norm": 0.16454409062862396,
      "learning_rate": 0.00013338260958833798,
      "loss": 0.181,
      "step": 16230
    },
    {
      "epoch": 1.0003079765937788,
      "grad_norm": 0.16416873037815094,
      "learning_rate": 0.00013334154604250078,
      "loss": 0.1786,
      "step": 16240
    },
    {
      "epoch": 1.0009239297813366,
      "grad_norm": 0.15495650470256805,
      "learning_rate": 0.0001333004824966636,
      "loss": 0.1807,
      "step": 16250
    },
    {
      "epoch": 1.0015398829688944,
      "grad_norm": 0.18338291347026825,
      "learning_rate": 0.00013325941895082642,
      "loss": 0.1807,
      "step": 16260
    },
    {
      "epoch": 1.0021558361564522,
      "grad_norm": 0.23171842098236084,
      "learning_rate": 0.00013321835540498924,
      "loss": 0.1792,
      "step": 16270
    },
    {
      "epoch": 1.0027717893440098,
      "grad_norm": 0.16565461456775665,
      "learning_rate": 0.00013317729185915206,
      "loss": 0.18,
      "step": 16280
    },
    {
      "epoch": 1.0033877425315676,
      "grad_norm": 0.369819313287735,
      "learning_rate": 0.00013313622831331488,
      "loss": 0.1818,
      "step": 16290
    },
    {
      "epoch": 1.0040036957191254,
      "grad_norm": 0.16879746317863464,
      "learning_rate": 0.00013309516476747767,
      "loss": 0.1791,
      "step": 16300
    },
    {
      "epoch": 1.004619648906683,
      "grad_norm": 0.1732664406299591,
      "learning_rate": 0.00013305410122164052,
      "loss": 0.1797,
      "step": 16310
    },
    {
      "epoch": 1.0052356020942408,
      "grad_norm": 0.18247514963150024,
      "learning_rate": 0.0001330130376758033,
      "loss": 0.1818,
      "step": 16320
    },
    {
      "epoch": 1.0058515552817986,
      "grad_norm": 0.17401982843875885,
      "learning_rate": 0.00013297197412996613,
      "loss": 0.1786,
      "step": 16330
    },
    {
      "epoch": 1.0064675084693564,
      "grad_norm": 0.16025976836681366,
      "learning_rate": 0.00013293091058412895,
      "loss": 0.1771,
      "step": 16340
    },
    {
      "epoch": 1.007083461656914,
      "grad_norm": 0.15526491403579712,
      "learning_rate": 0.00013288984703829177,
      "loss": 0.1779,
      "step": 16350
    },
    {
      "epoch": 1.0076994148444718,
      "grad_norm": 0.19323284924030304,
      "learning_rate": 0.0001328487834924546,
      "loss": 0.1787,
      "step": 16360
    },
    {
      "epoch": 1.0083153680320296,
      "grad_norm": 0.17541459202766418,
      "learning_rate": 0.0001328077199466174,
      "loss": 0.18,
      "step": 16370
    },
    {
      "epoch": 1.0089313212195874,
      "grad_norm": 0.17831110954284668,
      "learning_rate": 0.0001327666564007802,
      "loss": 0.1807,
      "step": 16380
    },
    {
      "epoch": 1.009547274407145,
      "grad_norm": 0.15851502120494843,
      "learning_rate": 0.00013272559285494305,
      "loss": 0.1804,
      "step": 16390
    },
    {
      "epoch": 1.0101632275947028,
      "grad_norm": 0.15989217162132263,
      "learning_rate": 0.00013268452930910584,
      "loss": 0.1783,
      "step": 16400
    },
    {
      "epoch": 1.0107791807822606,
      "grad_norm": 0.23070909082889557,
      "learning_rate": 0.00013264346576326866,
      "loss": 0.1782,
      "step": 16410
    },
    {
      "epoch": 1.0113951339698184,
      "grad_norm": 0.1537352055311203,
      "learning_rate": 0.00013260240221743148,
      "loss": 0.1811,
      "step": 16420
    },
    {
      "epoch": 1.012011087157376,
      "grad_norm": 0.1433056741952896,
      "learning_rate": 0.0001325613386715943,
      "loss": 0.1776,
      "step": 16430
    },
    {
      "epoch": 1.0126270403449338,
      "grad_norm": 0.1998724490404129,
      "learning_rate": 0.00013252027512575712,
      "loss": 0.1807,
      "step": 16440
    },
    {
      "epoch": 1.0132429935324916,
      "grad_norm": 0.18265371024608612,
      "learning_rate": 0.00013247921157991994,
      "loss": 0.1785,
      "step": 16450
    },
    {
      "epoch": 1.0138589467200492,
      "grad_norm": 0.2020517736673355,
      "learning_rate": 0.00013243814803408274,
      "loss": 0.1806,
      "step": 16460
    },
    {
      "epoch": 1.014474899907607,
      "grad_norm": 0.17719492316246033,
      "learning_rate": 0.00013239708448824558,
      "loss": 0.1795,
      "step": 16470
    },
    {
      "epoch": 1.0150908530951648,
      "grad_norm": 0.1324494183063507,
      "learning_rate": 0.00013235602094240838,
      "loss": 0.1785,
      "step": 16480
    },
    {
      "epoch": 1.0157068062827226,
      "grad_norm": 0.15134112536907196,
      "learning_rate": 0.0001323149573965712,
      "loss": 0.1811,
      "step": 16490
    },
    {
      "epoch": 1.0163227594702802,
      "grad_norm": 0.1838676482439041,
      "learning_rate": 0.00013227389385073402,
      "loss": 0.1788,
      "step": 16500
    },
    {
      "epoch": 1.016938712657838,
      "grad_norm": 0.17033042013645172,
      "learning_rate": 0.00013223283030489684,
      "loss": 0.1816,
      "step": 16510
    },
    {
      "epoch": 1.0175546658453958,
      "grad_norm": 0.16784748435020447,
      "learning_rate": 0.00013219176675905966,
      "loss": 0.1778,
      "step": 16520
    },
    {
      "epoch": 1.0181706190329536,
      "grad_norm": 0.1992085576057434,
      "learning_rate": 0.00013215070321322248,
      "loss": 0.1788,
      "step": 16530
    },
    {
      "epoch": 1.0187865722205112,
      "grad_norm": 0.16671772301197052,
      "learning_rate": 0.00013210963966738527,
      "loss": 0.1809,
      "step": 16540
    },
    {
      "epoch": 1.019402525408069,
      "grad_norm": 0.16915293037891388,
      "learning_rate": 0.00013206857612154812,
      "loss": 0.1808,
      "step": 16550
    },
    {
      "epoch": 1.0200184785956268,
      "grad_norm": 0.21260954439640045,
      "learning_rate": 0.0001320275125757109,
      "loss": 0.1792,
      "step": 16560
    },
    {
      "epoch": 1.0206344317831846,
      "grad_norm": 0.18983609974384308,
      "learning_rate": 0.00013198644902987373,
      "loss": 0.1801,
      "step": 16570
    },
    {
      "epoch": 1.0212503849707422,
      "grad_norm": 0.13217759132385254,
      "learning_rate": 0.00013194538548403655,
      "loss": 0.1776,
      "step": 16580
    },
    {
      "epoch": 1.0218663381583,
      "grad_norm": 0.1612788736820221,
      "learning_rate": 0.00013190432193819937,
      "loss": 0.1786,
      "step": 16590
    },
    {
      "epoch": 1.0224822913458578,
      "grad_norm": 0.19692718982696533,
      "learning_rate": 0.0001318632583923622,
      "loss": 0.1786,
      "step": 16600
    },
    {
      "epoch": 1.0230982445334154,
      "grad_norm": 0.16359445452690125,
      "learning_rate": 0.000131822194846525,
      "loss": 0.1803,
      "step": 16610
    },
    {
      "epoch": 1.0237141977209732,
      "grad_norm": 0.13801400363445282,
      "learning_rate": 0.0001317811313006878,
      "loss": 0.1803,
      "step": 16620
    },
    {
      "epoch": 1.024330150908531,
      "grad_norm": 0.28769591450691223,
      "learning_rate": 0.00013174006775485065,
      "loss": 0.1802,
      "step": 16630
    },
    {
      "epoch": 1.0249461040960888,
      "grad_norm": 0.18719616532325745,
      "learning_rate": 0.00013169900420901347,
      "loss": 0.18,
      "step": 16640
    },
    {
      "epoch": 1.0255620572836464,
      "grad_norm": 0.15610730648040771,
      "learning_rate": 0.00013165794066317626,
      "loss": 0.1764,
      "step": 16650
    },
    {
      "epoch": 1.0261780104712042,
      "grad_norm": 0.152836412191391,
      "learning_rate": 0.0001316168771173391,
      "loss": 0.1803,
      "step": 16660
    },
    {
      "epoch": 1.026793963658762,
      "grad_norm": 0.1957559585571289,
      "learning_rate": 0.0001315758135715019,
      "loss": 0.1779,
      "step": 16670
    },
    {
      "epoch": 1.0274099168463198,
      "grad_norm": 0.2341579645872116,
      "learning_rate": 0.00013153475002566472,
      "loss": 0.1792,
      "step": 16680
    },
    {
      "epoch": 1.0280258700338774,
      "grad_norm": 0.17701031267642975,
      "learning_rate": 0.00013149368647982754,
      "loss": 0.1783,
      "step": 16690
    },
    {
      "epoch": 1.0286418232214352,
      "grad_norm": 0.1569625437259674,
      "learning_rate": 0.00013145262293399036,
      "loss": 0.179,
      "step": 16700
    },
    {
      "epoch": 1.029257776408993,
      "grad_norm": 0.18232472240924835,
      "learning_rate": 0.00013141155938815318,
      "loss": 0.1784,
      "step": 16710
    },
    {
      "epoch": 1.0298737295965508,
      "grad_norm": 0.16033534705638885,
      "learning_rate": 0.000131370495842316,
      "loss": 0.1799,
      "step": 16720
    },
    {
      "epoch": 1.0304896827841084,
      "grad_norm": 0.1948264241218567,
      "learning_rate": 0.0001313294322964788,
      "loss": 0.1799,
      "step": 16730
    },
    {
      "epoch": 1.0311056359716662,
      "grad_norm": 0.15684308111667633,
      "learning_rate": 0.00013128836875064164,
      "loss": 0.1769,
      "step": 16740
    },
    {
      "epoch": 1.031721589159224,
      "grad_norm": 0.1656811386346817,
      "learning_rate": 0.00013124730520480444,
      "loss": 0.1811,
      "step": 16750
    },
    {
      "epoch": 1.0323375423467815,
      "grad_norm": 0.16070902347564697,
      "learning_rate": 0.00013120624165896726,
      "loss": 0.1773,
      "step": 16760
    },
    {
      "epoch": 1.0329534955343393,
      "grad_norm": 0.1571994125843048,
      "learning_rate": 0.00013116517811313008,
      "loss": 0.1776,
      "step": 16770
    },
    {
      "epoch": 1.0335694487218972,
      "grad_norm": 0.1549055129289627,
      "learning_rate": 0.0001311241145672929,
      "loss": 0.1791,
      "step": 16780
    },
    {
      "epoch": 1.034185401909455,
      "grad_norm": 0.20193029940128326,
      "learning_rate": 0.00013108305102145572,
      "loss": 0.1823,
      "step": 16790
    },
    {
      "epoch": 1.0348013550970125,
      "grad_norm": 0.15332917869091034,
      "learning_rate": 0.00013104198747561854,
      "loss": 0.1766,
      "step": 16800
    },
    {
      "epoch": 1.0354173082845703,
      "grad_norm": 0.20855754613876343,
      "learning_rate": 0.00013100092392978133,
      "loss": 0.1787,
      "step": 16810
    },
    {
      "epoch": 1.0360332614721282,
      "grad_norm": 0.1686023473739624,
      "learning_rate": 0.00013095986038394418,
      "loss": 0.1812,
      "step": 16820
    },
    {
      "epoch": 1.036649214659686,
      "grad_norm": 0.16036739945411682,
      "learning_rate": 0.00013091879683810697,
      "loss": 0.1785,
      "step": 16830
    },
    {
      "epoch": 1.0372651678472435,
      "grad_norm": 0.14495527744293213,
      "learning_rate": 0.0001308777332922698,
      "loss": 0.1778,
      "step": 16840
    },
    {
      "epoch": 1.0378811210348013,
      "grad_norm": 0.13184292614459991,
      "learning_rate": 0.0001308366697464326,
      "loss": 0.1799,
      "step": 16850
    },
    {
      "epoch": 1.0384970742223592,
      "grad_norm": 0.19687435030937195,
      "learning_rate": 0.00013079560620059543,
      "loss": 0.181,
      "step": 16860
    },
    {
      "epoch": 1.039113027409917,
      "grad_norm": 0.17343588173389435,
      "learning_rate": 0.00013075454265475825,
      "loss": 0.1801,
      "step": 16870
    },
    {
      "epoch": 1.0397289805974745,
      "grad_norm": 0.25826495885849,
      "learning_rate": 0.00013071347910892107,
      "loss": 0.1824,
      "step": 16880
    },
    {
      "epoch": 1.0403449337850323,
      "grad_norm": 0.17261554300785065,
      "learning_rate": 0.00013067241556308386,
      "loss": 0.1788,
      "step": 16890
    },
    {
      "epoch": 1.0409608869725901,
      "grad_norm": 0.17188622057437897,
      "learning_rate": 0.0001306313520172467,
      "loss": 0.1763,
      "step": 16900
    },
    {
      "epoch": 1.0415768401601477,
      "grad_norm": 0.17218618094921112,
      "learning_rate": 0.0001305902884714095,
      "loss": 0.1757,
      "step": 16910
    },
    {
      "epoch": 1.0421927933477055,
      "grad_norm": 0.19593119621276855,
      "learning_rate": 0.00013054922492557232,
      "loss": 0.1777,
      "step": 16920
    },
    {
      "epoch": 1.0428087465352633,
      "grad_norm": 0.15523496270179749,
      "learning_rate": 0.00013050816137973514,
      "loss": 0.1805,
      "step": 16930
    },
    {
      "epoch": 1.0434246997228211,
      "grad_norm": 0.23245124518871307,
      "learning_rate": 0.00013046709783389796,
      "loss": 0.1797,
      "step": 16940
    },
    {
      "epoch": 1.0440406529103787,
      "grad_norm": 0.16680145263671875,
      "learning_rate": 0.00013042603428806078,
      "loss": 0.1773,
      "step": 16950
    },
    {
      "epoch": 1.0446566060979365,
      "grad_norm": 0.16918693482875824,
      "learning_rate": 0.0001303849707422236,
      "loss": 0.1787,
      "step": 16960
    },
    {
      "epoch": 1.0452725592854943,
      "grad_norm": 0.1874675154685974,
      "learning_rate": 0.0001303439071963864,
      "loss": 0.1776,
      "step": 16970
    },
    {
      "epoch": 1.0458885124730521,
      "grad_norm": 0.19290313124656677,
      "learning_rate": 0.00013030284365054924,
      "loss": 0.1803,
      "step": 16980
    },
    {
      "epoch": 1.0465044656606097,
      "grad_norm": 0.16257794201374054,
      "learning_rate": 0.00013026178010471203,
      "loss": 0.1777,
      "step": 16990
    },
    {
      "epoch": 1.0471204188481675,
      "grad_norm": 0.18653537333011627,
      "learning_rate": 0.00013022071655887485,
      "loss": 0.1762,
      "step": 17000
    },
    {
      "epoch": 1.0477363720357253,
      "grad_norm": 0.1552833616733551,
      "learning_rate": 0.00013017965301303767,
      "loss": 0.1783,
      "step": 17010
    },
    {
      "epoch": 1.048352325223283,
      "grad_norm": 0.18481960892677307,
      "learning_rate": 0.0001301385894672005,
      "loss": 0.1787,
      "step": 17020
    },
    {
      "epoch": 1.0489682784108407,
      "grad_norm": 0.1529955118894577,
      "learning_rate": 0.00013009752592136331,
      "loss": 0.1782,
      "step": 17030
    },
    {
      "epoch": 1.0495842315983985,
      "grad_norm": 0.17399871349334717,
      "learning_rate": 0.00013005646237552613,
      "loss": 0.178,
      "step": 17040
    },
    {
      "epoch": 1.0502001847859563,
      "grad_norm": 0.20071621239185333,
      "learning_rate": 0.00013001539882968893,
      "loss": 0.1772,
      "step": 17050
    },
    {
      "epoch": 1.050816137973514,
      "grad_norm": 0.1456817388534546,
      "learning_rate": 0.00012997433528385177,
      "loss": 0.179,
      "step": 17060
    },
    {
      "epoch": 1.0514320911610717,
      "grad_norm": 0.17430324852466583,
      "learning_rate": 0.0001299332717380146,
      "loss": 0.1809,
      "step": 17070
    },
    {
      "epoch": 1.0520480443486295,
      "grad_norm": 0.1737498790025711,
      "learning_rate": 0.0001298922081921774,
      "loss": 0.18,
      "step": 17080
    },
    {
      "epoch": 1.0526639975361873,
      "grad_norm": 0.1652078628540039,
      "learning_rate": 0.00012985114464634023,
      "loss": 0.1774,
      "step": 17090
    },
    {
      "epoch": 1.053279950723745,
      "grad_norm": 0.15519395470619202,
      "learning_rate": 0.00012981008110050303,
      "loss": 0.1808,
      "step": 17100
    },
    {
      "epoch": 1.0538959039113027,
      "grad_norm": 0.16024072468280792,
      "learning_rate": 0.00012976901755466588,
      "loss": 0.1785,
      "step": 17110
    },
    {
      "epoch": 1.0545118570988605,
      "grad_norm": 0.17320676147937775,
      "learning_rate": 0.00012972795400882867,
      "loss": 0.1798,
      "step": 17120
    },
    {
      "epoch": 1.0551278102864183,
      "grad_norm": 0.16982202231884003,
      "learning_rate": 0.0001296868904629915,
      "loss": 0.1777,
      "step": 17130
    },
    {
      "epoch": 1.055743763473976,
      "grad_norm": 0.1666068434715271,
      "learning_rate": 0.0001296458269171543,
      "loss": 0.1835,
      "step": 17140
    },
    {
      "epoch": 1.0563597166615337,
      "grad_norm": 0.16348843276500702,
      "learning_rate": 0.00012960476337131713,
      "loss": 0.1797,
      "step": 17150
    },
    {
      "epoch": 1.0569756698490915,
      "grad_norm": 0.15950314700603485,
      "learning_rate": 0.00012956369982547992,
      "loss": 0.1807,
      "step": 17160
    },
    {
      "epoch": 1.057591623036649,
      "grad_norm": 0.17385371029376984,
      "learning_rate": 0.00012952263627964277,
      "loss": 0.1781,
      "step": 17170
    },
    {
      "epoch": 1.058207576224207,
      "grad_norm": 0.16753339767456055,
      "learning_rate": 0.00012948157273380556,
      "loss": 0.179,
      "step": 17180
    },
    {
      "epoch": 1.0588235294117647,
      "grad_norm": 0.1891651451587677,
      "learning_rate": 0.0001294405091879684,
      "loss": 0.1786,
      "step": 17190
    },
    {
      "epoch": 1.0594394825993225,
      "grad_norm": 0.16460628807544708,
      "learning_rate": 0.0001293994456421312,
      "loss": 0.1784,
      "step": 17200
    },
    {
      "epoch": 1.06005543578688,
      "grad_norm": 0.1652272790670395,
      "learning_rate": 0.00012935838209629402,
      "loss": 0.1802,
      "step": 17210
    },
    {
      "epoch": 1.060671388974438,
      "grad_norm": 0.17404723167419434,
      "learning_rate": 0.00012931731855045684,
      "loss": 0.1828,
      "step": 17220
    },
    {
      "epoch": 1.0612873421619957,
      "grad_norm": 0.18522900342941284,
      "learning_rate": 0.00012927625500461966,
      "loss": 0.1785,
      "step": 17230
    },
    {
      "epoch": 1.0619032953495535,
      "grad_norm": 0.19781453907489777,
      "learning_rate": 0.00012923519145878245,
      "loss": 0.1791,
      "step": 17240
    },
    {
      "epoch": 1.062519248537111,
      "grad_norm": 0.16184844076633453,
      "learning_rate": 0.0001291941279129453,
      "loss": 0.1794,
      "step": 17250
    },
    {
      "epoch": 1.063135201724669,
      "grad_norm": 0.20142315328121185,
      "learning_rate": 0.0001291530643671081,
      "loss": 0.1814,
      "step": 17260
    },
    {
      "epoch": 1.0637511549122267,
      "grad_norm": 0.17349526286125183,
      "learning_rate": 0.00012911200082127094,
      "loss": 0.1762,
      "step": 17270
    },
    {
      "epoch": 1.0643671080997845,
      "grad_norm": 0.25669020414352417,
      "learning_rate": 0.00012907093727543373,
      "loss": 0.1809,
      "step": 17280
    },
    {
      "epoch": 1.064983061287342,
      "grad_norm": 0.2196313589811325,
      "learning_rate": 0.00012902987372959655,
      "loss": 0.1791,
      "step": 17290
    },
    {
      "epoch": 1.0655990144749,
      "grad_norm": 0.20757940411567688,
      "learning_rate": 0.00012898881018375937,
      "loss": 0.1781,
      "step": 17300
    },
    {
      "epoch": 1.0662149676624577,
      "grad_norm": 0.18663059175014496,
      "learning_rate": 0.0001289477466379222,
      "loss": 0.1834,
      "step": 17310
    },
    {
      "epoch": 1.0668309208500153,
      "grad_norm": 0.21098868548870087,
      "learning_rate": 0.00012890668309208501,
      "loss": 0.1801,
      "step": 17320
    },
    {
      "epoch": 1.067446874037573,
      "grad_norm": 0.14099052548408508,
      "learning_rate": 0.00012886561954624783,
      "loss": 0.1782,
      "step": 17330
    },
    {
      "epoch": 1.068062827225131,
      "grad_norm": 0.1760936975479126,
      "learning_rate": 0.00012882455600041063,
      "loss": 0.1771,
      "step": 17340
    },
    {
      "epoch": 1.0686787804126887,
      "grad_norm": 0.17741109430789948,
      "learning_rate": 0.00012878349245457347,
      "loss": 0.1807,
      "step": 17350
    },
    {
      "epoch": 1.0692947336002463,
      "grad_norm": 0.31622403860092163,
      "learning_rate": 0.00012874242890873627,
      "loss": 0.1812,
      "step": 17360
    },
    {
      "epoch": 1.069910686787804,
      "grad_norm": 0.14300216734409332,
      "learning_rate": 0.0001287013653628991,
      "loss": 0.1822,
      "step": 17370
    },
    {
      "epoch": 1.070526639975362,
      "grad_norm": 0.15377482771873474,
      "learning_rate": 0.0001286603018170619,
      "loss": 0.1791,
      "step": 17380
    },
    {
      "epoch": 1.0711425931629197,
      "grad_norm": 0.1659596860408783,
      "learning_rate": 0.00012861923827122473,
      "loss": 0.1818,
      "step": 17390
    },
    {
      "epoch": 1.0717585463504773,
      "grad_norm": 0.1639799326658249,
      "learning_rate": 0.00012857817472538755,
      "loss": 0.1797,
      "step": 17400
    },
    {
      "epoch": 1.072374499538035,
      "grad_norm": 0.18617385625839233,
      "learning_rate": 0.00012853711117955037,
      "loss": 0.1797,
      "step": 17410
    },
    {
      "epoch": 1.072990452725593,
      "grad_norm": 0.2035294771194458,
      "learning_rate": 0.00012849604763371316,
      "loss": 0.1772,
      "step": 17420
    },
    {
      "epoch": 1.0736064059131505,
      "grad_norm": 0.17034895718097687,
      "learning_rate": 0.000128454984087876,
      "loss": 0.1791,
      "step": 17430
    },
    {
      "epoch": 1.0742223591007083,
      "grad_norm": 0.3093567490577698,
      "learning_rate": 0.0001284139205420388,
      "loss": 0.1802,
      "step": 17440
    },
    {
      "epoch": 1.074838312288266,
      "grad_norm": 0.17604774236679077,
      "learning_rate": 0.00012837285699620162,
      "loss": 0.1794,
      "step": 17450
    },
    {
      "epoch": 1.075454265475824,
      "grad_norm": 0.14998313784599304,
      "learning_rate": 0.00012833179345036444,
      "loss": 0.1827,
      "step": 17460
    },
    {
      "epoch": 1.0760702186633815,
      "grad_norm": 0.19897620379924774,
      "learning_rate": 0.00012829072990452726,
      "loss": 0.1795,
      "step": 17470
    },
    {
      "epoch": 1.0766861718509393,
      "grad_norm": 0.15463504195213318,
      "learning_rate": 0.00012824966635869008,
      "loss": 0.179,
      "step": 17480
    },
    {
      "epoch": 1.077302125038497,
      "grad_norm": 0.19532646238803864,
      "learning_rate": 0.0001282086028128529,
      "loss": 0.1809,
      "step": 17490
    },
    {
      "epoch": 1.077918078226055,
      "grad_norm": 0.1491674929857254,
      "learning_rate": 0.00012816753926701572,
      "loss": 0.1788,
      "step": 17500
    },
    {
      "epoch": 1.0785340314136125,
      "grad_norm": 0.17104703187942505,
      "learning_rate": 0.00012812647572117854,
      "loss": 0.1794,
      "step": 17510
    },
    {
      "epoch": 1.0791499846011703,
      "grad_norm": 0.1702258288860321,
      "learning_rate": 0.00012808541217534136,
      "loss": 0.1796,
      "step": 17520
    },
    {
      "epoch": 1.079765937788728,
      "grad_norm": 0.14837197959423065,
      "learning_rate": 0.00012804434862950415,
      "loss": 0.1818,
      "step": 17530
    },
    {
      "epoch": 1.080381890976286,
      "grad_norm": 0.15407568216323853,
      "learning_rate": 0.000128003285083667,
      "loss": 0.1797,
      "step": 17540
    },
    {
      "epoch": 1.0809978441638435,
      "grad_norm": 0.143072247505188,
      "learning_rate": 0.0001279622215378298,
      "loss": 0.179,
      "step": 17550
    },
    {
      "epoch": 1.0816137973514013,
      "grad_norm": 0.18814267218112946,
      "learning_rate": 0.00012792115799199261,
      "loss": 0.1788,
      "step": 17560
    },
    {
      "epoch": 1.082229750538959,
      "grad_norm": 0.1688871830701828,
      "learning_rate": 0.00012788009444615543,
      "loss": 0.1779,
      "step": 17570
    },
    {
      "epoch": 1.082845703726517,
      "grad_norm": 0.18157118558883667,
      "learning_rate": 0.00012783903090031825,
      "loss": 0.1778,
      "step": 17580
    },
    {
      "epoch": 1.0834616569140745,
      "grad_norm": 0.18332991003990173,
      "learning_rate": 0.00012779796735448107,
      "loss": 0.1815,
      "step": 17590
    },
    {
      "epoch": 1.0840776101016323,
      "grad_norm": 0.16113467514514923,
      "learning_rate": 0.0001277569038086439,
      "loss": 0.1761,
      "step": 17600
    },
    {
      "epoch": 1.08469356328919,
      "grad_norm": 0.15523144602775574,
      "learning_rate": 0.0001277158402628067,
      "loss": 0.1768,
      "step": 17610
    },
    {
      "epoch": 1.0853095164767477,
      "grad_norm": 0.17272768914699554,
      "learning_rate": 0.00012767477671696953,
      "loss": 0.1807,
      "step": 17620
    },
    {
      "epoch": 1.0859254696643055,
      "grad_norm": 0.25556498765945435,
      "learning_rate": 0.00012763371317113233,
      "loss": 0.1821,
      "step": 17630
    },
    {
      "epoch": 1.0865414228518633,
      "grad_norm": 0.21774442493915558,
      "learning_rate": 0.00012759264962529515,
      "loss": 0.1799,
      "step": 17640
    },
    {
      "epoch": 1.087157376039421,
      "grad_norm": 0.1827256679534912,
      "learning_rate": 0.00012755158607945797,
      "loss": 0.1802,
      "step": 17650
    },
    {
      "epoch": 1.0877733292269787,
      "grad_norm": 0.19874200224876404,
      "learning_rate": 0.0001275105225336208,
      "loss": 0.1785,
      "step": 17660
    },
    {
      "epoch": 1.0883892824145365,
      "grad_norm": 0.17622417211532593,
      "learning_rate": 0.0001274694589877836,
      "loss": 0.1807,
      "step": 17670
    },
    {
      "epoch": 1.0890052356020943,
      "grad_norm": 0.19663989543914795,
      "learning_rate": 0.00012742839544194643,
      "loss": 0.1804,
      "step": 17680
    },
    {
      "epoch": 1.089621188789652,
      "grad_norm": 0.16077633202075958,
      "learning_rate": 0.00012738733189610922,
      "loss": 0.1805,
      "step": 17690
    },
    {
      "epoch": 1.0902371419772097,
      "grad_norm": 0.1799134761095047,
      "learning_rate": 0.00012734626835027207,
      "loss": 0.1792,
      "step": 17700
    },
    {
      "epoch": 1.0908530951647675,
      "grad_norm": 0.1704254299402237,
      "learning_rate": 0.00012730520480443486,
      "loss": 0.1796,
      "step": 17710
    },
    {
      "epoch": 1.0914690483523253,
      "grad_norm": 0.17549504339694977,
      "learning_rate": 0.00012726414125859768,
      "loss": 0.1777,
      "step": 17720
    },
    {
      "epoch": 1.0920850015398829,
      "grad_norm": 0.14677344262599945,
      "learning_rate": 0.0001272230777127605,
      "loss": 0.1801,
      "step": 17730
    },
    {
      "epoch": 1.0927009547274407,
      "grad_norm": 0.17783141136169434,
      "learning_rate": 0.00012718201416692332,
      "loss": 0.1778,
      "step": 17740
    },
    {
      "epoch": 1.0933169079149985,
      "grad_norm": 0.15036547183990479,
      "learning_rate": 0.00012714095062108614,
      "loss": 0.1774,
      "step": 17750
    },
    {
      "epoch": 1.0939328611025563,
      "grad_norm": 0.17339260876178741,
      "learning_rate": 0.00012709988707524896,
      "loss": 0.1824,
      "step": 17760
    },
    {
      "epoch": 1.0945488142901139,
      "grad_norm": 0.17417971789836884,
      "learning_rate": 0.00012705882352941175,
      "loss": 0.1798,
      "step": 17770
    },
    {
      "epoch": 1.0951647674776717,
      "grad_norm": 0.18031398952007294,
      "learning_rate": 0.0001270177599835746,
      "loss": 0.1799,
      "step": 17780
    },
    {
      "epoch": 1.0957807206652295,
      "grad_norm": 0.15498332679271698,
      "learning_rate": 0.0001269766964377374,
      "loss": 0.1779,
      "step": 17790
    },
    {
      "epoch": 1.0963966738527873,
      "grad_norm": 0.16076329350471497,
      "learning_rate": 0.00012693563289190021,
      "loss": 0.1762,
      "step": 17800
    },
    {
      "epoch": 1.0970126270403449,
      "grad_norm": 0.16770906746387482,
      "learning_rate": 0.00012689456934606303,
      "loss": 0.1811,
      "step": 17810
    },
    {
      "epoch": 1.0976285802279027,
      "grad_norm": 0.14955642819404602,
      "learning_rate": 0.00012685350580022585,
      "loss": 0.1772,
      "step": 17820
    },
    {
      "epoch": 1.0982445334154605,
      "grad_norm": 0.1544719785451889,
      "learning_rate": 0.00012681244225438867,
      "loss": 0.1793,
      "step": 17830
    },
    {
      "epoch": 1.0988604866030183,
      "grad_norm": 0.15716552734375,
      "learning_rate": 0.0001267713787085515,
      "loss": 0.1765,
      "step": 17840
    },
    {
      "epoch": 1.0994764397905759,
      "grad_norm": 1.0169655084609985,
      "learning_rate": 0.0001267303151627143,
      "loss": 0.1828,
      "step": 17850
    },
    {
      "epoch": 1.1000923929781337,
      "grad_norm": 0.19419915974140167,
      "learning_rate": 0.00012668925161687713,
      "loss": 0.1801,
      "step": 17860
    },
    {
      "epoch": 1.1007083461656915,
      "grad_norm": 0.1549953669309616,
      "learning_rate": 0.00012664818807103993,
      "loss": 0.1797,
      "step": 17870
    },
    {
      "epoch": 1.1013242993532493,
      "grad_norm": 0.17632733285427094,
      "learning_rate": 0.00012660712452520275,
      "loss": 0.1785,
      "step": 17880
    },
    {
      "epoch": 1.1019402525408069,
      "grad_norm": 0.1690431833267212,
      "learning_rate": 0.00012656606097936557,
      "loss": 0.1801,
      "step": 17890
    },
    {
      "epoch": 1.1025562057283647,
      "grad_norm": 0.1480010598897934,
      "learning_rate": 0.0001265249974335284,
      "loss": 0.1806,
      "step": 17900
    },
    {
      "epoch": 1.1031721589159225,
      "grad_norm": 0.15878985822200775,
      "learning_rate": 0.0001264839338876912,
      "loss": 0.1786,
      "step": 17910
    },
    {
      "epoch": 1.10378811210348,
      "grad_norm": 0.20020692050457,
      "learning_rate": 0.00012644287034185403,
      "loss": 0.1794,
      "step": 17920
    },
    {
      "epoch": 1.1044040652910379,
      "grad_norm": 0.20414163172245026,
      "learning_rate": 0.00012640180679601685,
      "loss": 0.1801,
      "step": 17930
    },
    {
      "epoch": 1.1050200184785957,
      "grad_norm": 0.17978018522262573,
      "learning_rate": 0.00012636074325017967,
      "loss": 0.181,
      "step": 17940
    },
    {
      "epoch": 1.1056359716661535,
      "grad_norm": 0.15617936849594116,
      "learning_rate": 0.0001263196797043425,
      "loss": 0.1816,
      "step": 17950
    },
    {
      "epoch": 1.106251924853711,
      "grad_norm": 0.14854440093040466,
      "learning_rate": 0.00012627861615850528,
      "loss": 0.1821,
      "step": 17960
    },
    {
      "epoch": 1.1068678780412688,
      "grad_norm": 0.1860274225473404,
      "learning_rate": 0.00012623755261266813,
      "loss": 0.1796,
      "step": 17970
    },
    {
      "epoch": 1.1074838312288267,
      "grad_norm": 0.2378452569246292,
      "learning_rate": 0.00012619648906683092,
      "loss": 0.1761,
      "step": 17980
    },
    {
      "epoch": 1.1080997844163845,
      "grad_norm": 0.1597318947315216,
      "learning_rate": 0.00012615542552099374,
      "loss": 0.1783,
      "step": 17990
    },
    {
      "epoch": 1.108715737603942,
      "grad_norm": 0.1557302474975586,
      "learning_rate": 0.00012611436197515656,
      "loss": 0.1798,
      "step": 18000
    },
    {
      "epoch": 1.1093316907914998,
      "grad_norm": 0.15985670685768127,
      "learning_rate": 0.00012607329842931938,
      "loss": 0.1762,
      "step": 18010
    },
    {
      "epoch": 1.1099476439790577,
      "grad_norm": 0.21343015134334564,
      "learning_rate": 0.0001260322348834822,
      "loss": 0.1803,
      "step": 18020
    },
    {
      "epoch": 1.1105635971666152,
      "grad_norm": 0.16339166462421417,
      "learning_rate": 0.00012599117133764502,
      "loss": 0.1772,
      "step": 18030
    },
    {
      "epoch": 1.111179550354173,
      "grad_norm": 0.33642464876174927,
      "learning_rate": 0.0001259501077918078,
      "loss": 0.1796,
      "step": 18040
    },
    {
      "epoch": 1.1117955035417308,
      "grad_norm": 0.1598159223794937,
      "learning_rate": 0.00012590904424597066,
      "loss": 0.1792,
      "step": 18050
    },
    {
      "epoch": 1.1124114567292887,
      "grad_norm": 0.1847032904624939,
      "learning_rate": 0.00012586798070013345,
      "loss": 0.1798,
      "step": 18060
    },
    {
      "epoch": 1.1130274099168462,
      "grad_norm": 0.18772748112678528,
      "learning_rate": 0.00012582691715429627,
      "loss": 0.1808,
      "step": 18070
    },
    {
      "epoch": 1.113643363104404,
      "grad_norm": 0.15332843363285065,
      "learning_rate": 0.0001257858536084591,
      "loss": 0.1815,
      "step": 18080
    },
    {
      "epoch": 1.1142593162919618,
      "grad_norm": 0.17303626239299774,
      "learning_rate": 0.0001257447900626219,
      "loss": 0.1786,
      "step": 18090
    },
    {
      "epoch": 1.1148752694795196,
      "grad_norm": 0.14901602268218994,
      "learning_rate": 0.00012570372651678473,
      "loss": 0.1773,
      "step": 18100
    },
    {
      "epoch": 1.1154912226670772,
      "grad_norm": 0.15863552689552307,
      "learning_rate": 0.00012566266297094755,
      "loss": 0.1801,
      "step": 18110
    },
    {
      "epoch": 1.116107175854635,
      "grad_norm": 0.16087862849235535,
      "learning_rate": 0.00012562159942511035,
      "loss": 0.1795,
      "step": 18120
    },
    {
      "epoch": 1.1167231290421928,
      "grad_norm": 0.15922118723392487,
      "learning_rate": 0.0001255805358792732,
      "loss": 0.1761,
      "step": 18130
    },
    {
      "epoch": 1.1173390822297504,
      "grad_norm": 0.20126205682754517,
      "learning_rate": 0.000125539472333436,
      "loss": 0.1801,
      "step": 18140
    },
    {
      "epoch": 1.1179550354173082,
      "grad_norm": 0.19671165943145752,
      "learning_rate": 0.00012549840878759883,
      "loss": 0.1787,
      "step": 18150
    },
    {
      "epoch": 1.118570988604866,
      "grad_norm": 0.16980260610580444,
      "learning_rate": 0.00012545734524176163,
      "loss": 0.1787,
      "step": 18160
    },
    {
      "epoch": 1.1191869417924238,
      "grad_norm": 0.15102538466453552,
      "learning_rate": 0.00012541628169592445,
      "loss": 0.1794,
      "step": 18170
    },
    {
      "epoch": 1.1198028949799814,
      "grad_norm": 0.18239642679691315,
      "learning_rate": 0.00012537521815008727,
      "loss": 0.1773,
      "step": 18180
    },
    {
      "epoch": 1.1204188481675392,
      "grad_norm": 0.179329514503479,
      "learning_rate": 0.0001253341546042501,
      "loss": 0.1808,
      "step": 18190
    },
    {
      "epoch": 1.121034801355097,
      "grad_norm": 0.2095956653356552,
      "learning_rate": 0.00012529309105841288,
      "loss": 0.1803,
      "step": 18200
    },
    {
      "epoch": 1.1216507545426548,
      "grad_norm": 0.14572949707508087,
      "learning_rate": 0.00012525202751257573,
      "loss": 0.1784,
      "step": 18210
    },
    {
      "epoch": 1.1222667077302124,
      "grad_norm": 0.17738284170627594,
      "learning_rate": 0.00012521096396673852,
      "loss": 0.1801,
      "step": 18220
    },
    {
      "epoch": 1.1228826609177702,
      "grad_norm": 0.19474951922893524,
      "learning_rate": 0.00012516990042090137,
      "loss": 0.1805,
      "step": 18230
    },
    {
      "epoch": 1.123498614105328,
      "grad_norm": 0.16242317855358124,
      "learning_rate": 0.00012512883687506416,
      "loss": 0.1794,
      "step": 18240
    },
    {
      "epoch": 1.1241145672928858,
      "grad_norm": 0.19978021085262299,
      "learning_rate": 0.00012508777332922698,
      "loss": 0.1799,
      "step": 18250
    },
    {
      "epoch": 1.1247305204804434,
      "grad_norm": 0.15640924870967865,
      "learning_rate": 0.0001250467097833898,
      "loss": 0.1825,
      "step": 18260
    },
    {
      "epoch": 1.1253464736680012,
      "grad_norm": 0.20365846157073975,
      "learning_rate": 0.00012500564623755262,
      "loss": 0.1795,
      "step": 18270
    },
    {
      "epoch": 1.125962426855559,
      "grad_norm": 0.16650697588920593,
      "learning_rate": 0.0001249645826917154,
      "loss": 0.1793,
      "step": 18280
    },
    {
      "epoch": 1.1265783800431168,
      "grad_norm": 0.3219783306121826,
      "learning_rate": 0.00012492351914587826,
      "loss": 0.1802,
      "step": 18290
    },
    {
      "epoch": 1.1271943332306744,
      "grad_norm": 0.1759611815214157,
      "learning_rate": 0.00012488245560004105,
      "loss": 0.1798,
      "step": 18300
    },
    {
      "epoch": 1.1278102864182322,
      "grad_norm": 0.23133094608783722,
      "learning_rate": 0.0001248413920542039,
      "loss": 0.1765,
      "step": 18310
    },
    {
      "epoch": 1.12842623960579,
      "grad_norm": 0.18662673234939575,
      "learning_rate": 0.0001248003285083667,
      "loss": 0.1794,
      "step": 18320
    },
    {
      "epoch": 1.1290421927933476,
      "grad_norm": 0.1868983805179596,
      "learning_rate": 0.0001247592649625295,
      "loss": 0.1757,
      "step": 18330
    },
    {
      "epoch": 1.1296581459809054,
      "grad_norm": 0.19118499755859375,
      "learning_rate": 0.00012471820141669233,
      "loss": 0.179,
      "step": 18340
    },
    {
      "epoch": 1.1302740991684632,
      "grad_norm": 0.1655474305152893,
      "learning_rate": 0.00012467713787085515,
      "loss": 0.1769,
      "step": 18350
    },
    {
      "epoch": 1.130890052356021,
      "grad_norm": 0.16328881680965424,
      "learning_rate": 0.00012463607432501797,
      "loss": 0.1799,
      "step": 18360
    },
    {
      "epoch": 1.1315060055435786,
      "grad_norm": 0.18904632329940796,
      "learning_rate": 0.0001245950107791808,
      "loss": 0.1779,
      "step": 18370
    },
    {
      "epoch": 1.1321219587311364,
      "grad_norm": 0.1654197722673416,
      "learning_rate": 0.0001245539472333436,
      "loss": 0.1778,
      "step": 18380
    },
    {
      "epoch": 1.1327379119186942,
      "grad_norm": 0.259748637676239,
      "learning_rate": 0.00012451288368750643,
      "loss": 0.1794,
      "step": 18390
    },
    {
      "epoch": 1.133353865106252,
      "grad_norm": 0.1721392124891281,
      "learning_rate": 0.00012447182014166925,
      "loss": 0.1792,
      "step": 18400
    },
    {
      "epoch": 1.1339698182938096,
      "grad_norm": 0.15606743097305298,
      "learning_rate": 0.00012443075659583205,
      "loss": 0.1778,
      "step": 18410
    },
    {
      "epoch": 1.1345857714813674,
      "grad_norm": 0.17273163795471191,
      "learning_rate": 0.0001243896930499949,
      "loss": 0.1785,
      "step": 18420
    },
    {
      "epoch": 1.1352017246689252,
      "grad_norm": 0.18388237059116364,
      "learning_rate": 0.00012434862950415769,
      "loss": 0.1784,
      "step": 18430
    },
    {
      "epoch": 1.1358176778564828,
      "grad_norm": 0.2607879340648651,
      "learning_rate": 0.0001243075659583205,
      "loss": 0.1791,
      "step": 18440
    },
    {
      "epoch": 1.1364336310440406,
      "grad_norm": 0.2519057095050812,
      "learning_rate": 0.00012426650241248333,
      "loss": 0.18,
      "step": 18450
    },
    {
      "epoch": 1.1370495842315984,
      "grad_norm": 0.16645121574401855,
      "learning_rate": 0.00012422543886664615,
      "loss": 0.1823,
      "step": 18460
    },
    {
      "epoch": 1.1376655374191562,
      "grad_norm": 0.2815655767917633,
      "learning_rate": 0.00012418437532080897,
      "loss": 0.1799,
      "step": 18470
    },
    {
      "epoch": 1.1382814906067138,
      "grad_norm": 0.15536276996135712,
      "learning_rate": 0.0001241433117749718,
      "loss": 0.1808,
      "step": 18480
    },
    {
      "epoch": 1.1388974437942716,
      "grad_norm": 0.1852840632200241,
      "learning_rate": 0.00012410224822913458,
      "loss": 0.1801,
      "step": 18490
    },
    {
      "epoch": 1.1395133969818294,
      "grad_norm": 0.19961391389369965,
      "learning_rate": 0.00012406118468329743,
      "loss": 0.1787,
      "step": 18500
    },
    {
      "epoch": 1.1401293501693872,
      "grad_norm": 0.18696480989456177,
      "learning_rate": 0.00012402012113746022,
      "loss": 0.1782,
      "step": 18510
    },
    {
      "epoch": 1.1407453033569448,
      "grad_norm": 0.622850775718689,
      "learning_rate": 0.00012397905759162304,
      "loss": 0.1825,
      "step": 18520
    },
    {
      "epoch": 1.1413612565445026,
      "grad_norm": 0.179438978433609,
      "learning_rate": 0.00012393799404578586,
      "loss": 0.1795,
      "step": 18530
    },
    {
      "epoch": 1.1419772097320604,
      "grad_norm": 0.14822161197662354,
      "learning_rate": 0.00012389693049994868,
      "loss": 0.1803,
      "step": 18540
    },
    {
      "epoch": 1.142593162919618,
      "grad_norm": 0.1591186225414276,
      "learning_rate": 0.0001238558669541115,
      "loss": 0.18,
      "step": 18550
    },
    {
      "epoch": 1.1432091161071758,
      "grad_norm": 0.1473691314458847,
      "learning_rate": 0.00012381480340827432,
      "loss": 0.179,
      "step": 18560
    },
    {
      "epoch": 1.1438250692947336,
      "grad_norm": 0.23917293548583984,
      "learning_rate": 0.0001237737398624371,
      "loss": 0.1781,
      "step": 18570
    },
    {
      "epoch": 1.1444410224822914,
      "grad_norm": 0.17414259910583496,
      "learning_rate": 0.00012373267631659996,
      "loss": 0.1796,
      "step": 18580
    },
    {
      "epoch": 1.1450569756698492,
      "grad_norm": 0.18130967020988464,
      "learning_rate": 0.00012369161277076275,
      "loss": 0.1783,
      "step": 18590
    },
    {
      "epoch": 1.1456729288574068,
      "grad_norm": 0.1814172863960266,
      "learning_rate": 0.00012365054922492557,
      "loss": 0.1788,
      "step": 18600
    },
    {
      "epoch": 1.1462888820449646,
      "grad_norm": 0.1693708598613739,
      "learning_rate": 0.0001236094856790884,
      "loss": 0.18,
      "step": 18610
    },
    {
      "epoch": 1.1469048352325224,
      "grad_norm": 0.17094366252422333,
      "learning_rate": 0.0001235684221332512,
      "loss": 0.178,
      "step": 18620
    },
    {
      "epoch": 1.14752078842008,
      "grad_norm": 0.15921227633953094,
      "learning_rate": 0.00012352735858741403,
      "loss": 0.1822,
      "step": 18630
    },
    {
      "epoch": 1.1481367416076378,
      "grad_norm": 0.17466005682945251,
      "learning_rate": 0.00012348629504157685,
      "loss": 0.1785,
      "step": 18640
    },
    {
      "epoch": 1.1487526947951956,
      "grad_norm": 0.17360900342464447,
      "learning_rate": 0.00012344523149573965,
      "loss": 0.1776,
      "step": 18650
    },
    {
      "epoch": 1.1493686479827534,
      "grad_norm": 0.21474172174930573,
      "learning_rate": 0.0001234041679499025,
      "loss": 0.1807,
      "step": 18660
    },
    {
      "epoch": 1.149984601170311,
      "grad_norm": 0.1486821323633194,
      "learning_rate": 0.00012336310440406529,
      "loss": 0.1793,
      "step": 18670
    },
    {
      "epoch": 1.1506005543578688,
      "grad_norm": 0.18053533136844635,
      "learning_rate": 0.0001233220408582281,
      "loss": 0.183,
      "step": 18680
    },
    {
      "epoch": 1.1512165075454266,
      "grad_norm": 0.1837375909090042,
      "learning_rate": 0.00012328097731239093,
      "loss": 0.1811,
      "step": 18690
    },
    {
      "epoch": 1.1518324607329844,
      "grad_norm": 0.15333764255046844,
      "learning_rate": 0.00012323991376655375,
      "loss": 0.1775,
      "step": 18700
    },
    {
      "epoch": 1.152448413920542,
      "grad_norm": 0.15547560155391693,
      "learning_rate": 0.00012319885022071657,
      "loss": 0.1797,
      "step": 18710
    },
    {
      "epoch": 1.1530643671080998,
      "grad_norm": 0.22144252061843872,
      "learning_rate": 0.00012315778667487939,
      "loss": 0.1797,
      "step": 18720
    },
    {
      "epoch": 1.1536803202956576,
      "grad_norm": 0.28791162371635437,
      "learning_rate": 0.00012311672312904218,
      "loss": 0.1793,
      "step": 18730
    },
    {
      "epoch": 1.1542962734832152,
      "grad_norm": 0.1793346256017685,
      "learning_rate": 0.00012307565958320503,
      "loss": 0.1785,
      "step": 18740
    },
    {
      "epoch": 1.154912226670773,
      "grad_norm": 0.16755472123622894,
      "learning_rate": 0.00012303459603736782,
      "loss": 0.1778,
      "step": 18750
    },
    {
      "epoch": 1.1555281798583308,
      "grad_norm": 0.13862834870815277,
      "learning_rate": 0.00012299353249153064,
      "loss": 0.179,
      "step": 18760
    },
    {
      "epoch": 1.1561441330458886,
      "grad_norm": 0.1550377607345581,
      "learning_rate": 0.00012295246894569349,
      "loss": 0.1802,
      "step": 18770
    },
    {
      "epoch": 1.1567600862334462,
      "grad_norm": 0.1659730225801468,
      "learning_rate": 0.00012291140539985628,
      "loss": 0.1777,
      "step": 18780
    },
    {
      "epoch": 1.157376039421004,
      "grad_norm": 0.1782277375459671,
      "learning_rate": 0.0001228703418540191,
      "loss": 0.1817,
      "step": 18790
    },
    {
      "epoch": 1.1579919926085618,
      "grad_norm": 0.1770172119140625,
      "learning_rate": 0.00012282927830818192,
      "loss": 0.1795,
      "step": 18800
    },
    {
      "epoch": 1.1586079457961196,
      "grad_norm": 0.16943491995334625,
      "learning_rate": 0.00012278821476234474,
      "loss": 0.1798,
      "step": 18810
    },
    {
      "epoch": 1.1592238989836772,
      "grad_norm": 0.15152861177921295,
      "learning_rate": 0.00012274715121650756,
      "loss": 0.1832,
      "step": 18820
    },
    {
      "epoch": 1.159839852171235,
      "grad_norm": 0.16995415091514587,
      "learning_rate": 0.00012270608767067038,
      "loss": 0.1792,
      "step": 18830
    },
    {
      "epoch": 1.1604558053587928,
      "grad_norm": 0.17799673974514008,
      "learning_rate": 0.00012266502412483317,
      "loss": 0.1789,
      "step": 18840
    },
    {
      "epoch": 1.1610717585463504,
      "grad_norm": 0.15108133852481842,
      "learning_rate": 0.00012262396057899602,
      "loss": 0.1787,
      "step": 18850
    },
    {
      "epoch": 1.1616877117339082,
      "grad_norm": 0.15331460535526276,
      "learning_rate": 0.0001225828970331588,
      "loss": 0.1768,
      "step": 18860
    },
    {
      "epoch": 1.162303664921466,
      "grad_norm": 0.19228386878967285,
      "learning_rate": 0.00012254183348732163,
      "loss": 0.1791,
      "step": 18870
    },
    {
      "epoch": 1.1629196181090238,
      "grad_norm": 0.14600569009780884,
      "learning_rate": 0.00012250076994148445,
      "loss": 0.178,
      "step": 18880
    },
    {
      "epoch": 1.1635355712965816,
      "grad_norm": 0.1784989982843399,
      "learning_rate": 0.00012245970639564727,
      "loss": 0.1788,
      "step": 18890
    },
    {
      "epoch": 1.1641515244841392,
      "grad_norm": 0.16395463049411774,
      "learning_rate": 0.0001224186428498101,
      "loss": 0.1785,
      "step": 18900
    },
    {
      "epoch": 1.164767477671697,
      "grad_norm": 0.14782527089118958,
      "learning_rate": 0.0001223775793039729,
      "loss": 0.1795,
      "step": 18910
    },
    {
      "epoch": 1.1653834308592548,
      "grad_norm": 0.16344565153121948,
      "learning_rate": 0.0001223365157581357,
      "loss": 0.1779,
      "step": 18920
    },
    {
      "epoch": 1.1659993840468124,
      "grad_norm": 0.19150227308273315,
      "learning_rate": 0.00012229545221229855,
      "loss": 0.1781,
      "step": 18930
    },
    {
      "epoch": 1.1666153372343702,
      "grad_norm": 0.1573149561882019,
      "learning_rate": 0.00012225438866646135,
      "loss": 0.1798,
      "step": 18940
    },
    {
      "epoch": 1.167231290421928,
      "grad_norm": 0.15066838264465332,
      "learning_rate": 0.00012221332512062417,
      "loss": 0.1826,
      "step": 18950
    },
    {
      "epoch": 1.1678472436094858,
      "grad_norm": 0.5778903961181641,
      "learning_rate": 0.00012217226157478699,
      "loss": 0.1802,
      "step": 18960
    },
    {
      "epoch": 1.1684631967970434,
      "grad_norm": 0.18213824927806854,
      "learning_rate": 0.0001221311980289498,
      "loss": 0.1801,
      "step": 18970
    },
    {
      "epoch": 1.1690791499846012,
      "grad_norm": 0.1795174777507782,
      "learning_rate": 0.00012209013448311263,
      "loss": 0.1792,
      "step": 18980
    },
    {
      "epoch": 1.169695103172159,
      "grad_norm": 0.16057226061820984,
      "learning_rate": 0.00012204907093727545,
      "loss": 0.177,
      "step": 18990
    },
    {
      "epoch": 1.1703110563597168,
      "grad_norm": 0.1828220635652542,
      "learning_rate": 0.00012201211374602197,
      "loss": 0.1796,
      "step": 19000
    },
    {
      "epoch": 1.1709270095472744,
      "grad_norm": 0.2630455195903778,
      "learning_rate": 0.0001219710502001848,
      "loss": 0.18,
      "step": 19010
    },
    {
      "epoch": 1.1715429627348322,
      "grad_norm": 0.18482500314712524,
      "learning_rate": 0.00012192998665434761,
      "loss": 0.1807,
      "step": 19020
    },
    {
      "epoch": 1.17215891592239,
      "grad_norm": 0.1601792275905609,
      "learning_rate": 0.00012188892310851043,
      "loss": 0.1792,
      "step": 19030
    },
    {
      "epoch": 1.1727748691099475,
      "grad_norm": 0.1719139814376831,
      "learning_rate": 0.00012184785956267324,
      "loss": 0.1795,
      "step": 19040
    },
    {
      "epoch": 1.1733908222975054,
      "grad_norm": 0.1626579910516739,
      "learning_rate": 0.00012180679601683607,
      "loss": 0.1762,
      "step": 19050
    },
    {
      "epoch": 1.1740067754850632,
      "grad_norm": 0.1503126174211502,
      "learning_rate": 0.00012176573247099888,
      "loss": 0.1792,
      "step": 19060
    },
    {
      "epoch": 1.174622728672621,
      "grad_norm": 0.1588844656944275,
      "learning_rate": 0.0001217246689251617,
      "loss": 0.1764,
      "step": 19070
    },
    {
      "epoch": 1.1752386818601785,
      "grad_norm": 0.13944511115550995,
      "learning_rate": 0.0001216836053793245,
      "loss": 0.1791,
      "step": 19080
    },
    {
      "epoch": 1.1758546350477364,
      "grad_norm": 0.14818361401557922,
      "learning_rate": 0.00012164254183348734,
      "loss": 0.1767,
      "step": 19090
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 0.17920570075511932,
      "learning_rate": 0.00012160147828765014,
      "loss": 0.1784,
      "step": 19100
    },
    {
      "epoch": 1.177086541422852,
      "grad_norm": 0.15179219841957092,
      "learning_rate": 0.00012156041474181296,
      "loss": 0.1811,
      "step": 19110
    },
    {
      "epoch": 1.1777024946104095,
      "grad_norm": 0.17206630110740662,
      "learning_rate": 0.00012151935119597577,
      "loss": 0.1792,
      "step": 19120
    },
    {
      "epoch": 1.1783184477979673,
      "grad_norm": 0.15337929129600525,
      "learning_rate": 0.0001214782876501386,
      "loss": 0.179,
      "step": 19130
    },
    {
      "epoch": 1.1789344009855252,
      "grad_norm": 0.15527036786079407,
      "learning_rate": 0.00012143722410430141,
      "loss": 0.1776,
      "step": 19140
    },
    {
      "epoch": 1.1795503541730827,
      "grad_norm": 0.15748906135559082,
      "learning_rate": 0.00012139616055846423,
      "loss": 0.1784,
      "step": 19150
    },
    {
      "epoch": 1.1801663073606405,
      "grad_norm": 0.1707913726568222,
      "learning_rate": 0.00012135509701262704,
      "loss": 0.1783,
      "step": 19160
    },
    {
      "epoch": 1.1807822605481983,
      "grad_norm": 0.17070578038692474,
      "learning_rate": 0.00012131403346678987,
      "loss": 0.1773,
      "step": 19170
    },
    {
      "epoch": 1.1813982137357562,
      "grad_norm": 0.22339248657226562,
      "learning_rate": 0.00012127296992095268,
      "loss": 0.1796,
      "step": 19180
    },
    {
      "epoch": 1.1820141669233137,
      "grad_norm": 0.15965940058231354,
      "learning_rate": 0.0001212319063751155,
      "loss": 0.1762,
      "step": 19190
    },
    {
      "epoch": 1.1826301201108715,
      "grad_norm": 0.16656915843486786,
      "learning_rate": 0.0001211908428292783,
      "loss": 0.1793,
      "step": 19200
    },
    {
      "epoch": 1.1832460732984293,
      "grad_norm": 0.16582295298576355,
      "learning_rate": 0.00012114977928344114,
      "loss": 0.1798,
      "step": 19210
    },
    {
      "epoch": 1.1838620264859872,
      "grad_norm": 0.16435885429382324,
      "learning_rate": 0.00012110871573760394,
      "loss": 0.1786,
      "step": 19220
    },
    {
      "epoch": 1.1844779796735447,
      "grad_norm": 0.19165855646133423,
      "learning_rate": 0.00012106765219176676,
      "loss": 0.1803,
      "step": 19230
    },
    {
      "epoch": 1.1850939328611025,
      "grad_norm": 0.19201511144638062,
      "learning_rate": 0.00012102658864592957,
      "loss": 0.178,
      "step": 19240
    },
    {
      "epoch": 1.1857098860486603,
      "grad_norm": 0.16963033378124237,
      "learning_rate": 0.0001209855251000924,
      "loss": 0.1766,
      "step": 19250
    },
    {
      "epoch": 1.186325839236218,
      "grad_norm": 0.15136636793613434,
      "learning_rate": 0.00012094446155425521,
      "loss": 0.1793,
      "step": 19260
    },
    {
      "epoch": 1.1869417924237757,
      "grad_norm": 0.1616848111152649,
      "learning_rate": 0.00012090339800841803,
      "loss": 0.1771,
      "step": 19270
    },
    {
      "epoch": 1.1875577456113335,
      "grad_norm": 0.1785324066877365,
      "learning_rate": 0.00012086233446258084,
      "loss": 0.1783,
      "step": 19280
    },
    {
      "epoch": 1.1881736987988913,
      "grad_norm": 0.15146414935588837,
      "learning_rate": 0.00012082127091674367,
      "loss": 0.1768,
      "step": 19290
    },
    {
      "epoch": 1.1887896519864491,
      "grad_norm": 0.18643808364868164,
      "learning_rate": 0.00012078020737090648,
      "loss": 0.1803,
      "step": 19300
    },
    {
      "epoch": 1.1894056051740067,
      "grad_norm": 0.16207441687583923,
      "learning_rate": 0.0001207391438250693,
      "loss": 0.1735,
      "step": 19310
    },
    {
      "epoch": 1.1900215583615645,
      "grad_norm": 0.14407356083393097,
      "learning_rate": 0.00012069808027923213,
      "loss": 0.1782,
      "step": 19320
    },
    {
      "epoch": 1.1906375115491223,
      "grad_norm": 0.1560736447572708,
      "learning_rate": 0.00012065701673339494,
      "loss": 0.1814,
      "step": 19330
    },
    {
      "epoch": 1.19125346473668,
      "grad_norm": 0.15965957939624786,
      "learning_rate": 0.00012061595318755776,
      "loss": 0.1784,
      "step": 19340
    },
    {
      "epoch": 1.1918694179242377,
      "grad_norm": 0.16137276589870453,
      "learning_rate": 0.00012057488964172056,
      "loss": 0.1782,
      "step": 19350
    },
    {
      "epoch": 1.1924853711117955,
      "grad_norm": 0.18567489087581635,
      "learning_rate": 0.0001205338260958834,
      "loss": 0.181,
      "step": 19360
    },
    {
      "epoch": 1.1931013242993533,
      "grad_norm": 0.2099091112613678,
      "learning_rate": 0.0001204927625500462,
      "loss": 0.1769,
      "step": 19370
    },
    {
      "epoch": 1.193717277486911,
      "grad_norm": 0.1591951847076416,
      "learning_rate": 0.00012045169900420902,
      "loss": 0.1796,
      "step": 19380
    },
    {
      "epoch": 1.1943332306744687,
      "grad_norm": 0.16111469268798828,
      "learning_rate": 0.00012041063545837183,
      "loss": 0.1782,
      "step": 19390
    },
    {
      "epoch": 1.1949491838620265,
      "grad_norm": 0.18235114216804504,
      "learning_rate": 0.00012036957191253466,
      "loss": 0.1788,
      "step": 19400
    },
    {
      "epoch": 1.1955651370495843,
      "grad_norm": 0.15398013591766357,
      "learning_rate": 0.00012032850836669747,
      "loss": 0.1768,
      "step": 19410
    },
    {
      "epoch": 1.196181090237142,
      "grad_norm": 0.1750762015581131,
      "learning_rate": 0.00012028744482086029,
      "loss": 0.1797,
      "step": 19420
    },
    {
      "epoch": 1.1967970434246997,
      "grad_norm": 0.18523365259170532,
      "learning_rate": 0.0001202463812750231,
      "loss": 0.1804,
      "step": 19430
    },
    {
      "epoch": 1.1974129966122575,
      "grad_norm": 0.14552068710327148,
      "learning_rate": 0.00012020531772918593,
      "loss": 0.178,
      "step": 19440
    },
    {
      "epoch": 1.1980289497998151,
      "grad_norm": 0.17001909017562866,
      "learning_rate": 0.00012016425418334874,
      "loss": 0.1777,
      "step": 19450
    },
    {
      "epoch": 1.198644902987373,
      "grad_norm": 0.16974979639053345,
      "learning_rate": 0.00012012319063751156,
      "loss": 0.1781,
      "step": 19460
    },
    {
      "epoch": 1.1992608561749307,
      "grad_norm": 0.1854993849992752,
      "learning_rate": 0.00012008212709167436,
      "loss": 0.178,
      "step": 19470
    },
    {
      "epoch": 1.1998768093624885,
      "grad_norm": 0.1833028942346573,
      "learning_rate": 0.0001200410635458372,
      "loss": 0.1777,
      "step": 19480
    },
    {
      "epoch": 1.200492762550046,
      "grad_norm": 0.16690169274806976,
      "learning_rate": 0.00012,
      "loss": 0.1805,
      "step": 19490
    },
    {
      "epoch": 1.201108715737604,
      "grad_norm": 0.13590435683727264,
      "learning_rate": 0.00011995893645416282,
      "loss": 0.1766,
      "step": 19500
    },
    {
      "epoch": 1.2017246689251617,
      "grad_norm": 0.17627538740634918,
      "learning_rate": 0.00011991787290832563,
      "loss": 0.179,
      "step": 19510
    },
    {
      "epoch": 1.2023406221127195,
      "grad_norm": 0.1563500016927719,
      "learning_rate": 0.00011987680936248846,
      "loss": 0.18,
      "step": 19520
    },
    {
      "epoch": 1.202956575300277,
      "grad_norm": 0.20141693949699402,
      "learning_rate": 0.00011983574581665127,
      "loss": 0.1785,
      "step": 19530
    },
    {
      "epoch": 1.203572528487835,
      "grad_norm": 0.16522230207920074,
      "learning_rate": 0.00011979468227081409,
      "loss": 0.1789,
      "step": 19540
    },
    {
      "epoch": 1.2041884816753927,
      "grad_norm": 0.1645548790693283,
      "learning_rate": 0.0001197536187249769,
      "loss": 0.1786,
      "step": 19550
    },
    {
      "epoch": 1.2048044348629503,
      "grad_norm": 0.14362424612045288,
      "learning_rate": 0.00011971255517913973,
      "loss": 0.1775,
      "step": 19560
    },
    {
      "epoch": 1.205420388050508,
      "grad_norm": 0.17336563766002655,
      "learning_rate": 0.00011967149163330254,
      "loss": 0.1782,
      "step": 19570
    },
    {
      "epoch": 1.206036341238066,
      "grad_norm": 0.1772487908601761,
      "learning_rate": 0.00011963042808746536,
      "loss": 0.1788,
      "step": 19580
    },
    {
      "epoch": 1.2066522944256237,
      "grad_norm": 0.16829559206962585,
      "learning_rate": 0.00011958936454162816,
      "loss": 0.1815,
      "step": 19590
    },
    {
      "epoch": 1.2072682476131815,
      "grad_norm": 0.18493962287902832,
      "learning_rate": 0.000119548300995791,
      "loss": 0.1792,
      "step": 19600
    },
    {
      "epoch": 1.207884200800739,
      "grad_norm": 0.15045520663261414,
      "learning_rate": 0.0001195072374499538,
      "loss": 0.1792,
      "step": 19610
    },
    {
      "epoch": 1.208500153988297,
      "grad_norm": 0.14734549820423126,
      "learning_rate": 0.00011946617390411662,
      "loss": 0.1766,
      "step": 19620
    },
    {
      "epoch": 1.2091161071758547,
      "grad_norm": 0.16507242619991302,
      "learning_rate": 0.00011942511035827943,
      "loss": 0.1788,
      "step": 19630
    },
    {
      "epoch": 1.2097320603634123,
      "grad_norm": 0.1523829996585846,
      "learning_rate": 0.00011938404681244226,
      "loss": 0.1775,
      "step": 19640
    },
    {
      "epoch": 1.21034801355097,
      "grad_norm": 0.1640116274356842,
      "learning_rate": 0.00011934298326660507,
      "loss": 0.1778,
      "step": 19650
    },
    {
      "epoch": 1.210963966738528,
      "grad_norm": 0.13653884828090668,
      "learning_rate": 0.0001193019197207679,
      "loss": 0.1793,
      "step": 19660
    },
    {
      "epoch": 1.2115799199260857,
      "grad_norm": 0.15272867679595947,
      "learning_rate": 0.0001192608561749307,
      "loss": 0.1802,
      "step": 19670
    },
    {
      "epoch": 1.2121958731136433,
      "grad_norm": 0.14956454932689667,
      "learning_rate": 0.00011921979262909353,
      "loss": 0.179,
      "step": 19680
    },
    {
      "epoch": 1.212811826301201,
      "grad_norm": 0.1825859695672989,
      "learning_rate": 0.00011917872908325634,
      "loss": 0.1802,
      "step": 19690
    },
    {
      "epoch": 1.213427779488759,
      "grad_norm": 0.14700689911842346,
      "learning_rate": 0.00011913766553741917,
      "loss": 0.1794,
      "step": 19700
    },
    {
      "epoch": 1.2140437326763167,
      "grad_norm": 0.1670919805765152,
      "learning_rate": 0.00011909660199158196,
      "loss": 0.1781,
      "step": 19710
    },
    {
      "epoch": 1.2146596858638743,
      "grad_norm": 0.1452835351228714,
      "learning_rate": 0.0001190555384457448,
      "loss": 0.1775,
      "step": 19720
    },
    {
      "epoch": 1.215275639051432,
      "grad_norm": 0.14553141593933105,
      "learning_rate": 0.0001190144748999076,
      "loss": 0.1799,
      "step": 19730
    },
    {
      "epoch": 1.21589159223899,
      "grad_norm": 0.16724281013011932,
      "learning_rate": 0.00011897341135407044,
      "loss": 0.1753,
      "step": 19740
    },
    {
      "epoch": 1.2165075454265475,
      "grad_norm": 0.18724897503852844,
      "learning_rate": 0.00011893234780823326,
      "loss": 0.1787,
      "step": 19750
    },
    {
      "epoch": 1.2171234986141053,
      "grad_norm": 0.1679595559835434,
      "learning_rate": 0.00011889128426239606,
      "loss": 0.1753,
      "step": 19760
    },
    {
      "epoch": 1.217739451801663,
      "grad_norm": 0.15579374134540558,
      "learning_rate": 0.0001188502207165589,
      "loss": 0.1754,
      "step": 19770
    },
    {
      "epoch": 1.218355404989221,
      "grad_norm": 0.1535976231098175,
      "learning_rate": 0.0001188091571707217,
      "loss": 0.179,
      "step": 19780
    },
    {
      "epoch": 1.2189713581767785,
      "grad_norm": 0.1849772036075592,
      "learning_rate": 0.00011876809362488452,
      "loss": 0.178,
      "step": 19790
    },
    {
      "epoch": 1.2195873113643363,
      "grad_norm": 0.14735516905784607,
      "learning_rate": 0.00011872703007904733,
      "loss": 0.1765,
      "step": 19800
    },
    {
      "epoch": 1.220203264551894,
      "grad_norm": 0.17020976543426514,
      "learning_rate": 0.00011868596653321016,
      "loss": 0.1773,
      "step": 19810
    },
    {
      "epoch": 1.220819217739452,
      "grad_norm": 0.17348314821720123,
      "learning_rate": 0.00011864490298737297,
      "loss": 0.1775,
      "step": 19820
    },
    {
      "epoch": 1.2214351709270095,
      "grad_norm": 0.18515588343143463,
      "learning_rate": 0.00011860383944153579,
      "loss": 0.1787,
      "step": 19830
    },
    {
      "epoch": 1.2220511241145673,
      "grad_norm": 0.1645832061767578,
      "learning_rate": 0.0001185627758956986,
      "loss": 0.1759,
      "step": 19840
    },
    {
      "epoch": 1.222667077302125,
      "grad_norm": 0.16657397150993347,
      "learning_rate": 0.00011852171234986143,
      "loss": 0.1785,
      "step": 19850
    },
    {
      "epoch": 1.2232830304896827,
      "grad_norm": 0.19028834998607635,
      "learning_rate": 0.00011848064880402424,
      "loss": 0.1777,
      "step": 19860
    },
    {
      "epoch": 1.2238989836772405,
      "grad_norm": 0.17299385368824005,
      "learning_rate": 0.00011843958525818706,
      "loss": 0.1796,
      "step": 19870
    },
    {
      "epoch": 1.2245149368647983,
      "grad_norm": 0.18648883700370789,
      "learning_rate": 0.00011839852171234986,
      "loss": 0.1777,
      "step": 19880
    },
    {
      "epoch": 1.225130890052356,
      "grad_norm": 0.14676246047019958,
      "learning_rate": 0.0001183574581665127,
      "loss": 0.1797,
      "step": 19890
    },
    {
      "epoch": 1.2257468432399137,
      "grad_norm": 0.18248090147972107,
      "learning_rate": 0.0001183163946206755,
      "loss": 0.1777,
      "step": 19900
    },
    {
      "epoch": 1.2263627964274715,
      "grad_norm": 0.16884714365005493,
      "learning_rate": 0.00011827533107483832,
      "loss": 0.1795,
      "step": 19910
    },
    {
      "epoch": 1.2269787496150293,
      "grad_norm": 0.15638002753257751,
      "learning_rate": 0.00011823426752900113,
      "loss": 0.1778,
      "step": 19920
    },
    {
      "epoch": 1.227594702802587,
      "grad_norm": 0.17233985662460327,
      "learning_rate": 0.00011819320398316396,
      "loss": 0.1777,
      "step": 19930
    },
    {
      "epoch": 1.2282106559901447,
      "grad_norm": 0.21271362900733948,
      "learning_rate": 0.00011815214043732677,
      "loss": 0.1788,
      "step": 19940
    },
    {
      "epoch": 1.2288266091777025,
      "grad_norm": 0.2136908769607544,
      "learning_rate": 0.00011811107689148959,
      "loss": 0.1789,
      "step": 19950
    },
    {
      "epoch": 1.2294425623652603,
      "grad_norm": 0.13201375305652618,
      "learning_rate": 0.0001180700133456524,
      "loss": 0.1787,
      "step": 19960
    },
    {
      "epoch": 1.2300585155528179,
      "grad_norm": 0.15495269000530243,
      "learning_rate": 0.00011802894979981523,
      "loss": 0.1772,
      "step": 19970
    },
    {
      "epoch": 1.2306744687403757,
      "grad_norm": 0.17063996195793152,
      "learning_rate": 0.00011798788625397804,
      "loss": 0.1771,
      "step": 19980
    },
    {
      "epoch": 1.2312904219279335,
      "grad_norm": 0.1440856158733368,
      "learning_rate": 0.00011794682270814086,
      "loss": 0.178,
      "step": 19990
    },
    {
      "epoch": 1.2319063751154913,
      "grad_norm": 0.14627549052238464,
      "learning_rate": 0.00011790575916230366,
      "loss": 0.1775,
      "step": 20000
    },
    {
      "epoch": 1.232522328303049,
      "grad_norm": 0.21299956738948822,
      "learning_rate": 0.0001178646956164665,
      "loss": 0.178,
      "step": 20010
    },
    {
      "epoch": 1.2331382814906067,
      "grad_norm": 0.14319263398647308,
      "learning_rate": 0.0001178236320706293,
      "loss": 0.1782,
      "step": 20020
    },
    {
      "epoch": 1.2337542346781645,
      "grad_norm": 0.14212654531002045,
      "learning_rate": 0.00011778256852479212,
      "loss": 0.1793,
      "step": 20030
    },
    {
      "epoch": 1.2343701878657223,
      "grad_norm": 0.16223624348640442,
      "learning_rate": 0.00011774150497895493,
      "loss": 0.1785,
      "step": 20040
    },
    {
      "epoch": 1.2349861410532799,
      "grad_norm": 0.1547197848558426,
      "learning_rate": 0.00011770044143311776,
      "loss": 0.1774,
      "step": 20050
    },
    {
      "epoch": 1.2356020942408377,
      "grad_norm": 0.1797521561384201,
      "learning_rate": 0.00011765937788728057,
      "loss": 0.1786,
      "step": 20060
    },
    {
      "epoch": 1.2362180474283955,
      "grad_norm": 0.16651055216789246,
      "learning_rate": 0.00011761831434144339,
      "loss": 0.1782,
      "step": 20070
    },
    {
      "epoch": 1.2368340006159533,
      "grad_norm": 0.16968925297260284,
      "learning_rate": 0.0001175772507956062,
      "loss": 0.177,
      "step": 20080
    },
    {
      "epoch": 1.2374499538035109,
      "grad_norm": 0.21898576617240906,
      "learning_rate": 0.00011753618724976903,
      "loss": 0.1789,
      "step": 20090
    },
    {
      "epoch": 1.2380659069910687,
      "grad_norm": 0.15825635194778442,
      "learning_rate": 0.00011749512370393184,
      "loss": 0.1796,
      "step": 20100
    },
    {
      "epoch": 1.2386818601786265,
      "grad_norm": 0.15514393150806427,
      "learning_rate": 0.00011745406015809466,
      "loss": 0.1791,
      "step": 20110
    },
    {
      "epoch": 1.2392978133661843,
      "grad_norm": 0.15579506754875183,
      "learning_rate": 0.00011741299661225746,
      "loss": 0.1782,
      "step": 20120
    },
    {
      "epoch": 1.2399137665537419,
      "grad_norm": 0.21275882422924042,
      "learning_rate": 0.0001173719330664203,
      "loss": 0.1771,
      "step": 20130
    },
    {
      "epoch": 1.2405297197412997,
      "grad_norm": 0.1496652066707611,
      "learning_rate": 0.0001173308695205831,
      "loss": 0.1766,
      "step": 20140
    },
    {
      "epoch": 1.2411456729288575,
      "grad_norm": 0.18322788178920746,
      "learning_rate": 0.00011728980597474592,
      "loss": 0.1789,
      "step": 20150
    },
    {
      "epoch": 1.241761626116415,
      "grad_norm": 0.17236173152923584,
      "learning_rate": 0.00011724874242890873,
      "loss": 0.1811,
      "step": 20160
    },
    {
      "epoch": 1.2423775793039729,
      "grad_norm": 0.16927185654640198,
      "learning_rate": 0.00011720767888307156,
      "loss": 0.1779,
      "step": 20170
    },
    {
      "epoch": 1.2429935324915307,
      "grad_norm": 0.15445344150066376,
      "learning_rate": 0.00011716661533723438,
      "loss": 0.1783,
      "step": 20180
    },
    {
      "epoch": 1.2436094856790885,
      "grad_norm": 0.14614765346050262,
      "learning_rate": 0.00011712555179139719,
      "loss": 0.1777,
      "step": 20190
    },
    {
      "epoch": 1.244225438866646,
      "grad_norm": 0.14773690700531006,
      "learning_rate": 0.00011708448824556002,
      "loss": 0.1789,
      "step": 20200
    },
    {
      "epoch": 1.2448413920542039,
      "grad_norm": 0.17984721064567566,
      "learning_rate": 0.00011704342469972283,
      "loss": 0.1773,
      "step": 20210
    },
    {
      "epoch": 1.2454573452417617,
      "grad_norm": 0.1503974050283432,
      "learning_rate": 0.00011700236115388565,
      "loss": 0.1788,
      "step": 20220
    },
    {
      "epoch": 1.2460732984293195,
      "grad_norm": 0.1603141725063324,
      "learning_rate": 0.00011696129760804846,
      "loss": 0.1764,
      "step": 20230
    },
    {
      "epoch": 1.246689251616877,
      "grad_norm": 0.16911686956882477,
      "learning_rate": 0.00011692023406221129,
      "loss": 0.1771,
      "step": 20240
    },
    {
      "epoch": 1.2473052048044349,
      "grad_norm": 0.17149899899959564,
      "learning_rate": 0.0001168791705163741,
      "loss": 0.1784,
      "step": 20250
    },
    {
      "epoch": 1.2479211579919927,
      "grad_norm": 0.14805170893669128,
      "learning_rate": 0.00011683810697053692,
      "loss": 0.18,
      "step": 20260
    },
    {
      "epoch": 1.2485371111795502,
      "grad_norm": 0.15700547397136688,
      "learning_rate": 0.00011679704342469972,
      "loss": 0.1808,
      "step": 20270
    },
    {
      "epoch": 1.249153064367108,
      "grad_norm": 0.15321588516235352,
      "learning_rate": 0.00011675597987886256,
      "loss": 0.1768,
      "step": 20280
    },
    {
      "epoch": 1.2497690175546659,
      "grad_norm": 0.1634100079536438,
      "learning_rate": 0.00011671491633302536,
      "loss": 0.1789,
      "step": 20290
    },
    {
      "epoch": 1.2503849707422237,
      "grad_norm": 0.19022397696971893,
      "learning_rate": 0.00011667385278718818,
      "loss": 0.1789,
      "step": 20300
    },
    {
      "epoch": 1.2510009239297815,
      "grad_norm": 0.17363175749778748,
      "learning_rate": 0.00011663278924135099,
      "loss": 0.1762,
      "step": 20310
    },
    {
      "epoch": 1.251616877117339,
      "grad_norm": 0.17502132058143616,
      "learning_rate": 0.00011659172569551382,
      "loss": 0.1776,
      "step": 20320
    },
    {
      "epoch": 1.2522328303048968,
      "grad_norm": 0.20328488945960999,
      "learning_rate": 0.00011655066214967663,
      "loss": 0.1805,
      "step": 20330
    },
    {
      "epoch": 1.2528487834924547,
      "grad_norm": 0.1441076248884201,
      "learning_rate": 0.00011650959860383945,
      "loss": 0.1787,
      "step": 20340
    },
    {
      "epoch": 1.2534647366800122,
      "grad_norm": 0.15443725883960724,
      "learning_rate": 0.00011646853505800226,
      "loss": 0.1791,
      "step": 20350
    },
    {
      "epoch": 1.25408068986757,
      "grad_norm": 0.1459771692752838,
      "learning_rate": 0.00011642747151216509,
      "loss": 0.1768,
      "step": 20360
    },
    {
      "epoch": 1.2546966430551278,
      "grad_norm": 0.1616714894771576,
      "learning_rate": 0.0001163864079663279,
      "loss": 0.1793,
      "step": 20370
    },
    {
      "epoch": 1.2553125962426854,
      "grad_norm": 0.163765087723732,
      "learning_rate": 0.00011634534442049072,
      "loss": 0.1784,
      "step": 20380
    },
    {
      "epoch": 1.2559285494302432,
      "grad_norm": 0.16998983919620514,
      "learning_rate": 0.00011630428087465352,
      "loss": 0.1793,
      "step": 20390
    },
    {
      "epoch": 1.256544502617801,
      "grad_norm": 0.19930937886238098,
      "learning_rate": 0.00011626321732881636,
      "loss": 0.1766,
      "step": 20400
    },
    {
      "epoch": 1.2571604558053588,
      "grad_norm": 0.16541653871536255,
      "learning_rate": 0.00011622215378297916,
      "loss": 0.1792,
      "step": 20410
    },
    {
      "epoch": 1.2577764089929167,
      "grad_norm": 0.14558202028274536,
      "learning_rate": 0.00011618109023714198,
      "loss": 0.1765,
      "step": 20420
    },
    {
      "epoch": 1.2583923621804742,
      "grad_norm": 0.16921241581439972,
      "learning_rate": 0.00011614002669130479,
      "loss": 0.18,
      "step": 20430
    },
    {
      "epoch": 1.259008315368032,
      "grad_norm": 0.15134240686893463,
      "learning_rate": 0.00011609896314546762,
      "loss": 0.1801,
      "step": 20440
    },
    {
      "epoch": 1.2596242685555898,
      "grad_norm": 0.1493835300207138,
      "learning_rate": 0.00011605789959963043,
      "loss": 0.1805,
      "step": 20450
    },
    {
      "epoch": 1.2602402217431474,
      "grad_norm": 0.17276142537593842,
      "learning_rate": 0.00011601683605379325,
      "loss": 0.1764,
      "step": 20460
    },
    {
      "epoch": 1.2608561749307052,
      "grad_norm": 0.17722421884536743,
      "learning_rate": 0.00011597577250795606,
      "loss": 0.1776,
      "step": 20470
    },
    {
      "epoch": 1.261472128118263,
      "grad_norm": 0.16771556437015533,
      "learning_rate": 0.00011593470896211889,
      "loss": 0.1807,
      "step": 20480
    },
    {
      "epoch": 1.2620880813058206,
      "grad_norm": 0.15727481245994568,
      "learning_rate": 0.0001158936454162817,
      "loss": 0.1816,
      "step": 20490
    },
    {
      "epoch": 1.2627040344933784,
      "grad_norm": 0.19757859408855438,
      "learning_rate": 0.00011585258187044452,
      "loss": 0.1762,
      "step": 20500
    },
    {
      "epoch": 1.2633199876809362,
      "grad_norm": 0.15870369970798492,
      "learning_rate": 0.00011581151832460732,
      "loss": 0.1793,
      "step": 20510
    },
    {
      "epoch": 1.263935940868494,
      "grad_norm": 0.16319136321544647,
      "learning_rate": 0.00011577045477877016,
      "loss": 0.1774,
      "step": 20520
    },
    {
      "epoch": 1.2645518940560518,
      "grad_norm": 0.16351735591888428,
      "learning_rate": 0.00011572939123293296,
      "loss": 0.1803,
      "step": 20530
    },
    {
      "epoch": 1.2651678472436094,
      "grad_norm": 0.14196962118148804,
      "learning_rate": 0.00011568832768709578,
      "loss": 0.1774,
      "step": 20540
    },
    {
      "epoch": 1.2657838004311672,
      "grad_norm": 0.18193669617176056,
      "learning_rate": 0.00011564726414125859,
      "loss": 0.1769,
      "step": 20550
    },
    {
      "epoch": 1.266399753618725,
      "grad_norm": 0.16353443264961243,
      "learning_rate": 0.00011560620059542142,
      "loss": 0.1798,
      "step": 20560
    },
    {
      "epoch": 1.2670157068062826,
      "grad_norm": 0.17408104240894318,
      "learning_rate": 0.00011556513704958423,
      "loss": 0.1801,
      "step": 20570
    },
    {
      "epoch": 1.2676316599938404,
      "grad_norm": 0.13816291093826294,
      "learning_rate": 0.00011552407350374705,
      "loss": 0.1794,
      "step": 20580
    },
    {
      "epoch": 1.2682476131813982,
      "grad_norm": 0.5176965594291687,
      "learning_rate": 0.00011548300995790985,
      "loss": 0.1786,
      "step": 20590
    },
    {
      "epoch": 1.268863566368956,
      "grad_norm": 0.14010083675384521,
      "learning_rate": 0.00011544194641207269,
      "loss": 0.1779,
      "step": 20600
    },
    {
      "epoch": 1.2694795195565138,
      "grad_norm": 0.15955328941345215,
      "learning_rate": 0.00011540088286623552,
      "loss": 0.1778,
      "step": 20610
    },
    {
      "epoch": 1.2700954727440714,
      "grad_norm": 0.16872075200080872,
      "learning_rate": 0.00011535981932039831,
      "loss": 0.1764,
      "step": 20620
    },
    {
      "epoch": 1.2707114259316292,
      "grad_norm": 0.16646994650363922,
      "learning_rate": 0.00011531875577456115,
      "loss": 0.1772,
      "step": 20630
    },
    {
      "epoch": 1.271327379119187,
      "grad_norm": 0.14389568567276,
      "learning_rate": 0.00011527769222872395,
      "loss": 0.1767,
      "step": 20640
    },
    {
      "epoch": 1.2719433323067446,
      "grad_norm": 0.16598960757255554,
      "learning_rate": 0.00011523662868288679,
      "loss": 0.178,
      "step": 20650
    },
    {
      "epoch": 1.2725592854943024,
      "grad_norm": 0.18234974145889282,
      "learning_rate": 0.00011519556513704958,
      "loss": 0.1781,
      "step": 20660
    },
    {
      "epoch": 1.2731752386818602,
      "grad_norm": 0.1537405252456665,
      "learning_rate": 0.00011515450159121242,
      "loss": 0.1774,
      "step": 20670
    },
    {
      "epoch": 1.2737911918694178,
      "grad_norm": 0.1514119654893875,
      "learning_rate": 0.00011511343804537522,
      "loss": 0.1803,
      "step": 20680
    },
    {
      "epoch": 1.2744071450569756,
      "grad_norm": 0.15487392246723175,
      "learning_rate": 0.00011507237449953806,
      "loss": 0.1786,
      "step": 20690
    },
    {
      "epoch": 1.2750230982445334,
      "grad_norm": 0.14867593348026276,
      "learning_rate": 0.00011503131095370085,
      "loss": 0.1795,
      "step": 20700
    },
    {
      "epoch": 1.2756390514320912,
      "grad_norm": 0.1487310379743576,
      "learning_rate": 0.00011499024740786368,
      "loss": 0.1792,
      "step": 20710
    },
    {
      "epoch": 1.276255004619649,
      "grad_norm": 0.21349023282527924,
      "learning_rate": 0.00011494918386202649,
      "loss": 0.1763,
      "step": 20720
    },
    {
      "epoch": 1.2768709578072066,
      "grad_norm": 0.17342068254947662,
      "learning_rate": 0.00011490812031618932,
      "loss": 0.1781,
      "step": 20730
    },
    {
      "epoch": 1.2774869109947644,
      "grad_norm": 0.1754656285047531,
      "learning_rate": 0.00011486705677035213,
      "loss": 0.1832,
      "step": 20740
    },
    {
      "epoch": 1.2781028641823222,
      "grad_norm": 0.17387419939041138,
      "learning_rate": 0.00011482599322451495,
      "loss": 0.1805,
      "step": 20750
    },
    {
      "epoch": 1.2787188173698798,
      "grad_norm": 0.15842975676059723,
      "learning_rate": 0.00011478492967867775,
      "loss": 0.178,
      "step": 20760
    },
    {
      "epoch": 1.2793347705574376,
      "grad_norm": 0.18069888651371002,
      "learning_rate": 0.00011474386613284059,
      "loss": 0.177,
      "step": 20770
    },
    {
      "epoch": 1.2799507237449954,
      "grad_norm": 0.13829919695854187,
      "learning_rate": 0.0001147028025870034,
      "loss": 0.1765,
      "step": 20780
    },
    {
      "epoch": 1.280566676932553,
      "grad_norm": 0.1474648118019104,
      "learning_rate": 0.00011466173904116621,
      "loss": 0.1776,
      "step": 20790
    },
    {
      "epoch": 1.2811826301201108,
      "grad_norm": 0.14512619376182556,
      "learning_rate": 0.00011462067549532902,
      "loss": 0.1785,
      "step": 20800
    },
    {
      "epoch": 1.2817985833076686,
      "grad_norm": 0.18490444123744965,
      "learning_rate": 0.00011457961194949185,
      "loss": 0.1801,
      "step": 20810
    },
    {
      "epoch": 1.2824145364952264,
      "grad_norm": 0.1523023545742035,
      "learning_rate": 0.00011453854840365466,
      "loss": 0.1795,
      "step": 20820
    },
    {
      "epoch": 1.2830304896827842,
      "grad_norm": 0.16410794854164124,
      "learning_rate": 0.00011449748485781748,
      "loss": 0.1738,
      "step": 20830
    },
    {
      "epoch": 1.2836464428703418,
      "grad_norm": 0.15215052664279938,
      "learning_rate": 0.00011445642131198029,
      "loss": 0.1792,
      "step": 20840
    },
    {
      "epoch": 1.2842623960578996,
      "grad_norm": 0.14737769961357117,
      "learning_rate": 0.00011441535776614312,
      "loss": 0.1778,
      "step": 20850
    },
    {
      "epoch": 1.2848783492454574,
      "grad_norm": 0.1573108285665512,
      "learning_rate": 0.00011437429422030593,
      "loss": 0.18,
      "step": 20860
    },
    {
      "epoch": 1.285494302433015,
      "grad_norm": 0.21683110296726227,
      "learning_rate": 0.00011433323067446875,
      "loss": 0.1789,
      "step": 20870
    },
    {
      "epoch": 1.2861102556205728,
      "grad_norm": 0.17664414644241333,
      "learning_rate": 0.00011429216712863155,
      "loss": 0.1794,
      "step": 20880
    },
    {
      "epoch": 1.2867262088081306,
      "grad_norm": 0.17084810137748718,
      "learning_rate": 0.00011425110358279439,
      "loss": 0.1777,
      "step": 20890
    },
    {
      "epoch": 1.2873421619956884,
      "grad_norm": 0.15549691021442413,
      "learning_rate": 0.0001142100400369572,
      "loss": 0.1774,
      "step": 20900
    },
    {
      "epoch": 1.2879581151832462,
      "grad_norm": 0.17121468484401703,
      "learning_rate": 0.00011416897649112001,
      "loss": 0.176,
      "step": 20910
    },
    {
      "epoch": 1.2885740683708038,
      "grad_norm": 0.16509029269218445,
      "learning_rate": 0.00011412791294528282,
      "loss": 0.1788,
      "step": 20920
    },
    {
      "epoch": 1.2891900215583616,
      "grad_norm": 0.17004618048667908,
      "learning_rate": 0.00011408684939944565,
      "loss": 0.1773,
      "step": 20930
    },
    {
      "epoch": 1.2898059747459194,
      "grad_norm": 0.26370540261268616,
      "learning_rate": 0.00011404578585360846,
      "loss": 0.1792,
      "step": 20940
    },
    {
      "epoch": 1.290421927933477,
      "grad_norm": 0.1407003402709961,
      "learning_rate": 0.00011400472230777128,
      "loss": 0.177,
      "step": 20950
    },
    {
      "epoch": 1.2910378811210348,
      "grad_norm": 0.14915835857391357,
      "learning_rate": 0.00011396365876193409,
      "loss": 0.1804,
      "step": 20960
    },
    {
      "epoch": 1.2916538343085926,
      "grad_norm": 0.15537700057029724,
      "learning_rate": 0.00011392259521609692,
      "loss": 0.1806,
      "step": 20970
    },
    {
      "epoch": 1.2922697874961502,
      "grad_norm": 0.1962420791387558,
      "learning_rate": 0.00011388153167025973,
      "loss": 0.1787,
      "step": 20980
    },
    {
      "epoch": 1.292885740683708,
      "grad_norm": 0.1530531346797943,
      "learning_rate": 0.00011384046812442255,
      "loss": 0.1774,
      "step": 20990
    },
    {
      "epoch": 1.2935016938712658,
      "grad_norm": 0.1600295603275299,
      "learning_rate": 0.00011379940457858535,
      "loss": 0.1796,
      "step": 21000
    },
    {
      "epoch": 1.2941176470588236,
      "grad_norm": 0.16800901293754578,
      "learning_rate": 0.00011375834103274819,
      "loss": 0.1805,
      "step": 21010
    },
    {
      "epoch": 1.2947336002463814,
      "grad_norm": 0.16810612380504608,
      "learning_rate": 0.000113717277486911,
      "loss": 0.18,
      "step": 21020
    },
    {
      "epoch": 1.295349553433939,
      "grad_norm": 0.19988666474819183,
      "learning_rate": 0.00011367621394107381,
      "loss": 0.1764,
      "step": 21030
    },
    {
      "epoch": 1.2959655066214968,
      "grad_norm": 0.15290924906730652,
      "learning_rate": 0.00011363515039523665,
      "loss": 0.1807,
      "step": 21040
    },
    {
      "epoch": 1.2965814598090546,
      "grad_norm": 0.18369129300117493,
      "learning_rate": 0.00011359408684939945,
      "loss": 0.179,
      "step": 21050
    },
    {
      "epoch": 1.2971974129966122,
      "grad_norm": 0.1515430361032486,
      "learning_rate": 0.00011355302330356227,
      "loss": 0.177,
      "step": 21060
    },
    {
      "epoch": 1.29781336618417,
      "grad_norm": 0.5501571893692017,
      "learning_rate": 0.00011351195975772508,
      "loss": 0.18,
      "step": 21070
    },
    {
      "epoch": 1.2984293193717278,
      "grad_norm": 0.16206121444702148,
      "learning_rate": 0.00011347089621188791,
      "loss": 0.1797,
      "step": 21080
    },
    {
      "epoch": 1.2990452725592854,
      "grad_norm": 0.15031550824642181,
      "learning_rate": 0.00011342983266605072,
      "loss": 0.1778,
      "step": 21090
    },
    {
      "epoch": 1.2996612257468432,
      "grad_norm": 0.19165204465389252,
      "learning_rate": 0.00011338876912021354,
      "loss": 0.1795,
      "step": 21100
    },
    {
      "epoch": 1.300277178934401,
      "grad_norm": 0.15179678797721863,
      "learning_rate": 0.00011334770557437635,
      "loss": 0.1826,
      "step": 21110
    },
    {
      "epoch": 1.3008931321219588,
      "grad_norm": 0.1631835699081421,
      "learning_rate": 0.00011330664202853918,
      "loss": 0.1793,
      "step": 21120
    },
    {
      "epoch": 1.3015090853095166,
      "grad_norm": 0.15771234035491943,
      "learning_rate": 0.00011326557848270199,
      "loss": 0.1783,
      "step": 21130
    },
    {
      "epoch": 1.3021250384970742,
      "grad_norm": 0.1794324368238449,
      "learning_rate": 0.00011322451493686481,
      "loss": 0.1767,
      "step": 21140
    },
    {
      "epoch": 1.302740991684632,
      "grad_norm": 0.17075222730636597,
      "learning_rate": 0.00011318345139102761,
      "loss": 0.1794,
      "step": 21150
    },
    {
      "epoch": 1.3033569448721898,
      "grad_norm": 0.38686293363571167,
      "learning_rate": 0.00011314238784519045,
      "loss": 0.1794,
      "step": 21160
    },
    {
      "epoch": 1.3039728980597474,
      "grad_norm": 0.1520552784204483,
      "learning_rate": 0.00011310132429935325,
      "loss": 0.177,
      "step": 21170
    },
    {
      "epoch": 1.3045888512473052,
      "grad_norm": 0.20436885952949524,
      "learning_rate": 0.00011306026075351607,
      "loss": 0.1803,
      "step": 21180
    },
    {
      "epoch": 1.305204804434863,
      "grad_norm": 0.20209982991218567,
      "learning_rate": 0.00011301919720767888,
      "loss": 0.1795,
      "step": 21190
    },
    {
      "epoch": 1.3058207576224206,
      "grad_norm": 0.23297233879566193,
      "learning_rate": 0.00011297813366184171,
      "loss": 0.1812,
      "step": 21200
    },
    {
      "epoch": 1.3064367108099784,
      "grad_norm": 0.8474110960960388,
      "learning_rate": 0.00011293707011600452,
      "loss": 0.1817,
      "step": 21210
    },
    {
      "epoch": 1.3070526639975362,
      "grad_norm": 0.2552272081375122,
      "learning_rate": 0.00011289600657016734,
      "loss": 0.1812,
      "step": 21220
    },
    {
      "epoch": 1.307668617185094,
      "grad_norm": 0.17369496822357178,
      "learning_rate": 0.00011285494302433015,
      "loss": 0.1785,
      "step": 21230
    },
    {
      "epoch": 1.3082845703726518,
      "grad_norm": 0.18472763895988464,
      "learning_rate": 0.00011281387947849298,
      "loss": 0.1796,
      "step": 21240
    },
    {
      "epoch": 1.3089005235602094,
      "grad_norm": 0.1832960844039917,
      "learning_rate": 0.00011277281593265579,
      "loss": 0.1784,
      "step": 21250
    },
    {
      "epoch": 1.3095164767477672,
      "grad_norm": 0.15655888617038727,
      "learning_rate": 0.00011273175238681861,
      "loss": 0.1781,
      "step": 21260
    },
    {
      "epoch": 1.310132429935325,
      "grad_norm": 0.16488017141819,
      "learning_rate": 0.00011269068884098141,
      "loss": 0.1764,
      "step": 21270
    },
    {
      "epoch": 1.3107483831228826,
      "grad_norm": 0.2000788152217865,
      "learning_rate": 0.00011264962529514425,
      "loss": 0.177,
      "step": 21280
    },
    {
      "epoch": 1.3113643363104404,
      "grad_norm": 0.1864290088415146,
      "learning_rate": 0.00011260856174930705,
      "loss": 0.179,
      "step": 21290
    },
    {
      "epoch": 1.3119802894979982,
      "grad_norm": 0.3639797866344452,
      "learning_rate": 0.00011256749820346987,
      "loss": 0.1793,
      "step": 21300
    },
    {
      "epoch": 1.312596242685556,
      "grad_norm": 0.14033716917037964,
      "learning_rate": 0.00011252643465763268,
      "loss": 0.1805,
      "step": 21310
    },
    {
      "epoch": 1.3132121958731138,
      "grad_norm": 0.1728295236825943,
      "learning_rate": 0.00011248537111179551,
      "loss": 0.1794,
      "step": 21320
    },
    {
      "epoch": 1.3138281490606714,
      "grad_norm": 0.16499845683574677,
      "learning_rate": 0.00011244430756595832,
      "loss": 0.1781,
      "step": 21330
    },
    {
      "epoch": 1.3144441022482292,
      "grad_norm": 0.35543715953826904,
      "learning_rate": 0.00011240324402012114,
      "loss": 0.1778,
      "step": 21340
    },
    {
      "epoch": 1.315060055435787,
      "grad_norm": 0.13283787667751312,
      "learning_rate": 0.00011236218047428395,
      "loss": 0.1784,
      "step": 21350
    },
    {
      "epoch": 1.3156760086233446,
      "grad_norm": 0.16838957369327545,
      "learning_rate": 0.00011232111692844678,
      "loss": 0.1781,
      "step": 21360
    },
    {
      "epoch": 1.3162919618109024,
      "grad_norm": 0.14197935163974762,
      "learning_rate": 0.00011228005338260959,
      "loss": 0.1782,
      "step": 21370
    },
    {
      "epoch": 1.3169079149984602,
      "grad_norm": 0.1621394157409668,
      "learning_rate": 0.00011223898983677241,
      "loss": 0.1763,
      "step": 21380
    },
    {
      "epoch": 1.3175238681860177,
      "grad_norm": 0.1626114696264267,
      "learning_rate": 0.00011219792629093521,
      "loss": 0.1791,
      "step": 21390
    },
    {
      "epoch": 1.3181398213735755,
      "grad_norm": 0.16074210405349731,
      "learning_rate": 0.00011215686274509805,
      "loss": 0.1806,
      "step": 21400
    },
    {
      "epoch": 1.3187557745611334,
      "grad_norm": 0.15593992173671722,
      "learning_rate": 0.00011211579919926085,
      "loss": 0.1767,
      "step": 21410
    },
    {
      "epoch": 1.3193717277486912,
      "grad_norm": 0.19190345704555511,
      "learning_rate": 0.00011207473565342367,
      "loss": 0.1795,
      "step": 21420
    },
    {
      "epoch": 1.319987680936249,
      "grad_norm": 0.1354501098394394,
      "learning_rate": 0.00011203367210758648,
      "loss": 0.1799,
      "step": 21430
    },
    {
      "epoch": 1.3206036341238065,
      "grad_norm": 0.1791578233242035,
      "learning_rate": 0.00011199260856174931,
      "loss": 0.1771,
      "step": 21440
    },
    {
      "epoch": 1.3212195873113644,
      "grad_norm": 0.190371572971344,
      "learning_rate": 0.00011195154501591213,
      "loss": 0.1778,
      "step": 21450
    },
    {
      "epoch": 1.3218355404989222,
      "grad_norm": 0.15242551267147064,
      "learning_rate": 0.00011191048147007494,
      "loss": 0.1804,
      "step": 21460
    },
    {
      "epoch": 1.3224514936864797,
      "grad_norm": 0.1461404263973236,
      "learning_rate": 0.00011186941792423777,
      "loss": 0.1792,
      "step": 21470
    },
    {
      "epoch": 1.3230674468740375,
      "grad_norm": 0.6010628938674927,
      "learning_rate": 0.00011182835437840058,
      "loss": 0.1811,
      "step": 21480
    },
    {
      "epoch": 1.3236834000615953,
      "grad_norm": 0.16629640758037567,
      "learning_rate": 0.0001117872908325634,
      "loss": 0.1782,
      "step": 21490
    },
    {
      "epoch": 1.324299353249153,
      "grad_norm": 0.14700131118297577,
      "learning_rate": 0.00011174622728672621,
      "loss": 0.1781,
      "step": 21500
    },
    {
      "epoch": 1.3249153064367107,
      "grad_norm": 0.16166652739048004,
      "learning_rate": 0.00011170516374088904,
      "loss": 0.1771,
      "step": 21510
    },
    {
      "epoch": 1.3255312596242685,
      "grad_norm": 0.16057640314102173,
      "learning_rate": 0.00011166410019505185,
      "loss": 0.1793,
      "step": 21520
    },
    {
      "epoch": 1.3261472128118263,
      "grad_norm": 0.18226122856140137,
      "learning_rate": 0.00011162303664921467,
      "loss": 0.1791,
      "step": 21530
    },
    {
      "epoch": 1.3267631659993842,
      "grad_norm": 0.13827167451381683,
      "learning_rate": 0.00011158197310337747,
      "loss": 0.1782,
      "step": 21540
    },
    {
      "epoch": 1.3273791191869417,
      "grad_norm": 0.15993385016918182,
      "learning_rate": 0.00011154090955754031,
      "loss": 0.1793,
      "step": 21550
    },
    {
      "epoch": 1.3279950723744995,
      "grad_norm": 0.1484757959842682,
      "learning_rate": 0.00011149984601170311,
      "loss": 0.1808,
      "step": 21560
    },
    {
      "epoch": 1.3286110255620573,
      "grad_norm": 0.1581064909696579,
      "learning_rate": 0.00011145878246586595,
      "loss": 0.1781,
      "step": 21570
    },
    {
      "epoch": 1.329226978749615,
      "grad_norm": 0.1921776831150055,
      "learning_rate": 0.00011141771892002874,
      "loss": 0.178,
      "step": 21580
    },
    {
      "epoch": 1.3298429319371727,
      "grad_norm": 0.15467938780784607,
      "learning_rate": 0.00011137665537419157,
      "loss": 0.1773,
      "step": 21590
    },
    {
      "epoch": 1.3304588851247305,
      "grad_norm": 0.17987754940986633,
      "learning_rate": 0.00011133559182835438,
      "loss": 0.1779,
      "step": 21600
    },
    {
      "epoch": 1.3310748383122883,
      "grad_norm": 0.13308610022068024,
      "learning_rate": 0.00011129452828251721,
      "loss": 0.1797,
      "step": 21610
    },
    {
      "epoch": 1.3316907914998461,
      "grad_norm": 0.17141790688037872,
      "learning_rate": 0.00011125346473668001,
      "loss": 0.1791,
      "step": 21620
    },
    {
      "epoch": 1.3323067446874037,
      "grad_norm": 0.17123255133628845,
      "learning_rate": 0.00011121240119084284,
      "loss": 0.1761,
      "step": 21630
    },
    {
      "epoch": 1.3329226978749615,
      "grad_norm": 0.1527537703514099,
      "learning_rate": 0.00011117133764500565,
      "loss": 0.178,
      "step": 21640
    },
    {
      "epoch": 1.3335386510625193,
      "grad_norm": 0.19622671604156494,
      "learning_rate": 0.00011113027409916848,
      "loss": 0.1771,
      "step": 21650
    },
    {
      "epoch": 1.334154604250077,
      "grad_norm": 0.1471819132566452,
      "learning_rate": 0.00011108921055333127,
      "loss": 0.1769,
      "step": 21660
    },
    {
      "epoch": 1.3347705574376347,
      "grad_norm": 0.1519690304994583,
      "learning_rate": 0.00011104814700749411,
      "loss": 0.175,
      "step": 21670
    },
    {
      "epoch": 1.3353865106251925,
      "grad_norm": 0.15394166111946106,
      "learning_rate": 0.00011100708346165691,
      "loss": 0.1786,
      "step": 21680
    },
    {
      "epoch": 1.3360024638127501,
      "grad_norm": 0.19351400434970856,
      "learning_rate": 0.00011096601991581975,
      "loss": 0.1806,
      "step": 21690
    },
    {
      "epoch": 1.336618417000308,
      "grad_norm": 0.19910307228565216,
      "learning_rate": 0.00011092495636998254,
      "loss": 0.1784,
      "step": 21700
    },
    {
      "epoch": 1.3372343701878657,
      "grad_norm": 0.15556910634040833,
      "learning_rate": 0.00011088389282414537,
      "loss": 0.1771,
      "step": 21710
    },
    {
      "epoch": 1.3378503233754235,
      "grad_norm": 0.15508213639259338,
      "learning_rate": 0.00011084282927830818,
      "loss": 0.1755,
      "step": 21720
    },
    {
      "epoch": 1.3384662765629813,
      "grad_norm": 0.18108531832695007,
      "learning_rate": 0.00011080176573247101,
      "loss": 0.1755,
      "step": 21730
    },
    {
      "epoch": 1.339082229750539,
      "grad_norm": 0.15306013822555542,
      "learning_rate": 0.0001107607021866338,
      "loss": 0.1774,
      "step": 21740
    },
    {
      "epoch": 1.3396981829380967,
      "grad_norm": 0.14634670317173004,
      "learning_rate": 0.00011071963864079664,
      "loss": 0.1774,
      "step": 21750
    },
    {
      "epoch": 1.3403141361256545,
      "grad_norm": 0.16895021498203278,
      "learning_rate": 0.00011067857509495945,
      "loss": 0.1778,
      "step": 21760
    },
    {
      "epoch": 1.3409300893132121,
      "grad_norm": 0.14317671954631805,
      "learning_rate": 0.00011063751154912228,
      "loss": 0.178,
      "step": 21770
    },
    {
      "epoch": 1.34154604250077,
      "grad_norm": 0.1540875881910324,
      "learning_rate": 0.00011059644800328509,
      "loss": 0.179,
      "step": 21780
    },
    {
      "epoch": 1.3421619956883277,
      "grad_norm": 0.17162266373634338,
      "learning_rate": 0.00011055538445744791,
      "loss": 0.1808,
      "step": 21790
    },
    {
      "epoch": 1.3427779488758853,
      "grad_norm": 0.1349007785320282,
      "learning_rate": 0.00011051432091161071,
      "loss": 0.1779,
      "step": 21800
    },
    {
      "epoch": 1.3433939020634431,
      "grad_norm": 0.13243207335472107,
      "learning_rate": 0.00011047325736577355,
      "loss": 0.1765,
      "step": 21810
    },
    {
      "epoch": 1.344009855251001,
      "grad_norm": 0.19009993970394135,
      "learning_rate": 0.00011043219381993635,
      "loss": 0.1787,
      "step": 21820
    },
    {
      "epoch": 1.3446258084385587,
      "grad_norm": 0.15464133024215698,
      "learning_rate": 0.00011039113027409917,
      "loss": 0.1781,
      "step": 21830
    },
    {
      "epoch": 1.3452417616261165,
      "grad_norm": 0.15078209340572357,
      "learning_rate": 0.00011035006672826198,
      "loss": 0.1791,
      "step": 21840
    },
    {
      "epoch": 1.345857714813674,
      "grad_norm": 0.16096779704093933,
      "learning_rate": 0.00011030900318242481,
      "loss": 0.1774,
      "step": 21850
    },
    {
      "epoch": 1.346473668001232,
      "grad_norm": 0.1644018292427063,
      "learning_rate": 0.00011026793963658762,
      "loss": 0.1799,
      "step": 21860
    },
    {
      "epoch": 1.3470896211887897,
      "grad_norm": 0.16458415985107422,
      "learning_rate": 0.00011022687609075044,
      "loss": 0.177,
      "step": 21870
    },
    {
      "epoch": 1.3477055743763473,
      "grad_norm": 0.1623455286026001,
      "learning_rate": 0.00011018581254491327,
      "loss": 0.1782,
      "step": 21880
    },
    {
      "epoch": 1.348321527563905,
      "grad_norm": 0.13711442053318024,
      "learning_rate": 0.00011014474899907608,
      "loss": 0.177,
      "step": 21890
    },
    {
      "epoch": 1.348937480751463,
      "grad_norm": 0.1647259145975113,
      "learning_rate": 0.0001101036854532389,
      "loss": 0.1807,
      "step": 21900
    },
    {
      "epoch": 1.3495534339390205,
      "grad_norm": 0.17012037336826324,
      "learning_rate": 0.0001100626219074017,
      "loss": 0.177,
      "step": 21910
    },
    {
      "epoch": 1.3501693871265783,
      "grad_norm": 0.1750127673149109,
      "learning_rate": 0.00011002155836156454,
      "loss": 0.1744,
      "step": 21920
    },
    {
      "epoch": 1.350785340314136,
      "grad_norm": 0.3424963057041168,
      "learning_rate": 0.00010998049481572735,
      "loss": 0.1792,
      "step": 21930
    },
    {
      "epoch": 1.351401293501694,
      "grad_norm": 0.1684120148420334,
      "learning_rate": 0.00010993943126989017,
      "loss": 0.1748,
      "step": 21940
    },
    {
      "epoch": 1.3520172466892517,
      "grad_norm": 0.14503586292266846,
      "learning_rate": 0.00010989836772405297,
      "loss": 0.1794,
      "step": 21950
    },
    {
      "epoch": 1.3526331998768093,
      "grad_norm": 0.1505204290151596,
      "learning_rate": 0.00010985730417821581,
      "loss": 0.18,
      "step": 21960
    },
    {
      "epoch": 1.353249153064367,
      "grad_norm": 0.16943587362766266,
      "learning_rate": 0.00010981624063237861,
      "loss": 0.1767,
      "step": 21970
    },
    {
      "epoch": 1.353865106251925,
      "grad_norm": 0.13272123038768768,
      "learning_rate": 0.00010977517708654143,
      "loss": 0.1778,
      "step": 21980
    },
    {
      "epoch": 1.3544810594394825,
      "grad_norm": 0.1494801640510559,
      "learning_rate": 0.00010973411354070424,
      "loss": 0.1769,
      "step": 21990
    },
    {
      "epoch": 1.3550970126270403,
      "grad_norm": 0.17966288328170776,
      "learning_rate": 0.00010969304999486707,
      "loss": 0.1771,
      "step": 22000
    },
    {
      "epoch": 1.355712965814598,
      "grad_norm": 0.17608973383903503,
      "learning_rate": 0.00010965198644902988,
      "loss": 0.1798,
      "step": 22010
    },
    {
      "epoch": 1.356328919002156,
      "grad_norm": 0.21167348325252533,
      "learning_rate": 0.0001096109229031927,
      "loss": 0.1755,
      "step": 22020
    },
    {
      "epoch": 1.3569448721897137,
      "grad_norm": 0.16051924228668213,
      "learning_rate": 0.0001095698593573555,
      "loss": 0.1778,
      "step": 22030
    },
    {
      "epoch": 1.3575608253772713,
      "grad_norm": 0.14217056334018707,
      "learning_rate": 0.00010952879581151834,
      "loss": 0.178,
      "step": 22040
    },
    {
      "epoch": 1.358176778564829,
      "grad_norm": 0.18211396038532257,
      "learning_rate": 0.00010948773226568115,
      "loss": 0.1791,
      "step": 22050
    },
    {
      "epoch": 1.358792731752387,
      "grad_norm": 0.14573119580745697,
      "learning_rate": 0.00010944666871984397,
      "loss": 0.1751,
      "step": 22060
    },
    {
      "epoch": 1.3594086849399445,
      "grad_norm": 0.15184004604816437,
      "learning_rate": 0.00010940560517400677,
      "loss": 0.1787,
      "step": 22070
    },
    {
      "epoch": 1.3600246381275023,
      "grad_norm": 0.14441019296646118,
      "learning_rate": 0.0001093645416281696,
      "loss": 0.1772,
      "step": 22080
    },
    {
      "epoch": 1.36064059131506,
      "grad_norm": 0.16103442013263702,
      "learning_rate": 0.00010932347808233241,
      "loss": 0.1756,
      "step": 22090
    },
    {
      "epoch": 1.3612565445026177,
      "grad_norm": 0.1448974460363388,
      "learning_rate": 0.00010928241453649523,
      "loss": 0.1796,
      "step": 22100
    },
    {
      "epoch": 1.3618724976901755,
      "grad_norm": 0.208164244890213,
      "learning_rate": 0.00010924135099065804,
      "loss": 0.1798,
      "step": 22110
    },
    {
      "epoch": 1.3624884508777333,
      "grad_norm": 0.1679559350013733,
      "learning_rate": 0.00010920028744482087,
      "loss": 0.1785,
      "step": 22120
    },
    {
      "epoch": 1.363104404065291,
      "grad_norm": 0.18886300921440125,
      "learning_rate": 0.00010915922389898368,
      "loss": 0.1784,
      "step": 22130
    },
    {
      "epoch": 1.363720357252849,
      "grad_norm": 0.152431920170784,
      "learning_rate": 0.0001091181603531465,
      "loss": 0.1777,
      "step": 22140
    },
    {
      "epoch": 1.3643363104404065,
      "grad_norm": 0.1418520212173462,
      "learning_rate": 0.0001090770968073093,
      "loss": 0.1774,
      "step": 22150
    },
    {
      "epoch": 1.3649522636279643,
      "grad_norm": 0.1443876475095749,
      "learning_rate": 0.00010903603326147214,
      "loss": 0.1782,
      "step": 22160
    },
    {
      "epoch": 1.365568216815522,
      "grad_norm": 0.17496228218078613,
      "learning_rate": 0.00010899496971563495,
      "loss": 0.1786,
      "step": 22170
    },
    {
      "epoch": 1.3661841700030797,
      "grad_norm": 0.14006948471069336,
      "learning_rate": 0.00010895390616979777,
      "loss": 0.176,
      "step": 22180
    },
    {
      "epoch": 1.3668001231906375,
      "grad_norm": 0.1637182980775833,
      "learning_rate": 0.00010891284262396057,
      "loss": 0.1812,
      "step": 22190
    },
    {
      "epoch": 1.3674160763781953,
      "grad_norm": 0.1555367112159729,
      "learning_rate": 0.0001088717790781234,
      "loss": 0.1785,
      "step": 22200
    },
    {
      "epoch": 1.3680320295657529,
      "grad_norm": 0.14849823713302612,
      "learning_rate": 0.00010883071553228621,
      "loss": 0.1776,
      "step": 22210
    },
    {
      "epoch": 1.3686479827533107,
      "grad_norm": 0.14105671644210815,
      "learning_rate": 0.00010878965198644903,
      "loss": 0.1785,
      "step": 22220
    },
    {
      "epoch": 1.3692639359408685,
      "grad_norm": 0.14459699392318726,
      "learning_rate": 0.00010874858844061184,
      "loss": 0.1794,
      "step": 22230
    },
    {
      "epoch": 1.3698798891284263,
      "grad_norm": 0.1396961659193039,
      "learning_rate": 0.00010870752489477467,
      "loss": 0.1764,
      "step": 22240
    },
    {
      "epoch": 1.370495842315984,
      "grad_norm": 0.15890105068683624,
      "learning_rate": 0.00010866646134893748,
      "loss": 0.178,
      "step": 22250
    },
    {
      "epoch": 1.3711117955035417,
      "grad_norm": 0.17252397537231445,
      "learning_rate": 0.0001086253978031003,
      "loss": 0.1768,
      "step": 22260
    },
    {
      "epoch": 1.3717277486910995,
      "grad_norm": 0.13474051654338837,
      "learning_rate": 0.0001085843342572631,
      "loss": 0.1775,
      "step": 22270
    },
    {
      "epoch": 1.3723437018786573,
      "grad_norm": 0.16283215582370758,
      "learning_rate": 0.00010854327071142594,
      "loss": 0.1804,
      "step": 22280
    },
    {
      "epoch": 1.3729596550662149,
      "grad_norm": 0.18407219648361206,
      "learning_rate": 0.00010850220716558875,
      "loss": 0.1805,
      "step": 22290
    },
    {
      "epoch": 1.3735756082537727,
      "grad_norm": 0.16781511902809143,
      "learning_rate": 0.00010846114361975157,
      "loss": 0.1765,
      "step": 22300
    },
    {
      "epoch": 1.3741915614413305,
      "grad_norm": 0.15109319984912872,
      "learning_rate": 0.0001084200800739144,
      "loss": 0.1795,
      "step": 22310
    },
    {
      "epoch": 1.3748075146288883,
      "grad_norm": 0.16634206473827362,
      "learning_rate": 0.0001083790165280772,
      "loss": 0.1795,
      "step": 22320
    },
    {
      "epoch": 1.375423467816446,
      "grad_norm": 0.15049432218074799,
      "learning_rate": 0.00010833795298224003,
      "loss": 0.1783,
      "step": 22330
    },
    {
      "epoch": 1.3760394210040037,
      "grad_norm": 0.18404164910316467,
      "learning_rate": 0.00010829688943640283,
      "loss": 0.1779,
      "step": 22340
    },
    {
      "epoch": 1.3766553741915615,
      "grad_norm": 0.16863267123699188,
      "learning_rate": 0.00010825582589056567,
      "loss": 0.1779,
      "step": 22350
    },
    {
      "epoch": 1.3772713273791193,
      "grad_norm": 0.1576312631368637,
      "learning_rate": 0.00010821476234472847,
      "loss": 0.1792,
      "step": 22360
    },
    {
      "epoch": 1.3778872805666769,
      "grad_norm": 0.36806368827819824,
      "learning_rate": 0.00010817369879889129,
      "loss": 0.1774,
      "step": 22370
    },
    {
      "epoch": 1.3785032337542347,
      "grad_norm": 0.12934209406375885,
      "learning_rate": 0.0001081326352530541,
      "loss": 0.1762,
      "step": 22380
    },
    {
      "epoch": 1.3791191869417925,
      "grad_norm": 0.1402592658996582,
      "learning_rate": 0.00010809157170721693,
      "loss": 0.1779,
      "step": 22390
    },
    {
      "epoch": 1.37973514012935,
      "grad_norm": 0.1913057416677475,
      "learning_rate": 0.00010805050816137974,
      "loss": 0.1786,
      "step": 22400
    },
    {
      "epoch": 1.3803510933169079,
      "grad_norm": 0.18671555817127228,
      "learning_rate": 0.00010800944461554256,
      "loss": 0.1798,
      "step": 22410
    },
    {
      "epoch": 1.3809670465044657,
      "grad_norm": 0.23659050464630127,
      "learning_rate": 0.00010796838106970537,
      "loss": 0.1778,
      "step": 22420
    },
    {
      "epoch": 1.3815829996920235,
      "grad_norm": 0.16114191710948944,
      "learning_rate": 0.0001079273175238682,
      "loss": 0.1789,
      "step": 22430
    },
    {
      "epoch": 1.3821989528795813,
      "grad_norm": 0.146579310297966,
      "learning_rate": 0.000107886253978031,
      "loss": 0.1804,
      "step": 22440
    },
    {
      "epoch": 1.3828149060671389,
      "grad_norm": 0.2002502828836441,
      "learning_rate": 0.00010784519043219383,
      "loss": 0.1786,
      "step": 22450
    },
    {
      "epoch": 1.3834308592546967,
      "grad_norm": 0.1633358597755432,
      "learning_rate": 0.00010780412688635663,
      "loss": 0.1792,
      "step": 22460
    },
    {
      "epoch": 1.3840468124422545,
      "grad_norm": 0.1665455549955368,
      "learning_rate": 0.00010776306334051947,
      "loss": 0.1774,
      "step": 22470
    },
    {
      "epoch": 1.384662765629812,
      "grad_norm": 0.163435697555542,
      "learning_rate": 0.00010772199979468227,
      "loss": 0.1786,
      "step": 22480
    },
    {
      "epoch": 1.3852787188173699,
      "grad_norm": 0.17369788885116577,
      "learning_rate": 0.00010768093624884509,
      "loss": 0.1792,
      "step": 22490
    },
    {
      "epoch": 1.3858946720049277,
      "grad_norm": 0.15812569856643677,
      "learning_rate": 0.0001076398727030079,
      "loss": 0.1783,
      "step": 22500
    },
    {
      "epoch": 1.3865106251924852,
      "grad_norm": 0.14921976625919342,
      "learning_rate": 0.00010759880915717073,
      "loss": 0.1768,
      "step": 22510
    },
    {
      "epoch": 1.387126578380043,
      "grad_norm": 0.506806492805481,
      "learning_rate": 0.00010755774561133354,
      "loss": 0.179,
      "step": 22520
    },
    {
      "epoch": 1.3877425315676009,
      "grad_norm": 0.1630716323852539,
      "learning_rate": 0.00010751668206549636,
      "loss": 0.1785,
      "step": 22530
    },
    {
      "epoch": 1.3883584847551587,
      "grad_norm": 0.18283997476100922,
      "learning_rate": 0.00010747561851965917,
      "loss": 0.1748,
      "step": 22540
    },
    {
      "epoch": 1.3889744379427165,
      "grad_norm": 0.2863897383213043,
      "learning_rate": 0.000107434554973822,
      "loss": 0.1791,
      "step": 22550
    },
    {
      "epoch": 1.389590391130274,
      "grad_norm": 0.16153812408447266,
      "learning_rate": 0.0001073934914279848,
      "loss": 0.1799,
      "step": 22560
    },
    {
      "epoch": 1.3902063443178319,
      "grad_norm": 0.15686044096946716,
      "learning_rate": 0.00010735242788214763,
      "loss": 0.1764,
      "step": 22570
    },
    {
      "epoch": 1.3908222975053897,
      "grad_norm": 0.14665354788303375,
      "learning_rate": 0.00010731136433631043,
      "loss": 0.1776,
      "step": 22580
    },
    {
      "epoch": 1.3914382506929472,
      "grad_norm": 0.1529930979013443,
      "learning_rate": 0.00010727030079047327,
      "loss": 0.1802,
      "step": 22590
    },
    {
      "epoch": 1.392054203880505,
      "grad_norm": 0.14652112126350403,
      "learning_rate": 0.00010722923724463607,
      "loss": 0.1784,
      "step": 22600
    },
    {
      "epoch": 1.3926701570680629,
      "grad_norm": 0.21620775759220123,
      "learning_rate": 0.0001071881736987989,
      "loss": 0.1788,
      "step": 22610
    },
    {
      "epoch": 1.3932861102556204,
      "grad_norm": 0.16652101278305054,
      "learning_rate": 0.0001071471101529617,
      "loss": 0.1787,
      "step": 22620
    },
    {
      "epoch": 1.3939020634431782,
      "grad_norm": 0.15928755700588226,
      "learning_rate": 0.00010710604660712453,
      "loss": 0.1766,
      "step": 22630
    },
    {
      "epoch": 1.394518016630736,
      "grad_norm": 0.1652258038520813,
      "learning_rate": 0.00010706498306128734,
      "loss": 0.1767,
      "step": 22640
    },
    {
      "epoch": 1.3951339698182939,
      "grad_norm": 0.1891489028930664,
      "learning_rate": 0.00010702391951545017,
      "loss": 0.1794,
      "step": 22650
    },
    {
      "epoch": 1.3957499230058517,
      "grad_norm": 0.17924928665161133,
      "learning_rate": 0.00010698285596961297,
      "loss": 0.1803,
      "step": 22660
    },
    {
      "epoch": 1.3963658761934092,
      "grad_norm": 0.13505274057388306,
      "learning_rate": 0.0001069417924237758,
      "loss": 0.1787,
      "step": 22670
    },
    {
      "epoch": 1.396981829380967,
      "grad_norm": 0.15339836478233337,
      "learning_rate": 0.0001069007288779386,
      "loss": 0.1763,
      "step": 22680
    },
    {
      "epoch": 1.3975977825685248,
      "grad_norm": 0.17809943854808807,
      "learning_rate": 0.00010685966533210144,
      "loss": 0.1784,
      "step": 22690
    },
    {
      "epoch": 1.3982137357560824,
      "grad_norm": 0.16617870330810547,
      "learning_rate": 0.00010681860178626423,
      "loss": 0.1775,
      "step": 22700
    },
    {
      "epoch": 1.3988296889436402,
      "grad_norm": 0.21206510066986084,
      "learning_rate": 0.00010677753824042707,
      "loss": 0.1748,
      "step": 22710
    },
    {
      "epoch": 1.399445642131198,
      "grad_norm": 0.177824005484581,
      "learning_rate": 0.00010673647469458987,
      "loss": 0.1783,
      "step": 22720
    },
    {
      "epoch": 1.4000615953187558,
      "grad_norm": 0.1731416881084442,
      "learning_rate": 0.0001066954111487527,
      "loss": 0.1789,
      "step": 22730
    },
    {
      "epoch": 1.4006775485063137,
      "grad_norm": 0.17636288702487946,
      "learning_rate": 0.00010665434760291553,
      "loss": 0.1796,
      "step": 22740
    },
    {
      "epoch": 1.4012935016938712,
      "grad_norm": 0.1609947830438614,
      "learning_rate": 0.00010661328405707833,
      "loss": 0.1791,
      "step": 22750
    },
    {
      "epoch": 1.401909454881429,
      "grad_norm": 0.1373288631439209,
      "learning_rate": 0.00010657222051124117,
      "loss": 0.1798,
      "step": 22760
    },
    {
      "epoch": 1.4025254080689868,
      "grad_norm": 0.18246833980083466,
      "learning_rate": 0.00010653115696540397,
      "loss": 0.1765,
      "step": 22770
    },
    {
      "epoch": 1.4031413612565444,
      "grad_norm": 0.16487805545330048,
      "learning_rate": 0.00010649009341956679,
      "loss": 0.1774,
      "step": 22780
    },
    {
      "epoch": 1.4037573144441022,
      "grad_norm": 0.2412330061197281,
      "learning_rate": 0.0001064490298737296,
      "loss": 0.1758,
      "step": 22790
    },
    {
      "epoch": 1.40437326763166,
      "grad_norm": 0.158582404255867,
      "learning_rate": 0.00010640796632789243,
      "loss": 0.1804,
      "step": 22800
    },
    {
      "epoch": 1.4049892208192176,
      "grad_norm": 0.1582622230052948,
      "learning_rate": 0.00010636690278205524,
      "loss": 0.176,
      "step": 22810
    },
    {
      "epoch": 1.4056051740067754,
      "grad_norm": 0.19386158883571625,
      "learning_rate": 0.00010632583923621806,
      "loss": 0.1808,
      "step": 22820
    },
    {
      "epoch": 1.4062211271943332,
      "grad_norm": 0.16697104275226593,
      "learning_rate": 0.00010628477569038087,
      "loss": 0.181,
      "step": 22830
    },
    {
      "epoch": 1.406837080381891,
      "grad_norm": 0.17467398941516876,
      "learning_rate": 0.0001062437121445437,
      "loss": 0.1779,
      "step": 22840
    },
    {
      "epoch": 1.4074530335694488,
      "grad_norm": 0.19770002365112305,
      "learning_rate": 0.0001062026485987065,
      "loss": 0.1801,
      "step": 22850
    },
    {
      "epoch": 1.4080689867570064,
      "grad_norm": 0.14753872156143188,
      "learning_rate": 0.00010616158505286933,
      "loss": 0.1799,
      "step": 22860
    },
    {
      "epoch": 1.4086849399445642,
      "grad_norm": 0.17278288304805756,
      "learning_rate": 0.00010612052150703213,
      "loss": 0.1795,
      "step": 22870
    },
    {
      "epoch": 1.409300893132122,
      "grad_norm": 0.14676788449287415,
      "learning_rate": 0.00010607945796119497,
      "loss": 0.1781,
      "step": 22880
    },
    {
      "epoch": 1.4099168463196796,
      "grad_norm": 0.17446164786815643,
      "learning_rate": 0.00010603839441535777,
      "loss": 0.1773,
      "step": 22890
    },
    {
      "epoch": 1.4105327995072374,
      "grad_norm": 0.2003561407327652,
      "learning_rate": 0.00010599733086952059,
      "loss": 0.1793,
      "step": 22900
    },
    {
      "epoch": 1.4111487526947952,
      "grad_norm": 0.16810409724712372,
      "learning_rate": 0.0001059562673236834,
      "loss": 0.1784,
      "step": 22910
    },
    {
      "epoch": 1.4117647058823528,
      "grad_norm": 0.18313464522361755,
      "learning_rate": 0.00010591520377784623,
      "loss": 0.1772,
      "step": 22920
    },
    {
      "epoch": 1.4123806590699106,
      "grad_norm": 0.1866903305053711,
      "learning_rate": 0.00010587414023200904,
      "loss": 0.1779,
      "step": 22930
    },
    {
      "epoch": 1.4129966122574684,
      "grad_norm": 0.166920006275177,
      "learning_rate": 0.00010583307668617186,
      "loss": 0.1771,
      "step": 22940
    },
    {
      "epoch": 1.4136125654450262,
      "grad_norm": 0.22607137262821198,
      "learning_rate": 0.00010579201314033467,
      "loss": 0.179,
      "step": 22950
    },
    {
      "epoch": 1.414228518632584,
      "grad_norm": 0.1947525143623352,
      "learning_rate": 0.0001057509495944975,
      "loss": 0.177,
      "step": 22960
    },
    {
      "epoch": 1.4148444718201416,
      "grad_norm": 0.16866551339626312,
      "learning_rate": 0.0001057098860486603,
      "loss": 0.1773,
      "step": 22970
    },
    {
      "epoch": 1.4154604250076994,
      "grad_norm": 0.1593647450208664,
      "learning_rate": 0.00010566882250282313,
      "loss": 0.1784,
      "step": 22980
    },
    {
      "epoch": 1.4160763781952572,
      "grad_norm": 0.17145012319087982,
      "learning_rate": 0.00010562775895698593,
      "loss": 0.1763,
      "step": 22990
    },
    {
      "epoch": 1.4166923313828148,
      "grad_norm": 0.14334742724895477,
      "learning_rate": 0.00010558669541114877,
      "loss": 0.1805,
      "step": 23000
    },
    {
      "epoch": 1.4173082845703726,
      "grad_norm": 0.1588553637266159,
      "learning_rate": 0.00010554563186531157,
      "loss": 0.1773,
      "step": 23010
    },
    {
      "epoch": 1.4179242377579304,
      "grad_norm": 0.14789684116840363,
      "learning_rate": 0.00010550456831947439,
      "loss": 0.1789,
      "step": 23020
    },
    {
      "epoch": 1.4185401909454882,
      "grad_norm": 0.1632310301065445,
      "learning_rate": 0.0001054635047736372,
      "loss": 0.1802,
      "step": 23030
    },
    {
      "epoch": 1.419156144133046,
      "grad_norm": 0.14302405714988708,
      "learning_rate": 0.00010542244122780003,
      "loss": 0.179,
      "step": 23040
    },
    {
      "epoch": 1.4197720973206036,
      "grad_norm": 0.1465931236743927,
      "learning_rate": 0.00010538137768196284,
      "loss": 0.1793,
      "step": 23050
    },
    {
      "epoch": 1.4203880505081614,
      "grad_norm": 0.1670164316892624,
      "learning_rate": 0.00010534031413612566,
      "loss": 0.1789,
      "step": 23060
    },
    {
      "epoch": 1.4210040036957192,
      "grad_norm": 0.16096949577331543,
      "learning_rate": 0.00010529925059028847,
      "loss": 0.1769,
      "step": 23070
    },
    {
      "epoch": 1.4216199568832768,
      "grad_norm": 0.16940711438655853,
      "learning_rate": 0.0001052581870444513,
      "loss": 0.175,
      "step": 23080
    },
    {
      "epoch": 1.4222359100708346,
      "grad_norm": 0.1690824031829834,
      "learning_rate": 0.0001052171234986141,
      "loss": 0.1778,
      "step": 23090
    },
    {
      "epoch": 1.4228518632583924,
      "grad_norm": 0.14183694124221802,
      "learning_rate": 0.00010517605995277693,
      "loss": 0.1773,
      "step": 23100
    },
    {
      "epoch": 1.42346781644595,
      "grad_norm": 0.188107430934906,
      "learning_rate": 0.00010513499640693973,
      "loss": 0.1771,
      "step": 23110
    },
    {
      "epoch": 1.4240837696335078,
      "grad_norm": 0.16495893895626068,
      "learning_rate": 0.00010509393286110257,
      "loss": 0.1785,
      "step": 23120
    },
    {
      "epoch": 1.4246997228210656,
      "grad_norm": 0.16445358097553253,
      "learning_rate": 0.00010505286931526537,
      "loss": 0.1769,
      "step": 23130
    },
    {
      "epoch": 1.4253156760086234,
      "grad_norm": 0.13685500621795654,
      "learning_rate": 0.00010501180576942819,
      "loss": 0.1785,
      "step": 23140
    },
    {
      "epoch": 1.4259316291961812,
      "grad_norm": 0.1660972237586975,
      "learning_rate": 0.00010497074222359103,
      "loss": 0.1766,
      "step": 23150
    },
    {
      "epoch": 1.4265475823837388,
      "grad_norm": 0.17757190763950348,
      "learning_rate": 0.00010492967867775383,
      "loss": 0.1784,
      "step": 23160
    },
    {
      "epoch": 1.4271635355712966,
      "grad_norm": 0.15351086854934692,
      "learning_rate": 0.00010488861513191665,
      "loss": 0.1769,
      "step": 23170
    },
    {
      "epoch": 1.4277794887588544,
      "grad_norm": 0.1710829734802246,
      "learning_rate": 0.00010484755158607946,
      "loss": 0.1755,
      "step": 23180
    },
    {
      "epoch": 1.428395441946412,
      "grad_norm": 0.1473894715309143,
      "learning_rate": 0.00010480648804024229,
      "loss": 0.1789,
      "step": 23190
    },
    {
      "epoch": 1.4290113951339698,
      "grad_norm": 0.14485834538936615,
      "learning_rate": 0.0001047654244944051,
      "loss": 0.1793,
      "step": 23200
    },
    {
      "epoch": 1.4296273483215276,
      "grad_norm": 0.16384552419185638,
      "learning_rate": 0.00010472436094856792,
      "loss": 0.1788,
      "step": 23210
    },
    {
      "epoch": 1.4302433015090852,
      "grad_norm": 0.14542606472969055,
      "learning_rate": 0.00010468329740273073,
      "loss": 0.1777,
      "step": 23220
    },
    {
      "epoch": 1.430859254696643,
      "grad_norm": 0.17009922862052917,
      "learning_rate": 0.00010464223385689356,
      "loss": 0.1785,
      "step": 23230
    },
    {
      "epoch": 1.4314752078842008,
      "grad_norm": 0.14330171048641205,
      "learning_rate": 0.00010460117031105637,
      "loss": 0.1787,
      "step": 23240
    },
    {
      "epoch": 1.4320911610717586,
      "grad_norm": 0.13863250613212585,
      "learning_rate": 0.00010456010676521919,
      "loss": 0.1747,
      "step": 23250
    },
    {
      "epoch": 1.4327071142593164,
      "grad_norm": 0.17488263547420502,
      "learning_rate": 0.00010451904321938199,
      "loss": 0.1792,
      "step": 23260
    },
    {
      "epoch": 1.433323067446874,
      "grad_norm": 0.17898711562156677,
      "learning_rate": 0.00010447797967354483,
      "loss": 0.1785,
      "step": 23270
    },
    {
      "epoch": 1.4339390206344318,
      "grad_norm": 0.1420460045337677,
      "learning_rate": 0.00010443691612770763,
      "loss": 0.178,
      "step": 23280
    },
    {
      "epoch": 1.4345549738219896,
      "grad_norm": 0.1946326196193695,
      "learning_rate": 0.00010439585258187045,
      "loss": 0.1794,
      "step": 23290
    },
    {
      "epoch": 1.4351709270095472,
      "grad_norm": 0.1325952559709549,
      "learning_rate": 0.00010435478903603326,
      "loss": 0.1773,
      "step": 23300
    },
    {
      "epoch": 1.435786880197105,
      "grad_norm": 0.15689188241958618,
      "learning_rate": 0.00010431372549019609,
      "loss": 0.1788,
      "step": 23310
    },
    {
      "epoch": 1.4364028333846628,
      "grad_norm": 0.1335296630859375,
      "learning_rate": 0.0001042726619443589,
      "loss": 0.1766,
      "step": 23320
    },
    {
      "epoch": 1.4370187865722204,
      "grad_norm": 0.5271214842796326,
      "learning_rate": 0.00010423159839852172,
      "loss": 0.1759,
      "step": 23330
    },
    {
      "epoch": 1.4376347397597782,
      "grad_norm": 0.16436924040317535,
      "learning_rate": 0.00010419053485268452,
      "loss": 0.178,
      "step": 23340
    },
    {
      "epoch": 1.438250692947336,
      "grad_norm": 0.24842657148838043,
      "learning_rate": 0.00010414947130684736,
      "loss": 0.1794,
      "step": 23350
    },
    {
      "epoch": 1.4388666461348938,
      "grad_norm": 0.13432110846042633,
      "learning_rate": 0.00010410840776101016,
      "loss": 0.1789,
      "step": 23360
    },
    {
      "epoch": 1.4394825993224516,
      "grad_norm": 0.16949059069156647,
      "learning_rate": 0.00010406734421517299,
      "loss": 0.1795,
      "step": 23370
    },
    {
      "epoch": 1.4400985525100092,
      "grad_norm": 0.21555843949317932,
      "learning_rate": 0.00010402628066933579,
      "loss": 0.1785,
      "step": 23380
    },
    {
      "epoch": 1.440714505697567,
      "grad_norm": 0.16180221736431122,
      "learning_rate": 0.00010398521712349863,
      "loss": 0.1781,
      "step": 23390
    },
    {
      "epoch": 1.4413304588851248,
      "grad_norm": 0.14342887699604034,
      "learning_rate": 0.00010394415357766143,
      "loss": 0.1787,
      "step": 23400
    },
    {
      "epoch": 1.4419464120726824,
      "grad_norm": 0.15060953795909882,
      "learning_rate": 0.00010390309003182425,
      "loss": 0.1758,
      "step": 23410
    },
    {
      "epoch": 1.4425623652602402,
      "grad_norm": 0.15136975049972534,
      "learning_rate": 0.00010386202648598706,
      "loss": 0.1799,
      "step": 23420
    },
    {
      "epoch": 1.443178318447798,
      "grad_norm": 0.14722877740859985,
      "learning_rate": 0.00010382096294014989,
      "loss": 0.1781,
      "step": 23430
    },
    {
      "epoch": 1.4437942716353558,
      "grad_norm": 0.16059525310993195,
      "learning_rate": 0.0001037798993943127,
      "loss": 0.1771,
      "step": 23440
    },
    {
      "epoch": 1.4444102248229136,
      "grad_norm": 0.16072313487529755,
      "learning_rate": 0.00010373883584847552,
      "loss": 0.1793,
      "step": 23450
    },
    {
      "epoch": 1.4450261780104712,
      "grad_norm": 0.1670580506324768,
      "learning_rate": 0.00010369777230263832,
      "loss": 0.1773,
      "step": 23460
    },
    {
      "epoch": 1.445642131198029,
      "grad_norm": 0.14649690687656403,
      "learning_rate": 0.00010365670875680116,
      "loss": 0.1762,
      "step": 23470
    },
    {
      "epoch": 1.4462580843855868,
      "grad_norm": 0.16345202922821045,
      "learning_rate": 0.00010361564521096396,
      "loss": 0.179,
      "step": 23480
    },
    {
      "epoch": 1.4468740375731444,
      "grad_norm": 0.15029187500476837,
      "learning_rate": 0.00010357458166512678,
      "loss": 0.18,
      "step": 23490
    },
    {
      "epoch": 1.4474899907607022,
      "grad_norm": 0.1413906216621399,
      "learning_rate": 0.00010353351811928959,
      "loss": 0.1775,
      "step": 23500
    },
    {
      "epoch": 1.44810594394826,
      "grad_norm": 0.13708390295505524,
      "learning_rate": 0.00010349245457345242,
      "loss": 0.178,
      "step": 23510
    },
    {
      "epoch": 1.4487218971358176,
      "grad_norm": 0.15186429023742676,
      "learning_rate": 0.00010345139102761523,
      "loss": 0.1778,
      "step": 23520
    },
    {
      "epoch": 1.4493378503233754,
      "grad_norm": 0.19274237751960754,
      "learning_rate": 0.00010341032748177805,
      "loss": 0.1756,
      "step": 23530
    },
    {
      "epoch": 1.4499538035109332,
      "grad_norm": 0.14975093305110931,
      "learning_rate": 0.00010336926393594086,
      "loss": 0.176,
      "step": 23540
    },
    {
      "epoch": 1.450569756698491,
      "grad_norm": 0.15557046234607697,
      "learning_rate": 0.00010332820039010369,
      "loss": 0.1789,
      "step": 23550
    },
    {
      "epoch": 1.4511857098860488,
      "grad_norm": 0.15371109545230865,
      "learning_rate": 0.0001032871368442665,
      "loss": 0.1796,
      "step": 23560
    },
    {
      "epoch": 1.4518016630736064,
      "grad_norm": 0.19962282478809357,
      "learning_rate": 0.00010324607329842932,
      "loss": 0.1784,
      "step": 23570
    },
    {
      "epoch": 1.4524176162611642,
      "grad_norm": 0.1537363976240158,
      "learning_rate": 0.00010320500975259215,
      "loss": 0.1767,
      "step": 23580
    },
    {
      "epoch": 1.453033569448722,
      "grad_norm": 0.17921730875968933,
      "learning_rate": 0.00010316394620675496,
      "loss": 0.1794,
      "step": 23590
    },
    {
      "epoch": 1.4536495226362796,
      "grad_norm": 0.16840527951717377,
      "learning_rate": 0.00010312288266091779,
      "loss": 0.1784,
      "step": 23600
    },
    {
      "epoch": 1.4542654758238374,
      "grad_norm": 0.15754401683807373,
      "learning_rate": 0.00010308181911508058,
      "loss": 0.1791,
      "step": 23610
    },
    {
      "epoch": 1.4548814290113952,
      "grad_norm": 0.19007514417171478,
      "learning_rate": 0.00010304075556924342,
      "loss": 0.1803,
      "step": 23620
    },
    {
      "epoch": 1.4554973821989527,
      "grad_norm": 0.184669628739357,
      "learning_rate": 0.00010299969202340622,
      "loss": 0.1774,
      "step": 23630
    },
    {
      "epoch": 1.4561133353865106,
      "grad_norm": 0.16027934849262238,
      "learning_rate": 0.00010295862847756906,
      "loss": 0.1769,
      "step": 23640
    },
    {
      "epoch": 1.4567292885740684,
      "grad_norm": 0.173012837767601,
      "learning_rate": 0.00010291756493173186,
      "loss": 0.1798,
      "step": 23650
    },
    {
      "epoch": 1.4573452417616262,
      "grad_norm": 0.20780067145824432,
      "learning_rate": 0.00010287650138589468,
      "loss": 0.1759,
      "step": 23660
    },
    {
      "epoch": 1.457961194949184,
      "grad_norm": 0.15483002364635468,
      "learning_rate": 0.00010283543784005749,
      "loss": 0.1771,
      "step": 23670
    },
    {
      "epoch": 1.4585771481367416,
      "grad_norm": 0.1790897399187088,
      "learning_rate": 0.00010279437429422032,
      "loss": 0.1786,
      "step": 23680
    },
    {
      "epoch": 1.4591931013242994,
      "grad_norm": 0.1540604680776596,
      "learning_rate": 0.00010275331074838313,
      "loss": 0.1787,
      "step": 23690
    },
    {
      "epoch": 1.4598090545118572,
      "grad_norm": 0.16520702838897705,
      "learning_rate": 0.00010271224720254595,
      "loss": 0.1797,
      "step": 23700
    },
    {
      "epoch": 1.4604250076994147,
      "grad_norm": 0.1665545552968979,
      "learning_rate": 0.00010267118365670876,
      "loss": 0.1784,
      "step": 23710
    },
    {
      "epoch": 1.4610409608869726,
      "grad_norm": 0.1729041188955307,
      "learning_rate": 0.00010263012011087159,
      "loss": 0.1767,
      "step": 23720
    },
    {
      "epoch": 1.4616569140745304,
      "grad_norm": 0.1639174371957779,
      "learning_rate": 0.0001025890565650344,
      "loss": 0.1776,
      "step": 23730
    },
    {
      "epoch": 1.4622728672620882,
      "grad_norm": 0.18284329771995544,
      "learning_rate": 0.00010254799301919722,
      "loss": 0.1783,
      "step": 23740
    },
    {
      "epoch": 1.4628888204496457,
      "grad_norm": 0.1297513097524643,
      "learning_rate": 0.00010250692947336002,
      "loss": 0.1768,
      "step": 23750
    },
    {
      "epoch": 1.4635047736372035,
      "grad_norm": 0.192364901304245,
      "learning_rate": 0.00010246586592752286,
      "loss": 0.1782,
      "step": 23760
    },
    {
      "epoch": 1.4641207268247614,
      "grad_norm": 0.15296456217765808,
      "learning_rate": 0.00010242480238168566,
      "loss": 0.1766,
      "step": 23770
    },
    {
      "epoch": 1.4647366800123192,
      "grad_norm": 0.18787570297718048,
      "learning_rate": 0.00010238373883584848,
      "loss": 0.1775,
      "step": 23780
    },
    {
      "epoch": 1.4653526331998767,
      "grad_norm": 0.20113852620124817,
      "learning_rate": 0.00010234267529001129,
      "loss": 0.1792,
      "step": 23790
    },
    {
      "epoch": 1.4659685863874345,
      "grad_norm": 0.16939452290534973,
      "learning_rate": 0.00010230161174417412,
      "loss": 0.1782,
      "step": 23800
    },
    {
      "epoch": 1.4665845395749924,
      "grad_norm": 0.22809000313282013,
      "learning_rate": 0.00010226054819833693,
      "loss": 0.1772,
      "step": 23810
    },
    {
      "epoch": 1.46720049276255,
      "grad_norm": 0.14753562211990356,
      "learning_rate": 0.00010221948465249975,
      "loss": 0.1775,
      "step": 23820
    },
    {
      "epoch": 1.4678164459501077,
      "grad_norm": 0.16947227716445923,
      "learning_rate": 0.00010217842110666256,
      "loss": 0.1772,
      "step": 23830
    },
    {
      "epoch": 1.4684323991376655,
      "grad_norm": 0.13873930275440216,
      "learning_rate": 0.00010213735756082539,
      "loss": 0.177,
      "step": 23840
    },
    {
      "epoch": 1.4690483523252233,
      "grad_norm": 0.15339377522468567,
      "learning_rate": 0.0001020962940149882,
      "loss": 0.1778,
      "step": 23850
    },
    {
      "epoch": 1.4696643055127812,
      "grad_norm": 0.14788034558296204,
      "learning_rate": 0.00010205523046915102,
      "loss": 0.1789,
      "step": 23860
    },
    {
      "epoch": 1.4702802587003387,
      "grad_norm": 0.1769704818725586,
      "learning_rate": 0.00010201416692331382,
      "loss": 0.1764,
      "step": 23870
    },
    {
      "epoch": 1.4708962118878965,
      "grad_norm": 0.23175247013568878,
      "learning_rate": 0.00010197310337747666,
      "loss": 0.1794,
      "step": 23880
    },
    {
      "epoch": 1.4715121650754543,
      "grad_norm": 0.15188182890415192,
      "learning_rate": 0.00010193203983163946,
      "loss": 0.1778,
      "step": 23890
    },
    {
      "epoch": 1.472128118263012,
      "grad_norm": 0.1502721905708313,
      "learning_rate": 0.00010189097628580228,
      "loss": 0.1763,
      "step": 23900
    },
    {
      "epoch": 1.4727440714505697,
      "grad_norm": 0.14355163276195526,
      "learning_rate": 0.00010184991273996509,
      "loss": 0.1768,
      "step": 23910
    },
    {
      "epoch": 1.4733600246381275,
      "grad_norm": 0.17124590277671814,
      "learning_rate": 0.00010180884919412792,
      "loss": 0.1753,
      "step": 23920
    },
    {
      "epoch": 1.4739759778256851,
      "grad_norm": 0.15164035558700562,
      "learning_rate": 0.00010176778564829073,
      "loss": 0.178,
      "step": 23930
    },
    {
      "epoch": 1.474591931013243,
      "grad_norm": 0.15551544725894928,
      "learning_rate": 0.00010172672210245355,
      "loss": 0.1784,
      "step": 23940
    },
    {
      "epoch": 1.4752078842008007,
      "grad_norm": 0.19030147790908813,
      "learning_rate": 0.00010168565855661636,
      "loss": 0.1769,
      "step": 23950
    },
    {
      "epoch": 1.4758238373883585,
      "grad_norm": 0.17215190827846527,
      "learning_rate": 0.00010164459501077919,
      "loss": 0.1799,
      "step": 23960
    },
    {
      "epoch": 1.4764397905759163,
      "grad_norm": 0.14797866344451904,
      "learning_rate": 0.000101603531464942,
      "loss": 0.1795,
      "step": 23970
    },
    {
      "epoch": 1.477055743763474,
      "grad_norm": 0.16398124396800995,
      "learning_rate": 0.00010156246791910482,
      "loss": 0.1763,
      "step": 23980
    },
    {
      "epoch": 1.4776716969510317,
      "grad_norm": 0.18134944140911102,
      "learning_rate": 0.00010152140437326762,
      "loss": 0.1774,
      "step": 23990
    },
    {
      "epoch": 1.4782876501385895,
      "grad_norm": 0.1646926999092102,
      "learning_rate": 0.00010148034082743046,
      "loss": 0.1791,
      "step": 24000
    },
    {
      "epoch": 1.4789036033261471,
      "grad_norm": 0.1381075382232666,
      "learning_rate": 0.00010143927728159328,
      "loss": 0.1782,
      "step": 24010
    },
    {
      "epoch": 1.479519556513705,
      "grad_norm": 0.1964319944381714,
      "learning_rate": 0.00010139821373575608,
      "loss": 0.1792,
      "step": 24020
    },
    {
      "epoch": 1.4801355097012627,
      "grad_norm": 0.20091193914413452,
      "learning_rate": 0.00010135715018991892,
      "loss": 0.1791,
      "step": 24030
    },
    {
      "epoch": 1.4807514628888203,
      "grad_norm": 0.15959830582141876,
      "learning_rate": 0.00010131608664408172,
      "loss": 0.177,
      "step": 24040
    },
    {
      "epoch": 1.4813674160763781,
      "grad_norm": 0.1611592024564743,
      "learning_rate": 0.00010127502309824454,
      "loss": 0.179,
      "step": 24050
    },
    {
      "epoch": 1.481983369263936,
      "grad_norm": 0.1526443511247635,
      "learning_rate": 0.00010123395955240735,
      "loss": 0.1769,
      "step": 24060
    },
    {
      "epoch": 1.4825993224514937,
      "grad_norm": 0.16148264706134796,
      "learning_rate": 0.00010119289600657018,
      "loss": 0.1761,
      "step": 24070
    },
    {
      "epoch": 1.4832152756390515,
      "grad_norm": 0.18523958325386047,
      "learning_rate": 0.00010115183246073299,
      "loss": 0.1771,
      "step": 24080
    },
    {
      "epoch": 1.4838312288266091,
      "grad_norm": 0.17745119333267212,
      "learning_rate": 0.00010111076891489581,
      "loss": 0.1766,
      "step": 24090
    },
    {
      "epoch": 1.484447182014167,
      "grad_norm": 0.1485222429037094,
      "learning_rate": 0.00010106970536905862,
      "loss": 0.1773,
      "step": 24100
    },
    {
      "epoch": 1.4850631352017247,
      "grad_norm": 0.1793123185634613,
      "learning_rate": 0.00010102864182322145,
      "loss": 0.1775,
      "step": 24110
    },
    {
      "epoch": 1.4856790883892823,
      "grad_norm": 0.16382956504821777,
      "learning_rate": 0.00010098757827738426,
      "loss": 0.1774,
      "step": 24120
    },
    {
      "epoch": 1.4862950415768401,
      "grad_norm": 0.15655165910720825,
      "learning_rate": 0.00010094651473154708,
      "loss": 0.1791,
      "step": 24130
    },
    {
      "epoch": 1.486910994764398,
      "grad_norm": 0.16638070344924927,
      "learning_rate": 0.00010090545118570988,
      "loss": 0.1774,
      "step": 24140
    },
    {
      "epoch": 1.4875269479519557,
      "grad_norm": 0.15229514241218567,
      "learning_rate": 0.00010086438763987272,
      "loss": 0.1793,
      "step": 24150
    },
    {
      "epoch": 1.4881429011395135,
      "grad_norm": 0.15962481498718262,
      "learning_rate": 0.00010082332409403552,
      "loss": 0.1764,
      "step": 24160
    },
    {
      "epoch": 1.4887588543270711,
      "grad_norm": 0.14000971615314484,
      "learning_rate": 0.00010078226054819834,
      "loss": 0.1789,
      "step": 24170
    },
    {
      "epoch": 1.489374807514629,
      "grad_norm": 0.1496448665857315,
      "learning_rate": 0.00010074119700236115,
      "loss": 0.1761,
      "step": 24180
    },
    {
      "epoch": 1.4899907607021867,
      "grad_norm": 0.1701885461807251,
      "learning_rate": 0.00010070013345652398,
      "loss": 0.1791,
      "step": 24190
    },
    {
      "epoch": 1.4906067138897443,
      "grad_norm": 0.1479300558567047,
      "learning_rate": 0.00010065906991068679,
      "loss": 0.1773,
      "step": 24200
    },
    {
      "epoch": 1.491222667077302,
      "grad_norm": 0.16163745522499084,
      "learning_rate": 0.00010061800636484961,
      "loss": 0.1791,
      "step": 24210
    },
    {
      "epoch": 1.49183862026486,
      "grad_norm": 0.1467011570930481,
      "learning_rate": 0.00010057694281901242,
      "loss": 0.1764,
      "step": 24220
    },
    {
      "epoch": 1.4924545734524175,
      "grad_norm": 0.37344372272491455,
      "learning_rate": 0.00010053587927317525,
      "loss": 0.1795,
      "step": 24230
    },
    {
      "epoch": 1.4930705266399753,
      "grad_norm": 0.16016608476638794,
      "learning_rate": 0.00010049481572733806,
      "loss": 0.1793,
      "step": 24240
    },
    {
      "epoch": 1.493686479827533,
      "grad_norm": 0.1688172072172165,
      "learning_rate": 0.00010045375218150088,
      "loss": 0.1787,
      "step": 24250
    },
    {
      "epoch": 1.494302433015091,
      "grad_norm": 0.1693359911441803,
      "learning_rate": 0.00010041268863566368,
      "loss": 0.1774,
      "step": 24260
    },
    {
      "epoch": 1.4949183862026487,
      "grad_norm": 0.1517283320426941,
      "learning_rate": 0.00010037162508982652,
      "loss": 0.1761,
      "step": 24270
    },
    {
      "epoch": 1.4955343393902063,
      "grad_norm": 0.15337543189525604,
      "learning_rate": 0.00010033056154398932,
      "loss": 0.179,
      "step": 24280
    },
    {
      "epoch": 1.496150292577764,
      "grad_norm": 0.16036130487918854,
      "learning_rate": 0.00010028949799815214,
      "loss": 0.1769,
      "step": 24290
    },
    {
      "epoch": 1.496766245765322,
      "grad_norm": 0.16241540014743805,
      "learning_rate": 0.00010024843445231495,
      "loss": 0.178,
      "step": 24300
    },
    {
      "epoch": 1.4973821989528795,
      "grad_norm": 0.14678233861923218,
      "learning_rate": 0.00010020737090647778,
      "loss": 0.179,
      "step": 24310
    },
    {
      "epoch": 1.4979981521404373,
      "grad_norm": 0.1633002609014511,
      "learning_rate": 0.00010016630736064059,
      "loss": 0.176,
      "step": 24320
    },
    {
      "epoch": 1.498614105327995,
      "grad_norm": 0.17196236550807953,
      "learning_rate": 0.00010012524381480341,
      "loss": 0.1773,
      "step": 24330
    },
    {
      "epoch": 1.4992300585155527,
      "grad_norm": 0.20405814051628113,
      "learning_rate": 0.00010008418026896622,
      "loss": 0.1777,
      "step": 24340
    },
    {
      "epoch": 1.4998460117031105,
      "grad_norm": 0.1580716222524643,
      "learning_rate": 0.00010004311672312905,
      "loss": 0.1773,
      "step": 24350
    },
    {
      "epoch": 1.5004619648906683,
      "grad_norm": 0.23300890624523163,
      "learning_rate": 0.00010000205317729186,
      "loss": 0.1764,
      "step": 24360
    },
    {
      "epoch": 1.501077918078226,
      "grad_norm": 0.16933032870292664,
      "learning_rate": 9.996098963145468e-05,
      "loss": 0.1787,
      "step": 24370
    },
    {
      "epoch": 1.501693871265784,
      "grad_norm": 0.14620625972747803,
      "learning_rate": 9.99199260856175e-05,
      "loss": 0.1783,
      "step": 24380
    },
    {
      "epoch": 1.5023098244533415,
      "grad_norm": 0.15462273359298706,
      "learning_rate": 9.987886253978032e-05,
      "loss": 0.1773,
      "step": 24390
    },
    {
      "epoch": 1.5029257776408993,
      "grad_norm": 0.15265116095542908,
      "learning_rate": 9.983779899394314e-05,
      "loss": 0.178,
      "step": 24400
    },
    {
      "epoch": 1.503541730828457,
      "grad_norm": 0.18640638887882233,
      "learning_rate": 9.979673544810594e-05,
      "loss": 0.179,
      "step": 24410
    },
    {
      "epoch": 1.5041576840160147,
      "grad_norm": 0.15753765404224396,
      "learning_rate": 9.975567190226876e-05,
      "loss": 0.177,
      "step": 24420
    },
    {
      "epoch": 1.5047736372035725,
      "grad_norm": 0.15161670744419098,
      "learning_rate": 9.971460835643158e-05,
      "loss": 0.1764,
      "step": 24430
    },
    {
      "epoch": 1.5053895903911303,
      "grad_norm": 0.14538811147212982,
      "learning_rate": 9.96735448105944e-05,
      "loss": 0.1775,
      "step": 24440
    },
    {
      "epoch": 1.5060055435786879,
      "grad_norm": 0.14240843057632446,
      "learning_rate": 9.963248126475721e-05,
      "loss": 0.1766,
      "step": 24450
    },
    {
      "epoch": 1.506621496766246,
      "grad_norm": 0.15782693028450012,
      "learning_rate": 9.959141771892003e-05,
      "loss": 0.178,
      "step": 24460
    },
    {
      "epoch": 1.5072374499538035,
      "grad_norm": 0.18097934126853943,
      "learning_rate": 9.955035417308285e-05,
      "loss": 0.1772,
      "step": 24470
    },
    {
      "epoch": 1.5078534031413613,
      "grad_norm": 0.16658365726470947,
      "learning_rate": 9.950929062724567e-05,
      "loss": 0.1772,
      "step": 24480
    },
    {
      "epoch": 1.508469356328919,
      "grad_norm": 0.1477389633655548,
      "learning_rate": 9.946822708140848e-05,
      "loss": 0.1774,
      "step": 24490
    },
    {
      "epoch": 1.5090853095164767,
      "grad_norm": 0.15459200739860535,
      "learning_rate": 9.94271635355713e-05,
      "loss": 0.1799,
      "step": 24500
    },
    {
      "epoch": 1.5097012627040345,
      "grad_norm": 0.2676428556442261,
      "learning_rate": 9.938609998973412e-05,
      "loss": 0.1777,
      "step": 24510
    },
    {
      "epoch": 1.5103172158915923,
      "grad_norm": 0.14992722868919373,
      "learning_rate": 9.934503644389694e-05,
      "loss": 0.1786,
      "step": 24520
    },
    {
      "epoch": 1.5109331690791499,
      "grad_norm": 0.15770180523395538,
      "learning_rate": 9.930397289805974e-05,
      "loss": 0.1768,
      "step": 24530
    },
    {
      "epoch": 1.5115491222667077,
      "grad_norm": 0.1252061128616333,
      "learning_rate": 9.926290935222256e-05,
      "loss": 0.1765,
      "step": 24540
    },
    {
      "epoch": 1.5121650754542655,
      "grad_norm": 0.16028191149234772,
      "learning_rate": 9.922184580638538e-05,
      "loss": 0.18,
      "step": 24550
    },
    {
      "epoch": 1.512781028641823,
      "grad_norm": 0.14696618914604187,
      "learning_rate": 9.91807822605482e-05,
      "loss": 0.1779,
      "step": 24560
    },
    {
      "epoch": 1.513396981829381,
      "grad_norm": 0.1452217996120453,
      "learning_rate": 9.913971871471101e-05,
      "loss": 0.1758,
      "step": 24570
    },
    {
      "epoch": 1.5140129350169387,
      "grad_norm": 0.16052846610546112,
      "learning_rate": 9.909865516887383e-05,
      "loss": 0.1768,
      "step": 24580
    },
    {
      "epoch": 1.5146288882044965,
      "grad_norm": 0.2463287115097046,
      "learning_rate": 9.905759162303665e-05,
      "loss": 0.1775,
      "step": 24590
    },
    {
      "epoch": 1.5152448413920543,
      "grad_norm": 0.16712146997451782,
      "learning_rate": 9.901652807719947e-05,
      "loss": 0.1755,
      "step": 24600
    },
    {
      "epoch": 1.5158607945796119,
      "grad_norm": 0.139370858669281,
      "learning_rate": 9.897546453136228e-05,
      "loss": 0.1782,
      "step": 24610
    },
    {
      "epoch": 1.5164767477671697,
      "grad_norm": 0.15288446843624115,
      "learning_rate": 9.89344009855251e-05,
      "loss": 0.1775,
      "step": 24620
    },
    {
      "epoch": 1.5170927009547275,
      "grad_norm": 0.16558176279067993,
      "learning_rate": 9.889333743968792e-05,
      "loss": 0.1782,
      "step": 24630
    },
    {
      "epoch": 1.517708654142285,
      "grad_norm": 0.17524921894073486,
      "learning_rate": 9.885227389385075e-05,
      "loss": 0.1781,
      "step": 24640
    },
    {
      "epoch": 1.518324607329843,
      "grad_norm": 0.15148672461509705,
      "learning_rate": 9.881121034801356e-05,
      "loss": 0.1787,
      "step": 24650
    },
    {
      "epoch": 1.5189405605174007,
      "grad_norm": 0.1544092297554016,
      "learning_rate": 9.877014680217638e-05,
      "loss": 0.179,
      "step": 24660
    },
    {
      "epoch": 1.5195565137049585,
      "grad_norm": 0.15227752923965454,
      "learning_rate": 9.87290832563392e-05,
      "loss": 0.1784,
      "step": 24670
    },
    {
      "epoch": 1.5201724668925163,
      "grad_norm": 0.1603454202413559,
      "learning_rate": 9.868801971050202e-05,
      "loss": 0.1789,
      "step": 24680
    },
    {
      "epoch": 1.5207884200800739,
      "grad_norm": 0.1496426910161972,
      "learning_rate": 9.864695616466482e-05,
      "loss": 0.1782,
      "step": 24690
    },
    {
      "epoch": 1.5214043732676317,
      "grad_norm": 0.1543893963098526,
      "learning_rate": 9.860589261882764e-05,
      "loss": 0.177,
      "step": 24700
    },
    {
      "epoch": 1.5220203264551895,
      "grad_norm": 0.14977297186851501,
      "learning_rate": 9.856482907299046e-05,
      "loss": 0.1781,
      "step": 24710
    },
    {
      "epoch": 1.522636279642747,
      "grad_norm": 0.144341841340065,
      "learning_rate": 9.852376552715328e-05,
      "loss": 0.1781,
      "step": 24720
    },
    {
      "epoch": 1.5232522328303049,
      "grad_norm": 0.13724608719348907,
      "learning_rate": 9.848270198131609e-05,
      "loss": 0.1795,
      "step": 24730
    },
    {
      "epoch": 1.5238681860178627,
      "grad_norm": 0.16077105700969696,
      "learning_rate": 9.844163843547891e-05,
      "loss": 0.1769,
      "step": 24740
    },
    {
      "epoch": 1.5244841392054203,
      "grad_norm": 0.18463478982448578,
      "learning_rate": 9.840057488964173e-05,
      "loss": 0.1767,
      "step": 24750
    },
    {
      "epoch": 1.5251000923929783,
      "grad_norm": 0.16024979948997498,
      "learning_rate": 9.835951134380455e-05,
      "loss": 0.1791,
      "step": 24760
    },
    {
      "epoch": 1.5257160455805359,
      "grad_norm": 0.15469972789287567,
      "learning_rate": 9.831844779796736e-05,
      "loss": 0.1794,
      "step": 24770
    },
    {
      "epoch": 1.5263319987680937,
      "grad_norm": 0.23135022819042206,
      "learning_rate": 9.827738425213018e-05,
      "loss": 0.178,
      "step": 24780
    },
    {
      "epoch": 1.5269479519556515,
      "grad_norm": 0.16055066883563995,
      "learning_rate": 9.8236320706293e-05,
      "loss": 0.1791,
      "step": 24790
    },
    {
      "epoch": 1.527563905143209,
      "grad_norm": 0.15187057852745056,
      "learning_rate": 9.819525716045582e-05,
      "loss": 0.1777,
      "step": 24800
    },
    {
      "epoch": 1.5281798583307669,
      "grad_norm": 0.17063391208648682,
      "learning_rate": 9.815419361461862e-05,
      "loss": 0.1774,
      "step": 24810
    },
    {
      "epoch": 1.5287958115183247,
      "grad_norm": 0.16453281044960022,
      "learning_rate": 9.811313006878144e-05,
      "loss": 0.1801,
      "step": 24820
    },
    {
      "epoch": 1.5294117647058822,
      "grad_norm": 0.14168959856033325,
      "learning_rate": 9.807206652294426e-05,
      "loss": 0.1773,
      "step": 24830
    },
    {
      "epoch": 1.53002771789344,
      "grad_norm": 0.1323186457157135,
      "learning_rate": 9.803100297710708e-05,
      "loss": 0.1766,
      "step": 24840
    },
    {
      "epoch": 1.5306436710809979,
      "grad_norm": 0.14019998908042908,
      "learning_rate": 9.798993943126989e-05,
      "loss": 0.1755,
      "step": 24850
    },
    {
      "epoch": 1.5312596242685554,
      "grad_norm": 0.13866829872131348,
      "learning_rate": 9.794887588543271e-05,
      "loss": 0.1785,
      "step": 24860
    },
    {
      "epoch": 1.5318755774561135,
      "grad_norm": 0.16660866141319275,
      "learning_rate": 9.790781233959553e-05,
      "loss": 0.1794,
      "step": 24870
    },
    {
      "epoch": 1.532491530643671,
      "grad_norm": 0.33007553219795227,
      "learning_rate": 9.786674879375835e-05,
      "loss": 0.1783,
      "step": 24880
    },
    {
      "epoch": 1.5331074838312289,
      "grad_norm": 0.1502135992050171,
      "learning_rate": 9.782568524792116e-05,
      "loss": 0.1774,
      "step": 24890
    },
    {
      "epoch": 1.5337234370187867,
      "grad_norm": 0.13583311438560486,
      "learning_rate": 9.778462170208398e-05,
      "loss": 0.1763,
      "step": 24900
    },
    {
      "epoch": 1.5343393902063442,
      "grad_norm": 0.13916552066802979,
      "learning_rate": 9.77435581562468e-05,
      "loss": 0.1812,
      "step": 24910
    },
    {
      "epoch": 1.534955343393902,
      "grad_norm": 0.1560535877943039,
      "learning_rate": 9.770249461040962e-05,
      "loss": 0.1769,
      "step": 24920
    },
    {
      "epoch": 1.5355712965814599,
      "grad_norm": 0.12887032330036163,
      "learning_rate": 9.766143106457242e-05,
      "loss": 0.176,
      "step": 24930
    },
    {
      "epoch": 1.5361872497690174,
      "grad_norm": 0.18053704500198364,
      "learning_rate": 9.762036751873524e-05,
      "loss": 0.179,
      "step": 24940
    },
    {
      "epoch": 1.5368032029565755,
      "grad_norm": 0.13418827950954437,
      "learning_rate": 9.757930397289806e-05,
      "loss": 0.1765,
      "step": 24950
    },
    {
      "epoch": 1.537419156144133,
      "grad_norm": 0.14707110822200775,
      "learning_rate": 9.753824042706088e-05,
      "loss": 0.1789,
      "step": 24960
    },
    {
      "epoch": 1.5380351093316906,
      "grad_norm": 0.15539342164993286,
      "learning_rate": 9.749717688122369e-05,
      "loss": 0.1799,
      "step": 24970
    },
    {
      "epoch": 1.5386510625192487,
      "grad_norm": 0.17879065871238708,
      "learning_rate": 9.745611333538651e-05,
      "loss": 0.1802,
      "step": 24980
    },
    {
      "epoch": 1.5392670157068062,
      "grad_norm": 0.15896101295948029,
      "learning_rate": 9.741504978954933e-05,
      "loss": 0.1794,
      "step": 24990
    },
    {
      "epoch": 1.539882968894364,
      "grad_norm": 0.1465841680765152,
      "learning_rate": 9.737398624371215e-05,
      "loss": 0.1758,
      "step": 25000
    },
    {
      "epoch": 1.5404989220819219,
      "grad_norm": 0.1344524621963501,
      "learning_rate": 9.733292269787496e-05,
      "loss": 0.1779,
      "step": 25010
    },
    {
      "epoch": 1.5411148752694794,
      "grad_norm": 0.13876603543758392,
      "learning_rate": 9.729185915203778e-05,
      "loss": 0.1767,
      "step": 25020
    },
    {
      "epoch": 1.5417308284570372,
      "grad_norm": 0.14803679287433624,
      "learning_rate": 9.72507956062006e-05,
      "loss": 0.1758,
      "step": 25030
    },
    {
      "epoch": 1.542346781644595,
      "grad_norm": 0.14213328063488007,
      "learning_rate": 9.720973206036342e-05,
      "loss": 0.179,
      "step": 25040
    },
    {
      "epoch": 1.5429627348321526,
      "grad_norm": 0.15281516313552856,
      "learning_rate": 9.716866851452622e-05,
      "loss": 0.1773,
      "step": 25050
    },
    {
      "epoch": 1.5435786880197107,
      "grad_norm": 0.16323156654834747,
      "learning_rate": 9.712760496868904e-05,
      "loss": 0.177,
      "step": 25060
    },
    {
      "epoch": 1.5441946412072682,
      "grad_norm": 0.20891839265823364,
      "learning_rate": 9.708654142285188e-05,
      "loss": 0.1778,
      "step": 25070
    },
    {
      "epoch": 1.544810594394826,
      "grad_norm": NaN,
      "learning_rate": 9.70454778770147e-05,
      "loss": 0.18,
      "step": 25080
    },
    {
      "epoch": 1.5454265475823838,
      "grad_norm": 0.17285779118537903,
      "learning_rate": 9.700852068576122e-05,
      "loss": 0.1777,
      "step": 25090
    },
    {
      "epoch": 1.5460425007699414,
      "grad_norm": 0.1579088717699051,
      "learning_rate": 9.696745713992403e-05,
      "loss": 0.1778,
      "step": 25100
    },
    {
      "epoch": 1.5466584539574992,
      "grad_norm": 0.15021838247776031,
      "learning_rate": 9.692639359408685e-05,
      "loss": 0.1772,
      "step": 25110
    },
    {
      "epoch": 1.547274407145057,
      "grad_norm": 0.15513566136360168,
      "learning_rate": 9.688533004824967e-05,
      "loss": 0.1777,
      "step": 25120
    },
    {
      "epoch": 1.5478903603326146,
      "grad_norm": 0.15650524199008942,
      "learning_rate": 9.684426650241249e-05,
      "loss": 0.1773,
      "step": 25130
    },
    {
      "epoch": 1.5485063135201724,
      "grad_norm": 0.16483692824840546,
      "learning_rate": 9.68032029565753e-05,
      "loss": 0.1777,
      "step": 25140
    },
    {
      "epoch": 1.5491222667077302,
      "grad_norm": 0.17964966595172882,
      "learning_rate": 9.676213941073811e-05,
      "loss": 0.1781,
      "step": 25150
    },
    {
      "epoch": 1.5497382198952878,
      "grad_norm": 0.17283512651920319,
      "learning_rate": 9.672107586490093e-05,
      "loss": 0.1777,
      "step": 25160
    },
    {
      "epoch": 1.5503541730828458,
      "grad_norm": 0.15410394966602325,
      "learning_rate": 9.668001231906375e-05,
      "loss": 0.1793,
      "step": 25170
    },
    {
      "epoch": 1.5509701262704034,
      "grad_norm": 0.15209347009658813,
      "learning_rate": 9.663894877322657e-05,
      "loss": 0.1755,
      "step": 25180
    },
    {
      "epoch": 1.5515860794579612,
      "grad_norm": 0.13882796466350555,
      "learning_rate": 9.659788522738938e-05,
      "loss": 0.1769,
      "step": 25190
    },
    {
      "epoch": 1.552202032645519,
      "grad_norm": 0.16792118549346924,
      "learning_rate": 9.655682168155221e-05,
      "loss": 0.179,
      "step": 25200
    },
    {
      "epoch": 1.5528179858330766,
      "grad_norm": 0.16913196444511414,
      "learning_rate": 9.651575813571503e-05,
      "loss": 0.1788,
      "step": 25210
    },
    {
      "epoch": 1.5534339390206344,
      "grad_norm": 0.15429051220417023,
      "learning_rate": 9.647469458987784e-05,
      "loss": 0.1784,
      "step": 25220
    },
    {
      "epoch": 1.5540498922081922,
      "grad_norm": 0.1334472894668579,
      "learning_rate": 9.643363104404066e-05,
      "loss": 0.1742,
      "step": 25230
    },
    {
      "epoch": 1.5546658453957498,
      "grad_norm": 0.13656970858573914,
      "learning_rate": 9.639256749820348e-05,
      "loss": 0.1784,
      "step": 25240
    },
    {
      "epoch": 1.5552817985833076,
      "grad_norm": 0.1701454520225525,
      "learning_rate": 9.63515039523663e-05,
      "loss": 0.1769,
      "step": 25250
    },
    {
      "epoch": 1.5558977517708654,
      "grad_norm": 0.1722726970911026,
      "learning_rate": 9.631044040652911e-05,
      "loss": 0.1762,
      "step": 25260
    },
    {
      "epoch": 1.556513704958423,
      "grad_norm": 0.14047843217849731,
      "learning_rate": 9.626937686069193e-05,
      "loss": 0.1772,
      "step": 25270
    },
    {
      "epoch": 1.557129658145981,
      "grad_norm": 0.15970070660114288,
      "learning_rate": 9.622831331485475e-05,
      "loss": 0.1797,
      "step": 25280
    },
    {
      "epoch": 1.5577456113335386,
      "grad_norm": 0.13491272926330566,
      "learning_rate": 9.618724976901757e-05,
      "loss": 0.1783,
      "step": 25290
    },
    {
      "epoch": 1.5583615645210964,
      "grad_norm": 0.12854385375976562,
      "learning_rate": 9.614618622318037e-05,
      "loss": 0.18,
      "step": 25300
    },
    {
      "epoch": 1.5589775177086542,
      "grad_norm": 0.17164568603038788,
      "learning_rate": 9.61051226773432e-05,
      "loss": 0.1774,
      "step": 25310
    },
    {
      "epoch": 1.5595934708962118,
      "grad_norm": 0.15026423335075378,
      "learning_rate": 9.606405913150601e-05,
      "loss": 0.177,
      "step": 25320
    },
    {
      "epoch": 1.5602094240837696,
      "grad_norm": 0.1313033103942871,
      "learning_rate": 9.602299558566883e-05,
      "loss": 0.1775,
      "step": 25330
    },
    {
      "epoch": 1.5608253772713274,
      "grad_norm": 0.17194314301013947,
      "learning_rate": 9.598193203983164e-05,
      "loss": 0.1807,
      "step": 25340
    },
    {
      "epoch": 1.561441330458885,
      "grad_norm": 0.1440819352865219,
      "learning_rate": 9.594086849399446e-05,
      "loss": 0.1773,
      "step": 25350
    },
    {
      "epoch": 1.562057283646443,
      "grad_norm": 0.17344330251216888,
      "learning_rate": 9.589980494815728e-05,
      "loss": 0.1776,
      "step": 25360
    },
    {
      "epoch": 1.5626732368340006,
      "grad_norm": 0.14512723684310913,
      "learning_rate": 9.58587414023201e-05,
      "loss": 0.1785,
      "step": 25370
    },
    {
      "epoch": 1.5632891900215584,
      "grad_norm": 0.14031678438186646,
      "learning_rate": 9.581767785648291e-05,
      "loss": 0.1769,
      "step": 25380
    },
    {
      "epoch": 1.5639051432091162,
      "grad_norm": 0.2738986313343048,
      "learning_rate": 9.577661431064573e-05,
      "loss": 0.1781,
      "step": 25390
    },
    {
      "epoch": 1.5645210963966738,
      "grad_norm": 0.15312200784683228,
      "learning_rate": 9.573555076480855e-05,
      "loss": 0.1763,
      "step": 25400
    },
    {
      "epoch": 1.5651370495842316,
      "grad_norm": 0.1556667983531952,
      "learning_rate": 9.569448721897137e-05,
      "loss": 0.1778,
      "step": 25410
    },
    {
      "epoch": 1.5657530027717894,
      "grad_norm": 0.16105997562408447,
      "learning_rate": 9.565342367313417e-05,
      "loss": 0.1764,
      "step": 25420
    },
    {
      "epoch": 1.566368955959347,
      "grad_norm": 0.13007421791553497,
      "learning_rate": 9.5612360127297e-05,
      "loss": 0.1767,
      "step": 25430
    },
    {
      "epoch": 1.5669849091469048,
      "grad_norm": 0.1498444825410843,
      "learning_rate": 9.557129658145981e-05,
      "loss": 0.1798,
      "step": 25440
    },
    {
      "epoch": 1.5676008623344626,
      "grad_norm": 0.15187203884124756,
      "learning_rate": 9.553023303562263e-05,
      "loss": 0.1756,
      "step": 25450
    },
    {
      "epoch": 1.5682168155220202,
      "grad_norm": 0.14207389950752258,
      "learning_rate": 9.548916948978544e-05,
      "loss": 0.1766,
      "step": 25460
    },
    {
      "epoch": 1.5688327687095782,
      "grad_norm": 0.146403968334198,
      "learning_rate": 9.544810594394826e-05,
      "loss": 0.1755,
      "step": 25470
    },
    {
      "epoch": 1.5694487218971358,
      "grad_norm": 0.21965178847312927,
      "learning_rate": 9.540704239811108e-05,
      "loss": 0.1801,
      "step": 25480
    },
    {
      "epoch": 1.5700646750846936,
      "grad_norm": 0.1523878276348114,
      "learning_rate": 9.53659788522739e-05,
      "loss": 0.1764,
      "step": 25490
    },
    {
      "epoch": 1.5706806282722514,
      "grad_norm": 0.16056522727012634,
      "learning_rate": 9.532491530643671e-05,
      "loss": 0.1802,
      "step": 25500
    },
    {
      "epoch": 1.571296581459809,
      "grad_norm": 0.1372910737991333,
      "learning_rate": 9.528385176059953e-05,
      "loss": 0.1764,
      "step": 25510
    },
    {
      "epoch": 1.5719125346473668,
      "grad_norm": 0.7893226146697998,
      "learning_rate": 9.524278821476235e-05,
      "loss": 0.1787,
      "step": 25520
    },
    {
      "epoch": 1.5725284878349246,
      "grad_norm": 0.16269977390766144,
      "learning_rate": 9.520172466892517e-05,
      "loss": 0.1785,
      "step": 25530
    },
    {
      "epoch": 1.5731444410224822,
      "grad_norm": 0.19678068161010742,
      "learning_rate": 9.516066112308797e-05,
      "loss": 0.179,
      "step": 25540
    },
    {
      "epoch": 1.57376039421004,
      "grad_norm": 0.14138628542423248,
      "learning_rate": 9.51195975772508e-05,
      "loss": 0.178,
      "step": 25550
    },
    {
      "epoch": 1.5743763473975978,
      "grad_norm": 0.1676260083913803,
      "learning_rate": 9.507853403141361e-05,
      "loss": 0.178,
      "step": 25560
    },
    {
      "epoch": 1.5749923005851554,
      "grad_norm": 0.16646742820739746,
      "learning_rate": 9.503747048557643e-05,
      "loss": 0.18,
      "step": 25570
    },
    {
      "epoch": 1.5756082537727134,
      "grad_norm": 0.13523197174072266,
      "learning_rate": 9.499640693973924e-05,
      "loss": 0.1778,
      "step": 25580
    },
    {
      "epoch": 1.576224206960271,
      "grad_norm": 0.1655728667974472,
      "learning_rate": 9.495534339390206e-05,
      "loss": 0.1775,
      "step": 25590
    },
    {
      "epoch": 1.5768401601478288,
      "grad_norm": 0.19500140845775604,
      "learning_rate": 9.491427984806488e-05,
      "loss": 0.1765,
      "step": 25600
    },
    {
      "epoch": 1.5774561133353866,
      "grad_norm": 0.1578228771686554,
      "learning_rate": 9.48732163022277e-05,
      "loss": 0.176,
      "step": 25610
    },
    {
      "epoch": 1.5780720665229442,
      "grad_norm": 0.17034965753555298,
      "learning_rate": 9.483215275639052e-05,
      "loss": 0.1785,
      "step": 25620
    },
    {
      "epoch": 1.578688019710502,
      "grad_norm": 0.1531260460615158,
      "learning_rate": 9.479108921055334e-05,
      "loss": 0.1785,
      "step": 25630
    },
    {
      "epoch": 1.5793039728980598,
      "grad_norm": 0.15780946612358093,
      "learning_rate": 9.475002566471616e-05,
      "loss": 0.1794,
      "step": 25640
    },
    {
      "epoch": 1.5799199260856174,
      "grad_norm": 0.15320566296577454,
      "learning_rate": 9.470896211887898e-05,
      "loss": 0.1788,
      "step": 25650
    },
    {
      "epoch": 1.5805358792731754,
      "grad_norm": 0.1538640707731247,
      "learning_rate": 9.466789857304179e-05,
      "loss": 0.1759,
      "step": 25660
    },
    {
      "epoch": 1.581151832460733,
      "grad_norm": 0.1678207814693451,
      "learning_rate": 9.462683502720461e-05,
      "loss": 0.1789,
      "step": 25670
    },
    {
      "epoch": 1.5817677856482906,
      "grad_norm": 0.1433187574148178,
      "learning_rate": 9.458577148136743e-05,
      "loss": 0.1788,
      "step": 25680
    },
    {
      "epoch": 1.5823837388358486,
      "grad_norm": 0.2959056794643402,
      "learning_rate": 9.454470793553025e-05,
      "loss": 0.1792,
      "step": 25690
    },
    {
      "epoch": 1.5829996920234062,
      "grad_norm": 0.16898082196712494,
      "learning_rate": 9.450364438969305e-05,
      "loss": 0.1794,
      "step": 25700
    },
    {
      "epoch": 1.583615645210964,
      "grad_norm": 0.1821897029876709,
      "learning_rate": 9.446258084385587e-05,
      "loss": 0.1801,
      "step": 25710
    },
    {
      "epoch": 1.5842315983985218,
      "grad_norm": 0.14593032002449036,
      "learning_rate": 9.44215172980187e-05,
      "loss": 0.1785,
      "step": 25720
    },
    {
      "epoch": 1.5848475515860794,
      "grad_norm": 0.1565188318490982,
      "learning_rate": 9.438045375218151e-05,
      "loss": 0.1794,
      "step": 25730
    },
    {
      "epoch": 1.5854635047736372,
      "grad_norm": 0.8059872984886169,
      "learning_rate": 9.433939020634432e-05,
      "loss": 0.1809,
      "step": 25740
    },
    {
      "epoch": 1.586079457961195,
      "grad_norm": 0.1849576085805893,
      "learning_rate": 9.429832666050714e-05,
      "loss": 0.1771,
      "step": 25750
    },
    {
      "epoch": 1.5866954111487526,
      "grad_norm": 0.5002298951148987,
      "learning_rate": 9.425726311466996e-05,
      "loss": 0.1794,
      "step": 25760
    },
    {
      "epoch": 1.5873113643363106,
      "grad_norm": 0.16727612912654877,
      "learning_rate": 9.421619956883278e-05,
      "loss": 0.1824,
      "step": 25770
    },
    {
      "epoch": 1.5879273175238682,
      "grad_norm": 0.18200528621673584,
      "learning_rate": 9.417513602299559e-05,
      "loss": 0.179,
      "step": 25780
    },
    {
      "epoch": 1.588543270711426,
      "grad_norm": 0.62065190076828,
      "learning_rate": 9.41340724771584e-05,
      "loss": 0.1825,
      "step": 25790
    },
    {
      "epoch": 1.5891592238989838,
      "grad_norm": 0.15864847600460052,
      "learning_rate": 9.409300893132123e-05,
      "loss": 0.1763,
      "step": 25800
    },
    {
      "epoch": 1.5897751770865414,
      "grad_norm": 0.16204336285591125,
      "learning_rate": 9.405194538548405e-05,
      "loss": 0.1765,
      "step": 25810
    },
    {
      "epoch": 1.5903911302740992,
      "grad_norm": 0.17304207384586334,
      "learning_rate": 9.401088183964685e-05,
      "loss": 0.1763,
      "step": 25820
    },
    {
      "epoch": 1.591007083461657,
      "grad_norm": 0.17274463176727295,
      "learning_rate": 9.396981829380967e-05,
      "loss": 0.1763,
      "step": 25830
    },
    {
      "epoch": 1.5916230366492146,
      "grad_norm": 0.1461736112833023,
      "learning_rate": 9.39287547479725e-05,
      "loss": 0.1748,
      "step": 25840
    },
    {
      "epoch": 1.5922389898367724,
      "grad_norm": 0.25649508833885193,
      "learning_rate": 9.388769120213531e-05,
      "loss": 0.1762,
      "step": 25850
    },
    {
      "epoch": 1.5928549430243302,
      "grad_norm": 0.1378600299358368,
      "learning_rate": 9.384662765629812e-05,
      "loss": 0.1787,
      "step": 25860
    },
    {
      "epoch": 1.5934708962118878,
      "grad_norm": 0.1911572515964508,
      "learning_rate": 9.380556411046094e-05,
      "loss": 0.1779,
      "step": 25870
    },
    {
      "epoch": 1.5940868493994458,
      "grad_norm": 0.1682307869195938,
      "learning_rate": 9.376450056462376e-05,
      "loss": 0.1803,
      "step": 25880
    },
    {
      "epoch": 1.5947028025870034,
      "grad_norm": 0.1593656837940216,
      "learning_rate": 9.372343701878658e-05,
      "loss": 0.1788,
      "step": 25890
    },
    {
      "epoch": 1.5953187557745612,
      "grad_norm": 0.16927285492420197,
      "learning_rate": 9.368237347294939e-05,
      "loss": 0.1772,
      "step": 25900
    },
    {
      "epoch": 1.595934708962119,
      "grad_norm": 0.15812934935092926,
      "learning_rate": 9.36413099271122e-05,
      "loss": 0.1773,
      "step": 25910
    },
    {
      "epoch": 1.5965506621496766,
      "grad_norm": 0.1918811947107315,
      "learning_rate": 9.360024638127503e-05,
      "loss": 0.1797,
      "step": 25920
    },
    {
      "epoch": 1.5971666153372344,
      "grad_norm": 0.16597174108028412,
      "learning_rate": 9.355918283543785e-05,
      "loss": 0.1767,
      "step": 25930
    },
    {
      "epoch": 1.5977825685247922,
      "grad_norm": 0.15183497965335846,
      "learning_rate": 9.351811928960065e-05,
      "loss": 0.1791,
      "step": 25940
    },
    {
      "epoch": 1.5983985217123498,
      "grad_norm": 0.16860386729240417,
      "learning_rate": 9.347705574376347e-05,
      "loss": 0.1785,
      "step": 25950
    },
    {
      "epoch": 1.5990144748999076,
      "grad_norm": 0.14644485712051392,
      "learning_rate": 9.343599219792629e-05,
      "loss": 0.1796,
      "step": 25960
    },
    {
      "epoch": 1.5996304280874654,
      "grad_norm": 0.14143195748329163,
      "learning_rate": 9.339492865208911e-05,
      "loss": 0.1791,
      "step": 25970
    },
    {
      "epoch": 1.600246381275023,
      "grad_norm": 0.15959607064723969,
      "learning_rate": 9.335386510625192e-05,
      "loss": 0.1757,
      "step": 25980
    },
    {
      "epoch": 1.600862334462581,
      "grad_norm": 0.15542005002498627,
      "learning_rate": 9.331280156041474e-05,
      "loss": 0.1771,
      "step": 25990
    },
    {
      "epoch": 1.6014782876501386,
      "grad_norm": 0.16718468070030212,
      "learning_rate": 9.327173801457756e-05,
      "loss": 0.177,
      "step": 26000
    },
    {
      "epoch": 1.6020942408376964,
      "grad_norm": 0.14738284051418304,
      "learning_rate": 9.323067446874038e-05,
      "loss": 0.1787,
      "step": 26010
    },
    {
      "epoch": 1.6027101940252542,
      "grad_norm": 0.18292205035686493,
      "learning_rate": 9.318961092290319e-05,
      "loss": 0.1795,
      "step": 26020
    },
    {
      "epoch": 1.6033261472128117,
      "grad_norm": 0.15847137570381165,
      "learning_rate": 9.3148547377066e-05,
      "loss": 0.1784,
      "step": 26030
    },
    {
      "epoch": 1.6039421004003696,
      "grad_norm": 0.1529800146818161,
      "learning_rate": 9.310748383122883e-05,
      "loss": 0.1784,
      "step": 26040
    },
    {
      "epoch": 1.6045580535879274,
      "grad_norm": 0.1992902010679245,
      "learning_rate": 9.306642028539166e-05,
      "loss": 0.1765,
      "step": 26050
    },
    {
      "epoch": 1.605174006775485,
      "grad_norm": 0.15154781937599182,
      "learning_rate": 9.302535673955447e-05,
      "loss": 0.1766,
      "step": 26060
    },
    {
      "epoch": 1.605789959963043,
      "grad_norm": 0.1532622128725052,
      "learning_rate": 9.298429319371729e-05,
      "loss": 0.178,
      "step": 26070
    },
    {
      "epoch": 1.6064059131506006,
      "grad_norm": 0.17293919622898102,
      "learning_rate": 9.29432296478801e-05,
      "loss": 0.177,
      "step": 26080
    },
    {
      "epoch": 1.6070218663381581,
      "grad_norm": 0.17221252620220184,
      "learning_rate": 9.290216610204293e-05,
      "loss": 0.1786,
      "step": 26090
    },
    {
      "epoch": 1.6076378195257162,
      "grad_norm": 0.16072221100330353,
      "learning_rate": 9.286110255620573e-05,
      "loss": 0.1776,
      "step": 26100
    },
    {
      "epoch": 1.6082537727132737,
      "grad_norm": 0.15792062878608704,
      "learning_rate": 9.282003901036855e-05,
      "loss": 0.1808,
      "step": 26110
    },
    {
      "epoch": 1.6088697259008315,
      "grad_norm": 0.16486655175685883,
      "learning_rate": 9.277897546453137e-05,
      "loss": 0.1752,
      "step": 26120
    },
    {
      "epoch": 1.6094856790883894,
      "grad_norm": 0.28809043765068054,
      "learning_rate": 9.273791191869419e-05,
      "loss": 0.1805,
      "step": 26130
    },
    {
      "epoch": 1.610101632275947,
      "grad_norm": 0.18846170604228973,
      "learning_rate": 9.2696848372857e-05,
      "loss": 0.176,
      "step": 26140
    },
    {
      "epoch": 1.6107175854635047,
      "grad_norm": 0.17909179627895355,
      "learning_rate": 9.265578482701982e-05,
      "loss": 0.1763,
      "step": 26150
    },
    {
      "epoch": 1.6113335386510625,
      "grad_norm": 0.18399658799171448,
      "learning_rate": 9.261472128118264e-05,
      "loss": 0.1792,
      "step": 26160
    },
    {
      "epoch": 1.6119494918386201,
      "grad_norm": 0.19846504926681519,
      "learning_rate": 9.257365773534546e-05,
      "loss": 0.179,
      "step": 26170
    },
    {
      "epoch": 1.6125654450261782,
      "grad_norm": 0.14191466569900513,
      "learning_rate": 9.253259418950827e-05,
      "loss": 0.1777,
      "step": 26180
    },
    {
      "epoch": 1.6131813982137357,
      "grad_norm": 0.14939628541469574,
      "learning_rate": 9.249153064367109e-05,
      "loss": 0.1767,
      "step": 26190
    },
    {
      "epoch": 1.6137973514012935,
      "grad_norm": 0.14900073409080505,
      "learning_rate": 9.24504670978339e-05,
      "loss": 0.1782,
      "step": 26200
    },
    {
      "epoch": 1.6144133045888514,
      "grad_norm": 0.16478271782398224,
      "learning_rate": 9.240940355199673e-05,
      "loss": 0.1787,
      "step": 26210
    },
    {
      "epoch": 1.615029257776409,
      "grad_norm": 0.14371447265148163,
      "learning_rate": 9.236834000615953e-05,
      "loss": 0.1765,
      "step": 26220
    },
    {
      "epoch": 1.6156452109639667,
      "grad_norm": 0.20208850502967834,
      "learning_rate": 9.232727646032235e-05,
      "loss": 0.1807,
      "step": 26230
    },
    {
      "epoch": 1.6162611641515245,
      "grad_norm": 0.16306884586811066,
      "learning_rate": 9.228621291448517e-05,
      "loss": 0.1777,
      "step": 26240
    },
    {
      "epoch": 1.6168771173390821,
      "grad_norm": 0.1443970501422882,
      "learning_rate": 9.224514936864799e-05,
      "loss": 0.1784,
      "step": 26250
    },
    {
      "epoch": 1.61749307052664,
      "grad_norm": 0.16782236099243164,
      "learning_rate": 9.22040858228108e-05,
      "loss": 0.1773,
      "step": 26260
    },
    {
      "epoch": 1.6181090237141977,
      "grad_norm": 0.16925321519374847,
      "learning_rate": 9.216302227697362e-05,
      "loss": 0.1777,
      "step": 26270
    },
    {
      "epoch": 1.6187249769017553,
      "grad_norm": 0.17406094074249268,
      "learning_rate": 9.212195873113644e-05,
      "loss": 0.1765,
      "step": 26280
    },
    {
      "epoch": 1.6193409300893133,
      "grad_norm": 0.14916642010211945,
      "learning_rate": 9.208089518529926e-05,
      "loss": 0.1787,
      "step": 26290
    },
    {
      "epoch": 1.619956883276871,
      "grad_norm": 0.1478777974843979,
      "learning_rate": 9.203983163946207e-05,
      "loss": 0.1765,
      "step": 26300
    },
    {
      "epoch": 1.6205728364644287,
      "grad_norm": 0.1362849622964859,
      "learning_rate": 9.199876809362489e-05,
      "loss": 0.1776,
      "step": 26310
    },
    {
      "epoch": 1.6211887896519865,
      "grad_norm": 0.15089839696884155,
      "learning_rate": 9.19577045477877e-05,
      "loss": 0.1767,
      "step": 26320
    },
    {
      "epoch": 1.6218047428395441,
      "grad_norm": 0.15090279281139374,
      "learning_rate": 9.191664100195053e-05,
      "loss": 0.1778,
      "step": 26330
    },
    {
      "epoch": 1.622420696027102,
      "grad_norm": 0.16187994182109833,
      "learning_rate": 9.187557745611333e-05,
      "loss": 0.1786,
      "step": 26340
    },
    {
      "epoch": 1.6230366492146597,
      "grad_norm": 0.16924859583377838,
      "learning_rate": 9.183451391027615e-05,
      "loss": 0.1779,
      "step": 26350
    },
    {
      "epoch": 1.6236526024022173,
      "grad_norm": 0.16389361023902893,
      "learning_rate": 9.179345036443897e-05,
      "loss": 0.1755,
      "step": 26360
    },
    {
      "epoch": 1.6242685555897753,
      "grad_norm": 0.15388643741607666,
      "learning_rate": 9.175238681860179e-05,
      "loss": 0.1768,
      "step": 26370
    },
    {
      "epoch": 1.624884508777333,
      "grad_norm": 0.1606944501399994,
      "learning_rate": 9.17113232727646e-05,
      "loss": 0.1791,
      "step": 26380
    },
    {
      "epoch": 1.6255004619648905,
      "grad_norm": 0.16634812951087952,
      "learning_rate": 9.167025972692742e-05,
      "loss": 0.177,
      "step": 26390
    },
    {
      "epoch": 1.6261164151524485,
      "grad_norm": 0.16099758446216583,
      "learning_rate": 9.162919618109024e-05,
      "loss": 0.1801,
      "step": 26400
    },
    {
      "epoch": 1.6267323683400061,
      "grad_norm": 0.1536453664302826,
      "learning_rate": 9.158813263525306e-05,
      "loss": 0.1774,
      "step": 26410
    },
    {
      "epoch": 1.627348321527564,
      "grad_norm": 0.18625839054584503,
      "learning_rate": 9.154706908941587e-05,
      "loss": 0.1779,
      "step": 26420
    },
    {
      "epoch": 1.6279642747151217,
      "grad_norm": 0.1583959013223648,
      "learning_rate": 9.150600554357869e-05,
      "loss": 0.1777,
      "step": 26430
    },
    {
      "epoch": 1.6285802279026793,
      "grad_norm": 0.15009558200836182,
      "learning_rate": 9.14649419977415e-05,
      "loss": 0.1771,
      "step": 26440
    },
    {
      "epoch": 1.6291961810902371,
      "grad_norm": 0.12956497073173523,
      "learning_rate": 9.142387845190433e-05,
      "loss": 0.1766,
      "step": 26450
    },
    {
      "epoch": 1.629812134277795,
      "grad_norm": 0.19292978942394257,
      "learning_rate": 9.138281490606713e-05,
      "loss": 0.1778,
      "step": 26460
    },
    {
      "epoch": 1.6304280874653525,
      "grad_norm": 0.18567512929439545,
      "learning_rate": 9.134175136022997e-05,
      "loss": 0.1787,
      "step": 26470
    },
    {
      "epoch": 1.6310440406529105,
      "grad_norm": 0.19769135117530823,
      "learning_rate": 9.130068781439279e-05,
      "loss": 0.1788,
      "step": 26480
    },
    {
      "epoch": 1.6316599938404681,
      "grad_norm": 0.1887347251176834,
      "learning_rate": 9.125962426855559e-05,
      "loss": 0.1765,
      "step": 26490
    },
    {
      "epoch": 1.632275947028026,
      "grad_norm": 0.17257797718048096,
      "learning_rate": 9.121856072271841e-05,
      "loss": 0.1768,
      "step": 26500
    },
    {
      "epoch": 1.6328919002155837,
      "grad_norm": 0.16973470151424408,
      "learning_rate": 9.117749717688123e-05,
      "loss": 0.1789,
      "step": 26510
    },
    {
      "epoch": 1.6335078534031413,
      "grad_norm": 0.13647377490997314,
      "learning_rate": 9.113643363104405e-05,
      "loss": 0.1772,
      "step": 26520
    },
    {
      "epoch": 1.6341238065906991,
      "grad_norm": 0.16296815872192383,
      "learning_rate": 9.109537008520687e-05,
      "loss": 0.1776,
      "step": 26530
    },
    {
      "epoch": 1.634739759778257,
      "grad_norm": 0.16755186021327972,
      "learning_rate": 9.105430653936968e-05,
      "loss": 0.1758,
      "step": 26540
    },
    {
      "epoch": 1.6353557129658145,
      "grad_norm": 0.2356729656457901,
      "learning_rate": 9.10132429935325e-05,
      "loss": 0.1786,
      "step": 26550
    },
    {
      "epoch": 1.6359716661533723,
      "grad_norm": 0.14152342081069946,
      "learning_rate": 9.097217944769532e-05,
      "loss": 0.177,
      "step": 26560
    },
    {
      "epoch": 1.63658761934093,
      "grad_norm": 0.17453427612781525,
      "learning_rate": 9.093111590185814e-05,
      "loss": 0.1781,
      "step": 26570
    },
    {
      "epoch": 1.6372035725284877,
      "grad_norm": 0.2067040205001831,
      "learning_rate": 9.089005235602095e-05,
      "loss": 0.1776,
      "step": 26580
    },
    {
      "epoch": 1.6378195257160457,
      "grad_norm": 0.15625786781311035,
      "learning_rate": 9.084898881018377e-05,
      "loss": 0.1785,
      "step": 26590
    },
    {
      "epoch": 1.6384354789036033,
      "grad_norm": 0.16439783573150635,
      "learning_rate": 9.080792526434659e-05,
      "loss": 0.1772,
      "step": 26600
    },
    {
      "epoch": 1.639051432091161,
      "grad_norm": 0.16354671120643616,
      "learning_rate": 9.07668617185094e-05,
      "loss": 0.1787,
      "step": 26610
    },
    {
      "epoch": 1.639667385278719,
      "grad_norm": 0.16499002277851105,
      "learning_rate": 9.072579817267221e-05,
      "loss": 0.1768,
      "step": 26620
    },
    {
      "epoch": 1.6402833384662765,
      "grad_norm": 0.22203628718852997,
      "learning_rate": 9.068473462683503e-05,
      "loss": 0.1794,
      "step": 26630
    },
    {
      "epoch": 1.6408992916538343,
      "grad_norm": 0.15780462324619293,
      "learning_rate": 9.064367108099785e-05,
      "loss": 0.1774,
      "step": 26640
    },
    {
      "epoch": 1.641515244841392,
      "grad_norm": 0.1512366384267807,
      "learning_rate": 9.060260753516067e-05,
      "loss": 0.1746,
      "step": 26650
    },
    {
      "epoch": 1.6421311980289497,
      "grad_norm": 0.15600621700286865,
      "learning_rate": 9.056154398932348e-05,
      "loss": 0.1766,
      "step": 26660
    },
    {
      "epoch": 1.6427471512165075,
      "grad_norm": 0.1471882462501526,
      "learning_rate": 9.05204804434863e-05,
      "loss": 0.179,
      "step": 26670
    },
    {
      "epoch": 1.6433631044040653,
      "grad_norm": 0.15915293991565704,
      "learning_rate": 9.047941689764912e-05,
      "loss": 0.1787,
      "step": 26680
    },
    {
      "epoch": 1.6439790575916229,
      "grad_norm": 0.17770057916641235,
      "learning_rate": 9.043835335181194e-05,
      "loss": 0.1784,
      "step": 26690
    },
    {
      "epoch": 1.644595010779181,
      "grad_norm": 0.18353796005249023,
      "learning_rate": 9.039728980597475e-05,
      "loss": 0.1766,
      "step": 26700
    },
    {
      "epoch": 1.6452109639667385,
      "grad_norm": 0.15780572593212128,
      "learning_rate": 9.035622626013757e-05,
      "loss": 0.178,
      "step": 26710
    },
    {
      "epoch": 1.6458269171542963,
      "grad_norm": 0.138588547706604,
      "learning_rate": 9.031516271430039e-05,
      "loss": 0.176,
      "step": 26720
    },
    {
      "epoch": 1.646442870341854,
      "grad_norm": 0.16761058568954468,
      "learning_rate": 9.02740991684632e-05,
      "loss": 0.1773,
      "step": 26730
    },
    {
      "epoch": 1.6470588235294117,
      "grad_norm": 0.21600520610809326,
      "learning_rate": 9.023303562262601e-05,
      "loss": 0.176,
      "step": 26740
    },
    {
      "epoch": 1.6476747767169695,
      "grad_norm": 0.1724279522895813,
      "learning_rate": 9.019197207678883e-05,
      "loss": 0.1781,
      "step": 26750
    },
    {
      "epoch": 1.6482907299045273,
      "grad_norm": 0.2044299840927124,
      "learning_rate": 9.015090853095165e-05,
      "loss": 0.1767,
      "step": 26760
    },
    {
      "epoch": 1.6489066830920849,
      "grad_norm": 0.1761915683746338,
      "learning_rate": 9.010984498511447e-05,
      "loss": 0.1762,
      "step": 26770
    },
    {
      "epoch": 1.649522636279643,
      "grad_norm": 0.16855183243751526,
      "learning_rate": 9.006878143927728e-05,
      "loss": 0.1805,
      "step": 26780
    },
    {
      "epoch": 1.6501385894672005,
      "grad_norm": 0.16931946575641632,
      "learning_rate": 9.00277178934401e-05,
      "loss": 0.1762,
      "step": 26790
    },
    {
      "epoch": 1.650754542654758,
      "grad_norm": 0.16743847727775574,
      "learning_rate": 8.998665434760292e-05,
      "loss": 0.1779,
      "step": 26800
    },
    {
      "epoch": 1.651370495842316,
      "grad_norm": 0.14713607728481293,
      "learning_rate": 8.994559080176574e-05,
      "loss": 0.1767,
      "step": 26810
    },
    {
      "epoch": 1.6519864490298737,
      "grad_norm": 0.1525770127773285,
      "learning_rate": 8.990452725592855e-05,
      "loss": 0.1775,
      "step": 26820
    },
    {
      "epoch": 1.6526024022174315,
      "grad_norm": 0.1719352751970291,
      "learning_rate": 8.986346371009137e-05,
      "loss": 0.1784,
      "step": 26830
    },
    {
      "epoch": 1.6532183554049893,
      "grad_norm": 0.1374424695968628,
      "learning_rate": 8.982240016425419e-05,
      "loss": 0.1784,
      "step": 26840
    },
    {
      "epoch": 1.6538343085925469,
      "grad_norm": 0.138319730758667,
      "learning_rate": 8.9781336618417e-05,
      "loss": 0.1772,
      "step": 26850
    },
    {
      "epoch": 1.6544502617801047,
      "grad_norm": 0.1420215219259262,
      "learning_rate": 8.974027307257981e-05,
      "loss": 0.1774,
      "step": 26860
    },
    {
      "epoch": 1.6550662149676625,
      "grad_norm": 0.17596355080604553,
      "learning_rate": 8.969920952674263e-05,
      "loss": 0.1766,
      "step": 26870
    },
    {
      "epoch": 1.65568216815522,
      "grad_norm": 0.14679747819900513,
      "learning_rate": 8.965814598090545e-05,
      "loss": 0.1775,
      "step": 26880
    },
    {
      "epoch": 1.656298121342778,
      "grad_norm": 0.1723630428314209,
      "learning_rate": 8.961708243506827e-05,
      "loss": 0.1791,
      "step": 26890
    },
    {
      "epoch": 1.6569140745303357,
      "grad_norm": 0.19284938275814056,
      "learning_rate": 8.957601888923109e-05,
      "loss": 0.1764,
      "step": 26900
    },
    {
      "epoch": 1.6575300277178935,
      "grad_norm": 0.14988642930984497,
      "learning_rate": 8.953495534339391e-05,
      "loss": 0.1772,
      "step": 26910
    },
    {
      "epoch": 1.6581459809054513,
      "grad_norm": 0.1632418930530548,
      "learning_rate": 8.949389179755673e-05,
      "loss": 0.1781,
      "step": 26920
    },
    {
      "epoch": 1.6587619340930089,
      "grad_norm": 0.13391296565532684,
      "learning_rate": 8.945282825171954e-05,
      "loss": 0.1774,
      "step": 26930
    },
    {
      "epoch": 1.6593778872805667,
      "grad_norm": 0.14409682154655457,
      "learning_rate": 8.941176470588236e-05,
      "loss": 0.1774,
      "step": 26940
    },
    {
      "epoch": 1.6599938404681245,
      "grad_norm": 0.15253795683383942,
      "learning_rate": 8.937070116004518e-05,
      "loss": 0.1782,
      "step": 26950
    },
    {
      "epoch": 1.660609793655682,
      "grad_norm": 0.14937379956245422,
      "learning_rate": 8.9329637614208e-05,
      "loss": 0.1772,
      "step": 26960
    },
    {
      "epoch": 1.6612257468432399,
      "grad_norm": 0.15163439512252808,
      "learning_rate": 8.92885740683708e-05,
      "loss": 0.178,
      "step": 26970
    },
    {
      "epoch": 1.6618417000307977,
      "grad_norm": 0.14895877242088318,
      "learning_rate": 8.924751052253363e-05,
      "loss": 0.1778,
      "step": 26980
    },
    {
      "epoch": 1.6624576532183553,
      "grad_norm": 0.1576050966978073,
      "learning_rate": 8.920644697669645e-05,
      "loss": 0.178,
      "step": 26990
    },
    {
      "epoch": 1.6630736064059133,
      "grad_norm": 0.14044316112995148,
      "learning_rate": 8.916538343085927e-05,
      "loss": 0.1754,
      "step": 27000
    },
    {
      "epoch": 1.6636895595934709,
      "grad_norm": 0.15464583039283752,
      "learning_rate": 8.912431988502207e-05,
      "loss": 0.1756,
      "step": 27010
    },
    {
      "epoch": 1.6643055127810287,
      "grad_norm": 0.16621437668800354,
      "learning_rate": 8.908325633918489e-05,
      "loss": 0.1794,
      "step": 27020
    },
    {
      "epoch": 1.6649214659685865,
      "grad_norm": 0.1556561440229416,
      "learning_rate": 8.904219279334771e-05,
      "loss": 0.1754,
      "step": 27030
    },
    {
      "epoch": 1.665537419156144,
      "grad_norm": 0.16700881719589233,
      "learning_rate": 8.900112924751053e-05,
      "loss": 0.1814,
      "step": 27040
    },
    {
      "epoch": 1.6661533723437019,
      "grad_norm": 0.18723586201667786,
      "learning_rate": 8.896006570167335e-05,
      "loss": 0.1749,
      "step": 27050
    },
    {
      "epoch": 1.6667693255312597,
      "grad_norm": 0.1539442092180252,
      "learning_rate": 8.891900215583616e-05,
      "loss": 0.1766,
      "step": 27060
    },
    {
      "epoch": 1.6673852787188173,
      "grad_norm": 0.1635078489780426,
      "learning_rate": 8.887793860999898e-05,
      "loss": 0.1778,
      "step": 27070
    },
    {
      "epoch": 1.6680012319063753,
      "grad_norm": 0.16797830164432526,
      "learning_rate": 8.88368750641618e-05,
      "loss": 0.1776,
      "step": 27080
    },
    {
      "epoch": 1.6686171850939329,
      "grad_norm": 0.17074833810329437,
      "learning_rate": 8.879581151832462e-05,
      "loss": 0.1787,
      "step": 27090
    },
    {
      "epoch": 1.6692331382814904,
      "grad_norm": 0.1661996692419052,
      "learning_rate": 8.875474797248743e-05,
      "loss": 0.1763,
      "step": 27100
    },
    {
      "epoch": 1.6698490914690485,
      "grad_norm": 0.1663377285003662,
      "learning_rate": 8.871368442665025e-05,
      "loss": 0.1786,
      "step": 27110
    },
    {
      "epoch": 1.670465044656606,
      "grad_norm": 0.1622568964958191,
      "learning_rate": 8.867262088081307e-05,
      "loss": 0.1762,
      "step": 27120
    },
    {
      "epoch": 1.6710809978441639,
      "grad_norm": 0.17004550993442535,
      "learning_rate": 8.863155733497589e-05,
      "loss": 0.1769,
      "step": 27130
    },
    {
      "epoch": 1.6716969510317217,
      "grad_norm": 0.16161707043647766,
      "learning_rate": 8.859049378913869e-05,
      "loss": 0.1765,
      "step": 27140
    },
    {
      "epoch": 1.6723129042192793,
      "grad_norm": 0.1632845550775528,
      "learning_rate": 8.854943024330151e-05,
      "loss": 0.1779,
      "step": 27150
    },
    {
      "epoch": 1.672928857406837,
      "grad_norm": 0.1667526811361313,
      "learning_rate": 8.850836669746433e-05,
      "loss": 0.1783,
      "step": 27160
    },
    {
      "epoch": 1.6735448105943949,
      "grad_norm": 0.15771515667438507,
      "learning_rate": 8.846730315162715e-05,
      "loss": 0.1768,
      "step": 27170
    },
    {
      "epoch": 1.6741607637819524,
      "grad_norm": 0.1597369760274887,
      "learning_rate": 8.842623960578996e-05,
      "loss": 0.1784,
      "step": 27180
    },
    {
      "epoch": 1.6747767169695105,
      "grad_norm": 0.16558416187763214,
      "learning_rate": 8.838517605995278e-05,
      "loss": 0.178,
      "step": 27190
    },
    {
      "epoch": 1.675392670157068,
      "grad_norm": 0.15154995024204254,
      "learning_rate": 8.83441125141156e-05,
      "loss": 0.1758,
      "step": 27200
    },
    {
      "epoch": 1.6760086233446259,
      "grad_norm": 0.16658692061901093,
      "learning_rate": 8.830304896827842e-05,
      "loss": 0.1762,
      "step": 27210
    },
    {
      "epoch": 1.6766245765321837,
      "grad_norm": 0.1614973098039627,
      "learning_rate": 8.826198542244122e-05,
      "loss": 0.1775,
      "step": 27220
    },
    {
      "epoch": 1.6772405297197412,
      "grad_norm": 0.16587600111961365,
      "learning_rate": 8.822092187660404e-05,
      "loss": 0.1785,
      "step": 27230
    },
    {
      "epoch": 1.677856482907299,
      "grad_norm": 0.13080459833145142,
      "learning_rate": 8.817985833076686e-05,
      "loss": 0.1784,
      "step": 27240
    },
    {
      "epoch": 1.6784724360948569,
      "grad_norm": 0.15575815737247467,
      "learning_rate": 8.813879478492968e-05,
      "loss": 0.1768,
      "step": 27250
    },
    {
      "epoch": 1.6790883892824144,
      "grad_norm": 0.14161236584186554,
      "learning_rate": 8.809773123909249e-05,
      "loss": 0.1775,
      "step": 27260
    },
    {
      "epoch": 1.6797043424699722,
      "grad_norm": 0.14620238542556763,
      "learning_rate": 8.805666769325531e-05,
      "loss": 0.177,
      "step": 27270
    },
    {
      "epoch": 1.68032029565753,
      "grad_norm": 0.24761131405830383,
      "learning_rate": 8.801560414741813e-05,
      "loss": 0.1779,
      "step": 27280
    },
    {
      "epoch": 1.6809362488450876,
      "grad_norm": 0.14285697042942047,
      "learning_rate": 8.797454060158095e-05,
      "loss": 0.1765,
      "step": 27290
    },
    {
      "epoch": 1.6815522020326457,
      "grad_norm": 0.13977208733558655,
      "learning_rate": 8.793347705574376e-05,
      "loss": 0.1765,
      "step": 27300
    },
    {
      "epoch": 1.6821681552202032,
      "grad_norm": 0.15364359319210052,
      "learning_rate": 8.789241350990658e-05,
      "loss": 0.179,
      "step": 27310
    },
    {
      "epoch": 1.682784108407761,
      "grad_norm": 0.1418323665857315,
      "learning_rate": 8.785134996406941e-05,
      "loss": 0.1769,
      "step": 27320
    },
    {
      "epoch": 1.6834000615953189,
      "grad_norm": 0.16151124238967896,
      "learning_rate": 8.781028641823222e-05,
      "loss": 0.178,
      "step": 27330
    },
    {
      "epoch": 1.6840160147828764,
      "grad_norm": 0.15091274678707123,
      "learning_rate": 8.776922287239504e-05,
      "loss": 0.1744,
      "step": 27340
    },
    {
      "epoch": 1.6846319679704342,
      "grad_norm": 0.1650048792362213,
      "learning_rate": 8.772815932655786e-05,
      "loss": 0.1776,
      "step": 27350
    },
    {
      "epoch": 1.685247921157992,
      "grad_norm": 0.17961128056049347,
      "learning_rate": 8.768709578072068e-05,
      "loss": 0.1791,
      "step": 27360
    },
    {
      "epoch": 1.6858638743455496,
      "grad_norm": 0.1591731458902359,
      "learning_rate": 8.764603223488348e-05,
      "loss": 0.1821,
      "step": 27370
    },
    {
      "epoch": 1.6864798275331074,
      "grad_norm": 0.17354802787303925,
      "learning_rate": 8.76049686890463e-05,
      "loss": 0.1768,
      "step": 27380
    },
    {
      "epoch": 1.6870957807206652,
      "grad_norm": 0.1621553599834442,
      "learning_rate": 8.756390514320912e-05,
      "loss": 0.1781,
      "step": 27390
    },
    {
      "epoch": 1.6877117339082228,
      "grad_norm": 0.1513974815607071,
      "learning_rate": 8.752284159737194e-05,
      "loss": 0.1785,
      "step": 27400
    },
    {
      "epoch": 1.6883276870957808,
      "grad_norm": 0.15322014689445496,
      "learning_rate": 8.748177805153475e-05,
      "loss": 0.1796,
      "step": 27410
    },
    {
      "epoch": 1.6889436402833384,
      "grad_norm": 0.15975125133991241,
      "learning_rate": 8.744071450569757e-05,
      "loss": 0.1774,
      "step": 27420
    },
    {
      "epoch": 1.6895595934708962,
      "grad_norm": 0.16111360490322113,
      "learning_rate": 8.739965095986039e-05,
      "loss": 0.177,
      "step": 27430
    },
    {
      "epoch": 1.690175546658454,
      "grad_norm": 0.16031119227409363,
      "learning_rate": 8.735858741402321e-05,
      "loss": 0.1789,
      "step": 27440
    },
    {
      "epoch": 1.6907914998460116,
      "grad_norm": 0.1639954298734665,
      "learning_rate": 8.731752386818602e-05,
      "loss": 0.1767,
      "step": 27450
    },
    {
      "epoch": 1.6914074530335694,
      "grad_norm": 0.15887145698070526,
      "learning_rate": 8.727646032234884e-05,
      "loss": 0.1767,
      "step": 27460
    },
    {
      "epoch": 1.6920234062211272,
      "grad_norm": 0.1481044739484787,
      "learning_rate": 8.723539677651166e-05,
      "loss": 0.1786,
      "step": 27470
    },
    {
      "epoch": 1.6926393594086848,
      "grad_norm": 0.17033855617046356,
      "learning_rate": 8.719433323067448e-05,
      "loss": 0.1778,
      "step": 27480
    },
    {
      "epoch": 1.6932553125962428,
      "grad_norm": 0.13252224028110504,
      "learning_rate": 8.715326968483728e-05,
      "loss": 0.1787,
      "step": 27490
    },
    {
      "epoch": 1.6938712657838004,
      "grad_norm": 0.14278115332126617,
      "learning_rate": 8.71122061390001e-05,
      "loss": 0.1767,
      "step": 27500
    },
    {
      "epoch": 1.694487218971358,
      "grad_norm": 0.13755665719509125,
      "learning_rate": 8.707114259316292e-05,
      "loss": 0.176,
      "step": 27510
    },
    {
      "epoch": 1.695103172158916,
      "grad_norm": 0.16505487263202667,
      "learning_rate": 8.703007904732574e-05,
      "loss": 0.1773,
      "step": 27520
    },
    {
      "epoch": 1.6957191253464736,
      "grad_norm": 0.22317901253700256,
      "learning_rate": 8.698901550148855e-05,
      "loss": 0.1781,
      "step": 27530
    },
    {
      "epoch": 1.6963350785340314,
      "grad_norm": 0.15914054214954376,
      "learning_rate": 8.694795195565137e-05,
      "loss": 0.1789,
      "step": 27540
    },
    {
      "epoch": 1.6969510317215892,
      "grad_norm": 0.1382189840078354,
      "learning_rate": 8.690688840981419e-05,
      "loss": 0.1771,
      "step": 27550
    },
    {
      "epoch": 1.6975669849091468,
      "grad_norm": 0.14233551919460297,
      "learning_rate": 8.686582486397701e-05,
      "loss": 0.176,
      "step": 27560
    },
    {
      "epoch": 1.6981829380967046,
      "grad_norm": 0.14878331124782562,
      "learning_rate": 8.682476131813982e-05,
      "loss": 0.1779,
      "step": 27570
    },
    {
      "epoch": 1.6987988912842624,
      "grad_norm": 0.14662353694438934,
      "learning_rate": 8.678369777230264e-05,
      "loss": 0.178,
      "step": 27580
    },
    {
      "epoch": 1.69941484447182,
      "grad_norm": 0.14747211337089539,
      "learning_rate": 8.674263422646546e-05,
      "loss": 0.1733,
      "step": 27590
    },
    {
      "epoch": 1.700030797659378,
      "grad_norm": 0.1518872231245041,
      "learning_rate": 8.6705677035212e-05,
      "loss": 0.1783,
      "step": 27600
    },
    {
      "epoch": 1.7006467508469356,
      "grad_norm": 0.1493661105632782,
      "learning_rate": 8.666461348937482e-05,
      "loss": 0.1779,
      "step": 27610
    },
    {
      "epoch": 1.7012627040344934,
      "grad_norm": 0.14176416397094727,
      "learning_rate": 8.662354994353764e-05,
      "loss": 0.1769,
      "step": 27620
    },
    {
      "epoch": 1.7018786572220512,
      "grad_norm": 0.15451756119728088,
      "learning_rate": 8.658659275228417e-05,
      "loss": 0.176,
      "step": 27630
    },
    {
      "epoch": 1.7024946104096088,
      "grad_norm": 0.13460925221443176,
      "learning_rate": 8.654552920644698e-05,
      "loss": 0.1778,
      "step": 27640
    },
    {
      "epoch": 1.7031105635971666,
      "grad_norm": 0.14453861117362976,
      "learning_rate": 8.65044656606098e-05,
      "loss": 0.1756,
      "step": 27650
    },
    {
      "epoch": 1.7037265167847244,
      "grad_norm": 0.17730934917926788,
      "learning_rate": 8.646340211477262e-05,
      "loss": 0.178,
      "step": 27660
    },
    {
      "epoch": 1.704342469972282,
      "grad_norm": 0.15908770263195038,
      "learning_rate": 8.642233856893544e-05,
      "loss": 0.1782,
      "step": 27670
    },
    {
      "epoch": 1.7049584231598398,
      "grad_norm": 0.14666810631752014,
      "learning_rate": 8.638127502309825e-05,
      "loss": 0.1758,
      "step": 27680
    },
    {
      "epoch": 1.7055743763473976,
      "grad_norm": 0.15415312349796295,
      "learning_rate": 8.634021147726107e-05,
      "loss": 0.176,
      "step": 27690
    },
    {
      "epoch": 1.7061903295349552,
      "grad_norm": 0.14814138412475586,
      "learning_rate": 8.629914793142389e-05,
      "loss": 0.1778,
      "step": 27700
    },
    {
      "epoch": 1.7068062827225132,
      "grad_norm": 0.16234906017780304,
      "learning_rate": 8.625808438558671e-05,
      "loss": 0.1801,
      "step": 27710
    },
    {
      "epoch": 1.7074222359100708,
      "grad_norm": 0.1478775441646576,
      "learning_rate": 8.621702083974951e-05,
      "loss": 0.1757,
      "step": 27720
    },
    {
      "epoch": 1.7080381890976286,
      "grad_norm": 0.16253915429115295,
      "learning_rate": 8.617595729391233e-05,
      "loss": 0.1758,
      "step": 27730
    },
    {
      "epoch": 1.7086541422851864,
      "grad_norm": 0.1455816775560379,
      "learning_rate": 8.613489374807515e-05,
      "loss": 0.1796,
      "step": 27740
    },
    {
      "epoch": 1.709270095472744,
      "grad_norm": 0.1455967128276825,
      "learning_rate": 8.609383020223797e-05,
      "loss": 0.1753,
      "step": 27750
    },
    {
      "epoch": 1.7098860486603018,
      "grad_norm": 0.14654208719730377,
      "learning_rate": 8.605276665640078e-05,
      "loss": 0.1764,
      "step": 27760
    },
    {
      "epoch": 1.7105020018478596,
      "grad_norm": 0.16210538148880005,
      "learning_rate": 8.60117031105636e-05,
      "loss": 0.1791,
      "step": 27770
    },
    {
      "epoch": 1.7111179550354172,
      "grad_norm": 0.1657765656709671,
      "learning_rate": 8.597063956472642e-05,
      "loss": 0.1798,
      "step": 27780
    },
    {
      "epoch": 1.7117339082229752,
      "grad_norm": 0.12991710007190704,
      "learning_rate": 8.592957601888924e-05,
      "loss": 0.1757,
      "step": 27790
    },
    {
      "epoch": 1.7123498614105328,
      "grad_norm": 0.1286577582359314,
      "learning_rate": 8.588851247305205e-05,
      "loss": 0.1788,
      "step": 27800
    },
    {
      "epoch": 1.7129658145980904,
      "grad_norm": 0.13755986094474792,
      "learning_rate": 8.584744892721487e-05,
      "loss": 0.1777,
      "step": 27810
    },
    {
      "epoch": 1.7135817677856484,
      "grad_norm": 0.17650578916072845,
      "learning_rate": 8.580638538137769e-05,
      "loss": 0.1761,
      "step": 27820
    },
    {
      "epoch": 1.714197720973206,
      "grad_norm": 0.15979771316051483,
      "learning_rate": 8.576532183554051e-05,
      "loss": 0.1773,
      "step": 27830
    },
    {
      "epoch": 1.7148136741607638,
      "grad_norm": 0.1785837858915329,
      "learning_rate": 8.572425828970331e-05,
      "loss": 0.18,
      "step": 27840
    },
    {
      "epoch": 1.7154296273483216,
      "grad_norm": 0.16033032536506653,
      "learning_rate": 8.568319474386613e-05,
      "loss": 0.1777,
      "step": 27850
    },
    {
      "epoch": 1.7160455805358792,
      "grad_norm": 0.16300709545612335,
      "learning_rate": 8.564213119802895e-05,
      "loss": 0.1774,
      "step": 27860
    },
    {
      "epoch": 1.716661533723437,
      "grad_norm": 0.17479990422725677,
      "learning_rate": 8.560106765219177e-05,
      "loss": 0.1795,
      "step": 27870
    },
    {
      "epoch": 1.7172774869109948,
      "grad_norm": 0.17020754516124725,
      "learning_rate": 8.556000410635458e-05,
      "loss": 0.1767,
      "step": 27880
    },
    {
      "epoch": 1.7178934400985524,
      "grad_norm": 0.1663791984319687,
      "learning_rate": 8.55189405605174e-05,
      "loss": 0.1775,
      "step": 27890
    },
    {
      "epoch": 1.7185093932861104,
      "grad_norm": 0.1428074687719345,
      "learning_rate": 8.547787701468022e-05,
      "loss": 0.1749,
      "step": 27900
    },
    {
      "epoch": 1.719125346473668,
      "grad_norm": 0.15373264253139496,
      "learning_rate": 8.543681346884304e-05,
      "loss": 0.1761,
      "step": 27910
    },
    {
      "epoch": 1.7197412996612258,
      "grad_norm": 0.16284550726413727,
      "learning_rate": 8.539574992300585e-05,
      "loss": 0.1773,
      "step": 27920
    },
    {
      "epoch": 1.7203572528487836,
      "grad_norm": 0.13456189632415771,
      "learning_rate": 8.535468637716867e-05,
      "loss": 0.176,
      "step": 27930
    },
    {
      "epoch": 1.7209732060363412,
      "grad_norm": 0.17923282086849213,
      "learning_rate": 8.531362283133149e-05,
      "loss": 0.1761,
      "step": 27940
    },
    {
      "epoch": 1.721589159223899,
      "grad_norm": 0.14936231076717377,
      "learning_rate": 8.52725592854943e-05,
      "loss": 0.1766,
      "step": 27950
    },
    {
      "epoch": 1.7222051124114568,
      "grad_norm": 0.15837416052818298,
      "learning_rate": 8.523149573965711e-05,
      "loss": 0.1766,
      "step": 27960
    },
    {
      "epoch": 1.7228210655990144,
      "grad_norm": 0.1445537507534027,
      "learning_rate": 8.519043219381993e-05,
      "loss": 0.1779,
      "step": 27970
    },
    {
      "epoch": 1.7234370187865722,
      "grad_norm": 0.17669248580932617,
      "learning_rate": 8.514936864798275e-05,
      "loss": 0.1775,
      "step": 27980
    },
    {
      "epoch": 1.72405297197413,
      "grad_norm": 0.15043020248413086,
      "learning_rate": 8.510830510214557e-05,
      "loss": 0.1774,
      "step": 27990
    },
    {
      "epoch": 1.7246689251616876,
      "grad_norm": 0.16458135843276978,
      "learning_rate": 8.506724155630838e-05,
      "loss": 0.1764,
      "step": 28000
    },
    {
      "epoch": 1.7252848783492456,
      "grad_norm": 0.1812794953584671,
      "learning_rate": 8.502617801047121e-05,
      "loss": 0.1767,
      "step": 28010
    },
    {
      "epoch": 1.7259008315368032,
      "grad_norm": 0.14014071226119995,
      "learning_rate": 8.498511446463403e-05,
      "loss": 0.1784,
      "step": 28020
    },
    {
      "epoch": 1.726516784724361,
      "grad_norm": 0.15879909694194794,
      "learning_rate": 8.494405091879685e-05,
      "loss": 0.1759,
      "step": 28030
    },
    {
      "epoch": 1.7271327379119188,
      "grad_norm": 0.15855996310710907,
      "learning_rate": 8.490298737295966e-05,
      "loss": 0.1751,
      "step": 28040
    },
    {
      "epoch": 1.7277486910994764,
      "grad_norm": 0.14559151232242584,
      "learning_rate": 8.486192382712248e-05,
      "loss": 0.1758,
      "step": 28050
    },
    {
      "epoch": 1.7283646442870342,
      "grad_norm": 0.1552676558494568,
      "learning_rate": 8.48208602812853e-05,
      "loss": 0.1797,
      "step": 28060
    },
    {
      "epoch": 1.728980597474592,
      "grad_norm": 0.17328082025051117,
      "learning_rate": 8.477979673544812e-05,
      "loss": 0.178,
      "step": 28070
    },
    {
      "epoch": 1.7295965506621496,
      "grad_norm": 0.16108830273151398,
      "learning_rate": 8.473873318961093e-05,
      "loss": 0.1753,
      "step": 28080
    },
    {
      "epoch": 1.7302125038497074,
      "grad_norm": 0.13888472318649292,
      "learning_rate": 8.469766964377375e-05,
      "loss": 0.1776,
      "step": 28090
    },
    {
      "epoch": 1.7308284570372652,
      "grad_norm": 0.15376897156238556,
      "learning_rate": 8.465660609793657e-05,
      "loss": 0.1771,
      "step": 28100
    },
    {
      "epoch": 1.7314444102248228,
      "grad_norm": 0.12724579870700836,
      "learning_rate": 8.461554255209939e-05,
      "loss": 0.1769,
      "step": 28110
    },
    {
      "epoch": 1.7320603634123808,
      "grad_norm": 0.15824735164642334,
      "learning_rate": 8.457447900626219e-05,
      "loss": 0.1762,
      "step": 28120
    },
    {
      "epoch": 1.7326763165999384,
      "grad_norm": 0.15509094297885895,
      "learning_rate": 8.453341546042501e-05,
      "loss": 0.177,
      "step": 28130
    },
    {
      "epoch": 1.7332922697874962,
      "grad_norm": 0.14258120954036713,
      "learning_rate": 8.449235191458783e-05,
      "loss": 0.1785,
      "step": 28140
    },
    {
      "epoch": 1.733908222975054,
      "grad_norm": 0.18835000693798065,
      "learning_rate": 8.445128836875065e-05,
      "loss": 0.1785,
      "step": 28150
    },
    {
      "epoch": 1.7345241761626116,
      "grad_norm": 0.1543918251991272,
      "learning_rate": 8.441022482291346e-05,
      "loss": 0.1757,
      "step": 28160
    },
    {
      "epoch": 1.7351401293501694,
      "grad_norm": 0.137336865067482,
      "learning_rate": 8.436916127707628e-05,
      "loss": 0.1789,
      "step": 28170
    },
    {
      "epoch": 1.7357560825377272,
      "grad_norm": 0.12200959026813507,
      "learning_rate": 8.43280977312391e-05,
      "loss": 0.1762,
      "step": 28180
    },
    {
      "epoch": 1.7363720357252848,
      "grad_norm": 0.13664186000823975,
      "learning_rate": 8.428703418540192e-05,
      "loss": 0.1791,
      "step": 28190
    },
    {
      "epoch": 1.7369879889128428,
      "grad_norm": 0.14886292815208435,
      "learning_rate": 8.424597063956473e-05,
      "loss": 0.1765,
      "step": 28200
    },
    {
      "epoch": 1.7376039421004004,
      "grad_norm": 0.168321430683136,
      "learning_rate": 8.420490709372755e-05,
      "loss": 0.1769,
      "step": 28210
    },
    {
      "epoch": 1.738219895287958,
      "grad_norm": 0.1489909142255783,
      "learning_rate": 8.416384354789037e-05,
      "loss": 0.1774,
      "step": 28220
    },
    {
      "epoch": 1.738835848475516,
      "grad_norm": 0.16233369708061218,
      "learning_rate": 8.412278000205319e-05,
      "loss": 0.176,
      "step": 28230
    },
    {
      "epoch": 1.7394518016630736,
      "grad_norm": 0.18115319311618805,
      "learning_rate": 8.408171645621599e-05,
      "loss": 0.1785,
      "step": 28240
    },
    {
      "epoch": 1.7400677548506314,
      "grad_norm": 0.13359135389328003,
      "learning_rate": 8.404065291037881e-05,
      "loss": 0.1748,
      "step": 28250
    },
    {
      "epoch": 1.7406837080381892,
      "grad_norm": 0.16180852055549622,
      "learning_rate": 8.399958936454163e-05,
      "loss": 0.1782,
      "step": 28260
    },
    {
      "epoch": 1.7412996612257468,
      "grad_norm": 0.15201342105865479,
      "learning_rate": 8.395852581870445e-05,
      "loss": 0.1781,
      "step": 28270
    },
    {
      "epoch": 1.7419156144133046,
      "grad_norm": 0.14894454181194305,
      "learning_rate": 8.391746227286726e-05,
      "loss": 0.1786,
      "step": 28280
    },
    {
      "epoch": 1.7425315676008624,
      "grad_norm": 0.15522554516792297,
      "learning_rate": 8.387639872703008e-05,
      "loss": 0.1785,
      "step": 28290
    },
    {
      "epoch": 1.74314752078842,
      "grad_norm": 0.13709424436092377,
      "learning_rate": 8.38353351811929e-05,
      "loss": 0.1769,
      "step": 28300
    },
    {
      "epoch": 1.743763473975978,
      "grad_norm": 0.15185613930225372,
      "learning_rate": 8.379427163535572e-05,
      "loss": 0.1762,
      "step": 28310
    },
    {
      "epoch": 1.7443794271635356,
      "grad_norm": 0.13687875866889954,
      "learning_rate": 8.375320808951853e-05,
      "loss": 0.1766,
      "step": 28320
    },
    {
      "epoch": 1.7449953803510934,
      "grad_norm": 0.1335165798664093,
      "learning_rate": 8.371214454368135e-05,
      "loss": 0.1763,
      "step": 28330
    },
    {
      "epoch": 1.7456113335386512,
      "grad_norm": 0.14892145991325378,
      "learning_rate": 8.367108099784417e-05,
      "loss": 0.1769,
      "step": 28340
    },
    {
      "epoch": 1.7462272867262087,
      "grad_norm": 0.15914322435855865,
      "learning_rate": 8.363001745200699e-05,
      "loss": 0.1791,
      "step": 28350
    },
    {
      "epoch": 1.7468432399137666,
      "grad_norm": 0.14607805013656616,
      "learning_rate": 8.358895390616979e-05,
      "loss": 0.1762,
      "step": 28360
    },
    {
      "epoch": 1.7474591931013244,
      "grad_norm": 0.14565159380435944,
      "learning_rate": 8.354789036033261e-05,
      "loss": 0.1781,
      "step": 28370
    },
    {
      "epoch": 1.748075146288882,
      "grad_norm": 0.13362036645412445,
      "learning_rate": 8.350682681449543e-05,
      "loss": 0.1768,
      "step": 28380
    },
    {
      "epoch": 1.7486910994764397,
      "grad_norm": 0.14887121319770813,
      "learning_rate": 8.346576326865825e-05,
      "loss": 0.1769,
      "step": 28390
    },
    {
      "epoch": 1.7493070526639976,
      "grad_norm": 0.1527874916791916,
      "learning_rate": 8.342469972282106e-05,
      "loss": 0.1754,
      "step": 28400
    },
    {
      "epoch": 1.7499230058515551,
      "grad_norm": 0.14828717708587646,
      "learning_rate": 8.338363617698388e-05,
      "loss": 0.1776,
      "step": 28410
    },
    {
      "epoch": 1.7505389590391132,
      "grad_norm": 0.16063420474529266,
      "learning_rate": 8.33425726311467e-05,
      "loss": 0.1779,
      "step": 28420
    },
    {
      "epoch": 1.7511549122266707,
      "grad_norm": 0.16757164895534515,
      "learning_rate": 8.330150908530952e-05,
      "loss": 0.1767,
      "step": 28430
    },
    {
      "epoch": 1.7517708654142286,
      "grad_norm": 0.16341440379619598,
      "learning_rate": 8.326044553947234e-05,
      "loss": 0.1779,
      "step": 28440
    },
    {
      "epoch": 1.7523868186017864,
      "grad_norm": 0.15444764494895935,
      "learning_rate": 8.321938199363516e-05,
      "loss": 0.1757,
      "step": 28450
    },
    {
      "epoch": 1.753002771789344,
      "grad_norm": 0.15436571836471558,
      "learning_rate": 8.317831844779798e-05,
      "loss": 0.1768,
      "step": 28460
    },
    {
      "epoch": 1.7536187249769017,
      "grad_norm": 0.1440030187368393,
      "learning_rate": 8.313725490196079e-05,
      "loss": 0.1762,
      "step": 28470
    },
    {
      "epoch": 1.7542346781644595,
      "grad_norm": 0.1692885458469391,
      "learning_rate": 8.30961913561236e-05,
      "loss": 0.1756,
      "step": 28480
    },
    {
      "epoch": 1.7548506313520171,
      "grad_norm": 0.16050677001476288,
      "learning_rate": 8.305512781028643e-05,
      "loss": 0.1742,
      "step": 28490
    },
    {
      "epoch": 1.755466584539575,
      "grad_norm": 0.1775323748588562,
      "learning_rate": 8.301406426444925e-05,
      "loss": 0.1748,
      "step": 28500
    },
    {
      "epoch": 1.7560825377271327,
      "grad_norm": 0.14725123345851898,
      "learning_rate": 8.297300071861207e-05,
      "loss": 0.1782,
      "step": 28510
    },
    {
      "epoch": 1.7566984909146903,
      "grad_norm": 0.1670393943786621,
      "learning_rate": 8.293193717277487e-05,
      "loss": 0.1759,
      "step": 28520
    },
    {
      "epoch": 1.7573144441022484,
      "grad_norm": 0.16340114176273346,
      "learning_rate": 8.289087362693769e-05,
      "loss": 0.1765,
      "step": 28530
    },
    {
      "epoch": 1.757930397289806,
      "grad_norm": 0.18149229884147644,
      "learning_rate": 8.284981008110051e-05,
      "loss": 0.1778,
      "step": 28540
    },
    {
      "epoch": 1.7585463504773637,
      "grad_norm": 0.1486237496137619,
      "learning_rate": 8.280874653526333e-05,
      "loss": 0.1764,
      "step": 28550
    },
    {
      "epoch": 1.7591623036649215,
      "grad_norm": 0.15408435463905334,
      "learning_rate": 8.276768298942614e-05,
      "loss": 0.1768,
      "step": 28560
    },
    {
      "epoch": 1.7597782568524791,
      "grad_norm": 0.17050443589687347,
      "learning_rate": 8.272661944358896e-05,
      "loss": 0.1772,
      "step": 28570
    },
    {
      "epoch": 1.760394210040037,
      "grad_norm": 0.17074577510356903,
      "learning_rate": 8.268555589775178e-05,
      "loss": 0.1781,
      "step": 28580
    },
    {
      "epoch": 1.7610101632275947,
      "grad_norm": 0.13500919938087463,
      "learning_rate": 8.26444923519146e-05,
      "loss": 0.1765,
      "step": 28590
    },
    {
      "epoch": 1.7616261164151523,
      "grad_norm": 0.1627226322889328,
      "learning_rate": 8.26034288060774e-05,
      "loss": 0.1745,
      "step": 28600
    },
    {
      "epoch": 1.7622420696027103,
      "grad_norm": 0.1619841307401657,
      "learning_rate": 8.256236526024023e-05,
      "loss": 0.1786,
      "step": 28610
    },
    {
      "epoch": 1.762858022790268,
      "grad_norm": 0.159348264336586,
      "learning_rate": 8.252130171440305e-05,
      "loss": 0.1776,
      "step": 28620
    },
    {
      "epoch": 1.7634739759778257,
      "grad_norm": 0.16847111284732819,
      "learning_rate": 8.248023816856587e-05,
      "loss": 0.1761,
      "step": 28630
    },
    {
      "epoch": 1.7640899291653835,
      "grad_norm": 0.1480887532234192,
      "learning_rate": 8.243917462272867e-05,
      "loss": 0.1763,
      "step": 28640
    },
    {
      "epoch": 1.7647058823529411,
      "grad_norm": 0.17566080391407013,
      "learning_rate": 8.239811107689149e-05,
      "loss": 0.1763,
      "step": 28650
    },
    {
      "epoch": 1.765321835540499,
      "grad_norm": 0.15125024318695068,
      "learning_rate": 8.235704753105431e-05,
      "loss": 0.1782,
      "step": 28660
    },
    {
      "epoch": 1.7659377887280567,
      "grad_norm": 0.14379306137561798,
      "learning_rate": 8.231598398521713e-05,
      "loss": 0.1774,
      "step": 28670
    },
    {
      "epoch": 1.7665537419156143,
      "grad_norm": 0.15478548407554626,
      "learning_rate": 8.227492043937994e-05,
      "loss": 0.178,
      "step": 28680
    },
    {
      "epoch": 1.7671696951031721,
      "grad_norm": 0.15240740776062012,
      "learning_rate": 8.223385689354276e-05,
      "loss": 0.1775,
      "step": 28690
    },
    {
      "epoch": 1.76778564829073,
      "grad_norm": 0.1523515284061432,
      "learning_rate": 8.219279334770558e-05,
      "loss": 0.1773,
      "step": 28700
    },
    {
      "epoch": 1.7684016014782875,
      "grad_norm": 0.18905894458293915,
      "learning_rate": 8.21517298018684e-05,
      "loss": 0.1771,
      "step": 28710
    },
    {
      "epoch": 1.7690175546658455,
      "grad_norm": 0.14044298231601715,
      "learning_rate": 8.21106662560312e-05,
      "loss": 0.179,
      "step": 28720
    },
    {
      "epoch": 1.7696335078534031,
      "grad_norm": 0.16818609833717346,
      "learning_rate": 8.206960271019403e-05,
      "loss": 0.1796,
      "step": 28730
    },
    {
      "epoch": 1.770249461040961,
      "grad_norm": 0.14575450122356415,
      "learning_rate": 8.202853916435685e-05,
      "loss": 0.1782,
      "step": 28740
    },
    {
      "epoch": 1.7708654142285187,
      "grad_norm": 0.15506063401699066,
      "learning_rate": 8.198747561851967e-05,
      "loss": 0.1776,
      "step": 28750
    },
    {
      "epoch": 1.7714813674160763,
      "grad_norm": 0.15830694139003754,
      "learning_rate": 8.194641207268247e-05,
      "loss": 0.1771,
      "step": 28760
    },
    {
      "epoch": 1.7720973206036341,
      "grad_norm": 0.15689894556999207,
      "learning_rate": 8.190534852684529e-05,
      "loss": 0.1768,
      "step": 28770
    },
    {
      "epoch": 1.772713273791192,
      "grad_norm": 0.14617820084095,
      "learning_rate": 8.186428498100811e-05,
      "loss": 0.1749,
      "step": 28780
    },
    {
      "epoch": 1.7733292269787495,
      "grad_norm": 0.18410924077033997,
      "learning_rate": 8.182322143517093e-05,
      "loss": 0.1772,
      "step": 28790
    },
    {
      "epoch": 1.7739451801663073,
      "grad_norm": 0.13736410439014435,
      "learning_rate": 8.178215788933374e-05,
      "loss": 0.1762,
      "step": 28800
    },
    {
      "epoch": 1.7745611333538651,
      "grad_norm": 0.14621911942958832,
      "learning_rate": 8.174109434349656e-05,
      "loss": 0.1765,
      "step": 28810
    },
    {
      "epoch": 1.7751770865414227,
      "grad_norm": 0.17638437449932098,
      "learning_rate": 8.170003079765938e-05,
      "loss": 0.1792,
      "step": 28820
    },
    {
      "epoch": 1.7757930397289807,
      "grad_norm": 0.14229421317577362,
      "learning_rate": 8.16589672518222e-05,
      "loss": 0.1773,
      "step": 28830
    },
    {
      "epoch": 1.7764089929165383,
      "grad_norm": 0.16159404814243317,
      "learning_rate": 8.1617903705985e-05,
      "loss": 0.1774,
      "step": 28840
    },
    {
      "epoch": 1.7770249461040961,
      "grad_norm": 0.14146359264850616,
      "learning_rate": 8.157684016014783e-05,
      "loss": 0.1787,
      "step": 28850
    },
    {
      "epoch": 1.777640899291654,
      "grad_norm": 0.14274072647094727,
      "learning_rate": 8.153577661431066e-05,
      "loss": 0.1761,
      "step": 28860
    },
    {
      "epoch": 1.7782568524792115,
      "grad_norm": 0.24579615890979767,
      "learning_rate": 8.149471306847347e-05,
      "loss": 0.1795,
      "step": 28870
    },
    {
      "epoch": 1.7788728056667693,
      "grad_norm": 0.1630687713623047,
      "learning_rate": 8.145364952263629e-05,
      "loss": 0.1785,
      "step": 28880
    },
    {
      "epoch": 1.7794887588543271,
      "grad_norm": 0.13578225672245026,
      "learning_rate": 8.14125859767991e-05,
      "loss": 0.1779,
      "step": 28890
    },
    {
      "epoch": 1.7801047120418847,
      "grad_norm": 0.4659985303878784,
      "learning_rate": 8.137152243096193e-05,
      "loss": 0.1779,
      "step": 28900
    },
    {
      "epoch": 1.7807206652294427,
      "grad_norm": 0.14449501037597656,
      "learning_rate": 8.133045888512473e-05,
      "loss": 0.1757,
      "step": 28910
    },
    {
      "epoch": 1.7813366184170003,
      "grad_norm": 0.1682107299566269,
      "learning_rate": 8.128939533928755e-05,
      "loss": 0.1779,
      "step": 28920
    },
    {
      "epoch": 1.7819525716045579,
      "grad_norm": 0.18359984457492828,
      "learning_rate": 8.124833179345037e-05,
      "loss": 0.1785,
      "step": 28930
    },
    {
      "epoch": 1.782568524792116,
      "grad_norm": 0.14517652988433838,
      "learning_rate": 8.120726824761319e-05,
      "loss": 0.1759,
      "step": 28940
    },
    {
      "epoch": 1.7831844779796735,
      "grad_norm": 0.17740243673324585,
      "learning_rate": 8.1166204701776e-05,
      "loss": 0.1788,
      "step": 28950
    },
    {
      "epoch": 1.7838004311672313,
      "grad_norm": 0.16076751053333282,
      "learning_rate": 8.112514115593882e-05,
      "loss": 0.1812,
      "step": 28960
    },
    {
      "epoch": 1.784416384354789,
      "grad_norm": 0.14947879314422607,
      "learning_rate": 8.108407761010164e-05,
      "loss": 0.1779,
      "step": 28970
    },
    {
      "epoch": 1.7850323375423467,
      "grad_norm": 0.17031824588775635,
      "learning_rate": 8.104301406426446e-05,
      "loss": 0.1772,
      "step": 28980
    },
    {
      "epoch": 1.7856482907299045,
      "grad_norm": 0.1761416494846344,
      "learning_rate": 8.100195051842727e-05,
      "loss": 0.1796,
      "step": 28990
    },
    {
      "epoch": 1.7862642439174623,
      "grad_norm": 0.17872773110866547,
      "learning_rate": 8.096088697259009e-05,
      "loss": 0.1772,
      "step": 29000
    },
    {
      "epoch": 1.7868801971050199,
      "grad_norm": 0.20315241813659668,
      "learning_rate": 8.09198234267529e-05,
      "loss": 0.1776,
      "step": 29010
    },
    {
      "epoch": 1.787496150292578,
      "grad_norm": 0.17254208028316498,
      "learning_rate": 8.087875988091573e-05,
      "loss": 0.177,
      "step": 29020
    },
    {
      "epoch": 1.7881121034801355,
      "grad_norm": 0.17968033254146576,
      "learning_rate": 8.083769633507855e-05,
      "loss": 0.1779,
      "step": 29030
    },
    {
      "epoch": 1.7887280566676933,
      "grad_norm": 0.16242702305316925,
      "learning_rate": 8.079663278924135e-05,
      "loss": 0.1774,
      "step": 29040
    },
    {
      "epoch": 1.789344009855251,
      "grad_norm": 0.15700660645961761,
      "learning_rate": 8.075556924340417e-05,
      "loss": 0.1761,
      "step": 29050
    },
    {
      "epoch": 1.7899599630428087,
      "grad_norm": 0.16075269877910614,
      "learning_rate": 8.071450569756699e-05,
      "loss": 0.1798,
      "step": 29060
    },
    {
      "epoch": 1.7905759162303665,
      "grad_norm": 0.14745861291885376,
      "learning_rate": 8.067344215172981e-05,
      "loss": 0.177,
      "step": 29070
    },
    {
      "epoch": 1.7911918694179243,
      "grad_norm": 0.16143037378787994,
      "learning_rate": 8.063237860589262e-05,
      "loss": 0.1755,
      "step": 29080
    },
    {
      "epoch": 1.7918078226054819,
      "grad_norm": 0.1832796037197113,
      "learning_rate": 8.059131506005544e-05,
      "loss": 0.1775,
      "step": 29090
    },
    {
      "epoch": 1.7924237757930397,
      "grad_norm": 0.15461944043636322,
      "learning_rate": 8.055025151421826e-05,
      "loss": 0.1755,
      "step": 29100
    },
    {
      "epoch": 1.7930397289805975,
      "grad_norm": 0.1642896980047226,
      "learning_rate": 8.050918796838108e-05,
      "loss": 0.1764,
      "step": 29110
    },
    {
      "epoch": 1.793655682168155,
      "grad_norm": 0.15315242111682892,
      "learning_rate": 8.046812442254389e-05,
      "loss": 0.1774,
      "step": 29120
    },
    {
      "epoch": 1.794271635355713,
      "grad_norm": 0.14368487894535065,
      "learning_rate": 8.04270608767067e-05,
      "loss": 0.1772,
      "step": 29130
    },
    {
      "epoch": 1.7948875885432707,
      "grad_norm": 0.1477176696062088,
      "learning_rate": 8.038599733086953e-05,
      "loss": 0.1775,
      "step": 29140
    },
    {
      "epoch": 1.7955035417308285,
      "grad_norm": 0.1716843992471695,
      "learning_rate": 8.034493378503235e-05,
      "loss": 0.1774,
      "step": 29150
    },
    {
      "epoch": 1.7961194949183863,
      "grad_norm": 0.15503990650177002,
      "learning_rate": 8.030387023919515e-05,
      "loss": 0.1775,
      "step": 29160
    },
    {
      "epoch": 1.7967354481059439,
      "grad_norm": 0.1252991259098053,
      "learning_rate": 8.026280669335797e-05,
      "loss": 0.1747,
      "step": 29170
    },
    {
      "epoch": 1.7973514012935017,
      "grad_norm": 0.17646831274032593,
      "learning_rate": 8.022174314752079e-05,
      "loss": 0.177,
      "step": 29180
    },
    {
      "epoch": 1.7979673544810595,
      "grad_norm": 0.16429316997528076,
      "learning_rate": 8.018067960168361e-05,
      "loss": 0.179,
      "step": 29190
    },
    {
      "epoch": 1.798583307668617,
      "grad_norm": 0.1614023596048355,
      "learning_rate": 8.013961605584642e-05,
      "loss": 0.1788,
      "step": 29200
    },
    {
      "epoch": 1.7991992608561749,
      "grad_norm": 0.14143940806388855,
      "learning_rate": 8.009855251000924e-05,
      "loss": 0.1782,
      "step": 29210
    },
    {
      "epoch": 1.7998152140437327,
      "grad_norm": 0.15013274550437927,
      "learning_rate": 8.005748896417206e-05,
      "loss": 0.1771,
      "step": 29220
    },
    {
      "epoch": 1.8004311672312903,
      "grad_norm": 0.15780168771743774,
      "learning_rate": 8.001642541833488e-05,
      "loss": 0.1759,
      "step": 29230
    },
    {
      "epoch": 1.8010471204188483,
      "grad_norm": 0.14522364735603333,
      "learning_rate": 7.997536187249769e-05,
      "loss": 0.178,
      "step": 29240
    },
    {
      "epoch": 1.8016630736064059,
      "grad_norm": 0.1561250388622284,
      "learning_rate": 7.99342983266605e-05,
      "loss": 0.1779,
      "step": 29250
    },
    {
      "epoch": 1.8022790267939637,
      "grad_norm": 0.15757635235786438,
      "learning_rate": 7.989323478082333e-05,
      "loss": 0.1762,
      "step": 29260
    },
    {
      "epoch": 1.8028949799815215,
      "grad_norm": 0.16123466193675995,
      "learning_rate": 7.985217123498615e-05,
      "loss": 0.1761,
      "step": 29270
    },
    {
      "epoch": 1.803510933169079,
      "grad_norm": 0.14911049604415894,
      "learning_rate": 7.981110768914897e-05,
      "loss": 0.177,
      "step": 29280
    },
    {
      "epoch": 1.8041268863566369,
      "grad_norm": 0.16132837533950806,
      "learning_rate": 7.977004414331179e-05,
      "loss": 0.1761,
      "step": 29290
    },
    {
      "epoch": 1.8047428395441947,
      "grad_norm": 0.18305449187755585,
      "learning_rate": 7.97289805974746e-05,
      "loss": 0.1787,
      "step": 29300
    },
    {
      "epoch": 1.8053587927317523,
      "grad_norm": 0.17334988713264465,
      "learning_rate": 7.968791705163741e-05,
      "loss": 0.1785,
      "step": 29310
    },
    {
      "epoch": 1.8059747459193103,
      "grad_norm": 0.17734278738498688,
      "learning_rate": 7.964685350580023e-05,
      "loss": 0.1761,
      "step": 29320
    },
    {
      "epoch": 1.8065906991068679,
      "grad_norm": 0.1421501189470291,
      "learning_rate": 7.960578995996305e-05,
      "loss": 0.1756,
      "step": 29330
    },
    {
      "epoch": 1.8072066522944257,
      "grad_norm": 0.15935386717319489,
      "learning_rate": 7.956472641412587e-05,
      "loss": 0.1783,
      "step": 29340
    },
    {
      "epoch": 1.8078226054819835,
      "grad_norm": 0.130338653922081,
      "learning_rate": 7.952366286828868e-05,
      "loss": 0.1761,
      "step": 29350
    },
    {
      "epoch": 1.808438558669541,
      "grad_norm": 0.16224254667758942,
      "learning_rate": 7.94825993224515e-05,
      "loss": 0.1772,
      "step": 29360
    },
    {
      "epoch": 1.8090545118570989,
      "grad_norm": 0.13574524223804474,
      "learning_rate": 7.944153577661432e-05,
      "loss": 0.1769,
      "step": 29370
    },
    {
      "epoch": 1.8096704650446567,
      "grad_norm": 0.13429668545722961,
      "learning_rate": 7.940047223077714e-05,
      "loss": 0.1777,
      "step": 29380
    },
    {
      "epoch": 1.8102864182322143,
      "grad_norm": 0.15056170523166656,
      "learning_rate": 7.935940868493994e-05,
      "loss": 0.1794,
      "step": 29390
    },
    {
      "epoch": 1.810902371419772,
      "grad_norm": 0.13171103596687317,
      "learning_rate": 7.931834513910276e-05,
      "loss": 0.1783,
      "step": 29400
    },
    {
      "epoch": 1.8115183246073299,
      "grad_norm": 0.17857801914215088,
      "learning_rate": 7.927728159326558e-05,
      "loss": 0.177,
      "step": 29410
    },
    {
      "epoch": 1.8121342777948874,
      "grad_norm": 0.15215003490447998,
      "learning_rate": 7.92362180474284e-05,
      "loss": 0.1779,
      "step": 29420
    },
    {
      "epoch": 1.8127502309824455,
      "grad_norm": 0.15683972835540771,
      "learning_rate": 7.919515450159121e-05,
      "loss": 0.1777,
      "step": 29430
    },
    {
      "epoch": 1.813366184170003,
      "grad_norm": 0.14081650972366333,
      "learning_rate": 7.915409095575403e-05,
      "loss": 0.1769,
      "step": 29440
    },
    {
      "epoch": 1.8139821373575609,
      "grad_norm": 0.14787332713603973,
      "learning_rate": 7.911302740991685e-05,
      "loss": 0.1759,
      "step": 29450
    },
    {
      "epoch": 1.8145980905451187,
      "grad_norm": 0.13783393800258636,
      "learning_rate": 7.907196386407967e-05,
      "loss": 0.1772,
      "step": 29460
    },
    {
      "epoch": 1.8152140437326763,
      "grad_norm": 0.1501401662826538,
      "learning_rate": 7.903090031824248e-05,
      "loss": 0.1764,
      "step": 29470
    },
    {
      "epoch": 1.815829996920234,
      "grad_norm": 0.1550849825143814,
      "learning_rate": 7.89898367724053e-05,
      "loss": 0.1752,
      "step": 29480
    },
    {
      "epoch": 1.8164459501077919,
      "grad_norm": 0.14817573130130768,
      "learning_rate": 7.894877322656812e-05,
      "loss": 0.1786,
      "step": 29490
    },
    {
      "epoch": 1.8170619032953494,
      "grad_norm": 0.14762070775032043,
      "learning_rate": 7.890770968073094e-05,
      "loss": 0.179,
      "step": 29500
    },
    {
      "epoch": 1.8176778564829073,
      "grad_norm": 0.12538516521453857,
      "learning_rate": 7.886664613489374e-05,
      "loss": 0.176,
      "step": 29510
    },
    {
      "epoch": 1.818293809670465,
      "grad_norm": 0.16276958584785461,
      "learning_rate": 7.882558258905656e-05,
      "loss": 0.1765,
      "step": 29520
    },
    {
      "epoch": 1.8189097628580226,
      "grad_norm": 0.1535021811723709,
      "learning_rate": 7.878451904321938e-05,
      "loss": 0.1743,
      "step": 29530
    },
    {
      "epoch": 1.8195257160455807,
      "grad_norm": 0.14770351350307465,
      "learning_rate": 7.87434554973822e-05,
      "loss": 0.1769,
      "step": 29540
    },
    {
      "epoch": 1.8201416692331382,
      "grad_norm": 0.13492318987846375,
      "learning_rate": 7.870239195154502e-05,
      "loss": 0.1776,
      "step": 29550
    },
    {
      "epoch": 1.820757622420696,
      "grad_norm": 0.16217190027236938,
      "learning_rate": 7.866132840570783e-05,
      "loss": 0.178,
      "step": 29560
    },
    {
      "epoch": 1.8213735756082539,
      "grad_norm": 0.1554039865732193,
      "learning_rate": 7.862026485987065e-05,
      "loss": 0.1787,
      "step": 29570
    },
    {
      "epoch": 1.8219895287958114,
      "grad_norm": 0.15357428789138794,
      "learning_rate": 7.857920131403347e-05,
      "loss": 0.1793,
      "step": 29580
    },
    {
      "epoch": 1.8226054819833692,
      "grad_norm": 0.13268905878067017,
      "learning_rate": 7.853813776819629e-05,
      "loss": 0.1746,
      "step": 29590
    },
    {
      "epoch": 1.823221435170927,
      "grad_norm": 0.13106057047843933,
      "learning_rate": 7.84970742223591e-05,
      "loss": 0.1778,
      "step": 29600
    },
    {
      "epoch": 1.8238373883584846,
      "grad_norm": 0.1413019895553589,
      "learning_rate": 7.845601067652192e-05,
      "loss": 0.1762,
      "step": 29610
    },
    {
      "epoch": 1.8244533415460427,
      "grad_norm": 0.14651471376419067,
      "learning_rate": 7.841494713068474e-05,
      "loss": 0.1772,
      "step": 29620
    },
    {
      "epoch": 1.8250692947336002,
      "grad_norm": 0.13806764781475067,
      "learning_rate": 7.837388358484756e-05,
      "loss": 0.1784,
      "step": 29630
    },
    {
      "epoch": 1.8256852479211578,
      "grad_norm": 0.19601109623908997,
      "learning_rate": 7.833282003901036e-05,
      "loss": 0.1799,
      "step": 29640
    },
    {
      "epoch": 1.8263012011087159,
      "grad_norm": 0.16676871478557587,
      "learning_rate": 7.829175649317318e-05,
      "loss": 0.1779,
      "step": 29650
    },
    {
      "epoch": 1.8269171542962734,
      "grad_norm": 0.13442149758338928,
      "learning_rate": 7.8250692947336e-05,
      "loss": 0.1768,
      "step": 29660
    },
    {
      "epoch": 1.8275331074838312,
      "grad_norm": 0.1490165740251541,
      "learning_rate": 7.820962940149882e-05,
      "loss": 0.1744,
      "step": 29670
    },
    {
      "epoch": 1.828149060671389,
      "grad_norm": 0.14758515357971191,
      "learning_rate": 7.816856585566163e-05,
      "loss": 0.1769,
      "step": 29680
    },
    {
      "epoch": 1.8287650138589466,
      "grad_norm": 0.1377723664045334,
      "learning_rate": 7.812750230982445e-05,
      "loss": 0.1738,
      "step": 29690
    },
    {
      "epoch": 1.8293809670465044,
      "grad_norm": 0.15329433977603912,
      "learning_rate": 7.808643876398727e-05,
      "loss": 0.176,
      "step": 29700
    },
    {
      "epoch": 1.8299969202340622,
      "grad_norm": 0.17073310911655426,
      "learning_rate": 7.804537521815009e-05,
      "loss": 0.1787,
      "step": 29710
    },
    {
      "epoch": 1.8306128734216198,
      "grad_norm": 0.15958727896213531,
      "learning_rate": 7.800431167231291e-05,
      "loss": 0.1757,
      "step": 29720
    },
    {
      "epoch": 1.8312288266091779,
      "grad_norm": 0.14076563715934753,
      "learning_rate": 7.796324812647573e-05,
      "loss": 0.1759,
      "step": 29730
    },
    {
      "epoch": 1.8318447797967354,
      "grad_norm": 0.1457311362028122,
      "learning_rate": 7.792218458063855e-05,
      "loss": 0.1771,
      "step": 29740
    },
    {
      "epoch": 1.8324607329842932,
      "grad_norm": 0.1431969702243805,
      "learning_rate": 7.788112103480136e-05,
      "loss": 0.1772,
      "step": 29750
    },
    {
      "epoch": 1.833076686171851,
      "grad_norm": 0.14837755262851715,
      "learning_rate": 7.784005748896418e-05,
      "loss": 0.1776,
      "step": 29760
    },
    {
      "epoch": 1.8336926393594086,
      "grad_norm": 0.16826577484607697,
      "learning_rate": 7.7798993943127e-05,
      "loss": 0.1772,
      "step": 29770
    },
    {
      "epoch": 1.8343085925469664,
      "grad_norm": 0.15019561350345612,
      "learning_rate": 7.775793039728982e-05,
      "loss": 0.1764,
      "step": 29780
    },
    {
      "epoch": 1.8349245457345242,
      "grad_norm": 0.1500396877527237,
      "learning_rate": 7.771686685145262e-05,
      "loss": 0.1771,
      "step": 29790
    },
    {
      "epoch": 1.8355404989220818,
      "grad_norm": 0.14933492243289948,
      "learning_rate": 7.767580330561544e-05,
      "loss": 0.1777,
      "step": 29800
    },
    {
      "epoch": 1.8361564521096396,
      "grad_norm": 0.15329131484031677,
      "learning_rate": 7.763473975977826e-05,
      "loss": 0.1794,
      "step": 29810
    },
    {
      "epoch": 1.8367724052971974,
      "grad_norm": 0.14984413981437683,
      "learning_rate": 7.759367621394108e-05,
      "loss": 0.1753,
      "step": 29820
    },
    {
      "epoch": 1.837388358484755,
      "grad_norm": 0.1850692629814148,
      "learning_rate": 7.755261266810389e-05,
      "loss": 0.1769,
      "step": 29830
    },
    {
      "epoch": 1.838004311672313,
      "grad_norm": 0.18015210330486298,
      "learning_rate": 7.751154912226671e-05,
      "loss": 0.1777,
      "step": 29840
    },
    {
      "epoch": 1.8386202648598706,
      "grad_norm": 0.16164171695709229,
      "learning_rate": 7.747048557642953e-05,
      "loss": 0.1767,
      "step": 29850
    },
    {
      "epoch": 1.8392362180474284,
      "grad_norm": 0.1467050164937973,
      "learning_rate": 7.742942203059235e-05,
      "loss": 0.1782,
      "step": 29860
    },
    {
      "epoch": 1.8398521712349862,
      "grad_norm": 0.15010100603103638,
      "learning_rate": 7.738835848475516e-05,
      "loss": 0.1763,
      "step": 29870
    },
    {
      "epoch": 1.8404681244225438,
      "grad_norm": 0.15412501990795135,
      "learning_rate": 7.734729493891798e-05,
      "loss": 0.1785,
      "step": 29880
    },
    {
      "epoch": 1.8410840776101016,
      "grad_norm": 0.15030354261398315,
      "learning_rate": 7.73062313930808e-05,
      "loss": 0.1773,
      "step": 29890
    },
    {
      "epoch": 1.8417000307976594,
      "grad_norm": 0.14369581639766693,
      "learning_rate": 7.726516784724362e-05,
      "loss": 0.1764,
      "step": 29900
    },
    {
      "epoch": 1.842315983985217,
      "grad_norm": 0.14912693202495575,
      "learning_rate": 7.722410430140642e-05,
      "loss": 0.1792,
      "step": 29910
    },
    {
      "epoch": 1.8429319371727748,
      "grad_norm": 0.1398961991071701,
      "learning_rate": 7.718304075556924e-05,
      "loss": 0.1797,
      "step": 29920
    },
    {
      "epoch": 1.8435478903603326,
      "grad_norm": 0.1438426673412323,
      "learning_rate": 7.714197720973206e-05,
      "loss": 0.1771,
      "step": 29930
    },
    {
      "epoch": 1.8441638435478902,
      "grad_norm": 0.16565033793449402,
      "learning_rate": 7.710091366389488e-05,
      "loss": 0.1768,
      "step": 29940
    },
    {
      "epoch": 1.8447797967354482,
      "grad_norm": 0.2879297435283661,
      "learning_rate": 7.705985011805769e-05,
      "loss": 0.174,
      "step": 29950
    },
    {
      "epoch": 1.8453957499230058,
      "grad_norm": 0.135335773229599,
      "learning_rate": 7.701878657222051e-05,
      "loss": 0.1763,
      "step": 29960
    },
    {
      "epoch": 1.8460117031105636,
      "grad_norm": 0.18634717166423798,
      "learning_rate": 7.697772302638333e-05,
      "loss": 0.1781,
      "step": 29970
    },
    {
      "epoch": 1.8466276562981214,
      "grad_norm": 0.13471049070358276,
      "learning_rate": 7.693665948054615e-05,
      "loss": 0.1769,
      "step": 29980
    },
    {
      "epoch": 1.847243609485679,
      "grad_norm": 0.1285185068845749,
      "learning_rate": 7.689559593470896e-05,
      "loss": 0.1767,
      "step": 29990
    },
    {
      "epoch": 1.8478595626732368,
      "grad_norm": 0.1497558355331421,
      "learning_rate": 7.685453238887178e-05,
      "loss": 0.177,
      "step": 30000
    },
    {
      "epoch": 1.8484755158607946,
      "grad_norm": 0.15144102275371552,
      "learning_rate": 7.68134688430346e-05,
      "loss": 0.1774,
      "step": 30010
    },
    {
      "epoch": 1.8490914690483522,
      "grad_norm": 0.12196126580238342,
      "learning_rate": 7.677240529719742e-05,
      "loss": 0.1787,
      "step": 30020
    },
    {
      "epoch": 1.8497074222359102,
      "grad_norm": 0.2395876944065094,
      "learning_rate": 7.673134175136022e-05,
      "loss": 0.1792,
      "step": 30030
    },
    {
      "epoch": 1.8503233754234678,
      "grad_norm": 0.1456718146800995,
      "learning_rate": 7.669027820552304e-05,
      "loss": 0.1782,
      "step": 30040
    },
    {
      "epoch": 1.8509393286110256,
      "grad_norm": 0.1277799755334854,
      "learning_rate": 7.664921465968586e-05,
      "loss": 0.1773,
      "step": 30050
    },
    {
      "epoch": 1.8515552817985834,
      "grad_norm": 0.15256211161613464,
      "learning_rate": 7.660815111384868e-05,
      "loss": 0.1757,
      "step": 30060
    },
    {
      "epoch": 1.852171234986141,
      "grad_norm": 0.16734173893928528,
      "learning_rate": 7.656708756801149e-05,
      "loss": 0.1763,
      "step": 30070
    },
    {
      "epoch": 1.8527871881736988,
      "grad_norm": 0.15844686329364777,
      "learning_rate": 7.652602402217431e-05,
      "loss": 0.1786,
      "step": 30080
    },
    {
      "epoch": 1.8534031413612566,
      "grad_norm": 0.13750974833965302,
      "learning_rate": 7.648496047633713e-05,
      "loss": 0.1776,
      "step": 30090
    },
    {
      "epoch": 1.8540190945488142,
      "grad_norm": 0.1538277566432953,
      "learning_rate": 7.644389693049995e-05,
      "loss": 0.1789,
      "step": 30100
    },
    {
      "epoch": 1.854635047736372,
      "grad_norm": 0.15676546096801758,
      "learning_rate": 7.640283338466277e-05,
      "loss": 0.1755,
      "step": 30110
    },
    {
      "epoch": 1.8552510009239298,
      "grad_norm": 0.13206857442855835,
      "learning_rate": 7.636176983882558e-05,
      "loss": 0.1765,
      "step": 30120
    },
    {
      "epoch": 1.8558669541114874,
      "grad_norm": 0.15363508462905884,
      "learning_rate": 7.632070629298841e-05,
      "loss": 0.1778,
      "step": 30130
    },
    {
      "epoch": 1.8564829072990454,
      "grad_norm": 0.163061261177063,
      "learning_rate": 7.627964274715123e-05,
      "loss": 0.1772,
      "step": 30140
    },
    {
      "epoch": 1.857098860486603,
      "grad_norm": 0.14819131791591644,
      "learning_rate": 7.623857920131404e-05,
      "loss": 0.1771,
      "step": 30150
    },
    {
      "epoch": 1.8577148136741608,
      "grad_norm": 0.14390642940998077,
      "learning_rate": 7.619751565547686e-05,
      "loss": 0.1757,
      "step": 30160
    },
    {
      "epoch": 1.8583307668617186,
      "grad_norm": 0.1602793186903,
      "learning_rate": 7.615645210963968e-05,
      "loss": 0.1759,
      "step": 30170
    },
    {
      "epoch": 1.8589467200492762,
      "grad_norm": 0.13934895396232605,
      "learning_rate": 7.61153885638025e-05,
      "loss": 0.1762,
      "step": 30180
    },
    {
      "epoch": 1.859562673236834,
      "grad_norm": 0.17777001857757568,
      "learning_rate": 7.60743250179653e-05,
      "loss": 0.1755,
      "step": 30190
    },
    {
      "epoch": 1.8601786264243918,
      "grad_norm": 0.16844390332698822,
      "learning_rate": 7.603326147212812e-05,
      "loss": 0.1793,
      "step": 30200
    },
    {
      "epoch": 1.8607945796119494,
      "grad_norm": 0.14486779272556305,
      "learning_rate": 7.599219792629094e-05,
      "loss": 0.177,
      "step": 30210
    },
    {
      "epoch": 1.8614105327995072,
      "grad_norm": 0.15077528357505798,
      "learning_rate": 7.595113438045376e-05,
      "loss": 0.1783,
      "step": 30220
    },
    {
      "epoch": 1.862026485987065,
      "grad_norm": 0.1969618946313858,
      "learning_rate": 7.591007083461657e-05,
      "loss": 0.1779,
      "step": 30230
    },
    {
      "epoch": 1.8626424391746226,
      "grad_norm": 0.15105301141738892,
      "learning_rate": 7.586900728877939e-05,
      "loss": 0.178,
      "step": 30240
    },
    {
      "epoch": 1.8632583923621806,
      "grad_norm": 0.16128218173980713,
      "learning_rate": 7.582794374294221e-05,
      "loss": 0.1765,
      "step": 30250
    },
    {
      "epoch": 1.8638743455497382,
      "grad_norm": 0.13705402612686157,
      "learning_rate": 7.578688019710503e-05,
      "loss": 0.1777,
      "step": 30260
    },
    {
      "epoch": 1.864490298737296,
      "grad_norm": 0.19092918932437897,
      "learning_rate": 7.574581665126784e-05,
      "loss": 0.1769,
      "step": 30270
    },
    {
      "epoch": 1.8651062519248538,
      "grad_norm": 0.14618760347366333,
      "learning_rate": 7.570475310543066e-05,
      "loss": 0.176,
      "step": 30280
    },
    {
      "epoch": 1.8657222051124114,
      "grad_norm": 0.14904148876667023,
      "learning_rate": 7.566368955959348e-05,
      "loss": 0.1776,
      "step": 30290
    },
    {
      "epoch": 1.8663381582999692,
      "grad_norm": 0.17153683304786682,
      "learning_rate": 7.56226260137563e-05,
      "loss": 0.1777,
      "step": 30300
    },
    {
      "epoch": 1.866954111487527,
      "grad_norm": 0.1425601989030838,
      "learning_rate": 7.55815624679191e-05,
      "loss": 0.1763,
      "step": 30310
    },
    {
      "epoch": 1.8675700646750846,
      "grad_norm": 0.14643186330795288,
      "learning_rate": 7.554049892208192e-05,
      "loss": 0.1767,
      "step": 30320
    },
    {
      "epoch": 1.8681860178626426,
      "grad_norm": 0.17061053216457367,
      "learning_rate": 7.549943537624474e-05,
      "loss": 0.1792,
      "step": 30330
    },
    {
      "epoch": 1.8688019710502002,
      "grad_norm": 0.1442379504442215,
      "learning_rate": 7.545837183040756e-05,
      "loss": 0.1774,
      "step": 30340
    },
    {
      "epoch": 1.8694179242377578,
      "grad_norm": 0.15041406452655792,
      "learning_rate": 7.541730828457037e-05,
      "loss": 0.177,
      "step": 30350
    },
    {
      "epoch": 1.8700338774253158,
      "grad_norm": 0.15918028354644775,
      "learning_rate": 7.537624473873319e-05,
      "loss": 0.1763,
      "step": 30360
    },
    {
      "epoch": 1.8706498306128734,
      "grad_norm": 0.16794463992118835,
      "learning_rate": 7.533518119289601e-05,
      "loss": 0.1793,
      "step": 30370
    },
    {
      "epoch": 1.8712657838004312,
      "grad_norm": 0.1594502031803131,
      "learning_rate": 7.529411764705883e-05,
      "loss": 0.1772,
      "step": 30380
    },
    {
      "epoch": 1.871881736987989,
      "grad_norm": 0.1386188119649887,
      "learning_rate": 7.525305410122164e-05,
      "loss": 0.1742,
      "step": 30390
    },
    {
      "epoch": 1.8724976901755466,
      "grad_norm": 0.14165908098220825,
      "learning_rate": 7.521199055538446e-05,
      "loss": 0.1765,
      "step": 30400
    },
    {
      "epoch": 1.8731136433631044,
      "grad_norm": 0.16636236011981964,
      "learning_rate": 7.517092700954728e-05,
      "loss": 0.1779,
      "step": 30410
    },
    {
      "epoch": 1.8737295965506622,
      "grad_norm": 0.15310102701187134,
      "learning_rate": 7.51298634637101e-05,
      "loss": 0.1796,
      "step": 30420
    },
    {
      "epoch": 1.8743455497382198,
      "grad_norm": 0.15264935791492462,
      "learning_rate": 7.50887999178729e-05,
      "loss": 0.1779,
      "step": 30430
    },
    {
      "epoch": 1.8749615029257778,
      "grad_norm": 0.1961677074432373,
      "learning_rate": 7.504773637203572e-05,
      "loss": 0.1787,
      "step": 30440
    },
    {
      "epoch": 1.8755774561133354,
      "grad_norm": 0.15895713865756989,
      "learning_rate": 7.500667282619854e-05,
      "loss": 0.1762,
      "step": 30450
    },
    {
      "epoch": 1.8761934093008932,
      "grad_norm": 0.14582744240760803,
      "learning_rate": 7.496560928036136e-05,
      "loss": 0.1778,
      "step": 30460
    },
    {
      "epoch": 1.876809362488451,
      "grad_norm": 0.14263847470283508,
      "learning_rate": 7.492454573452417e-05,
      "loss": 0.1759,
      "step": 30470
    },
    {
      "epoch": 1.8774253156760086,
      "grad_norm": 0.17067082226276398,
      "learning_rate": 7.488348218868699e-05,
      "loss": 0.1764,
      "step": 30480
    },
    {
      "epoch": 1.8780412688635664,
      "grad_norm": 0.17898431420326233,
      "learning_rate": 7.484241864284981e-05,
      "loss": 0.178,
      "step": 30490
    },
    {
      "epoch": 1.8786572220511242,
      "grad_norm": 0.1755354255437851,
      "learning_rate": 7.480135509701263e-05,
      "loss": 0.1775,
      "step": 30500
    },
    {
      "epoch": 1.8792731752386818,
      "grad_norm": 0.1504792869091034,
      "learning_rate": 7.476029155117544e-05,
      "loss": 0.1771,
      "step": 30510
    },
    {
      "epoch": 1.8798891284262396,
      "grad_norm": 0.24729253351688385,
      "learning_rate": 7.471922800533826e-05,
      "loss": 0.176,
      "step": 30520
    },
    {
      "epoch": 1.8805050816137974,
      "grad_norm": 0.15864019095897675,
      "learning_rate": 7.467816445950108e-05,
      "loss": 0.1751,
      "step": 30530
    },
    {
      "epoch": 1.881121034801355,
      "grad_norm": 0.15066668391227722,
      "learning_rate": 7.46371009136639e-05,
      "loss": 0.1772,
      "step": 30540
    },
    {
      "epoch": 1.881736987988913,
      "grad_norm": 0.14535917341709137,
      "learning_rate": 7.45960373678267e-05,
      "loss": 0.1766,
      "step": 30550
    },
    {
      "epoch": 1.8823529411764706,
      "grad_norm": 0.16851945221424103,
      "learning_rate": 7.455497382198954e-05,
      "loss": 0.1768,
      "step": 30560
    },
    {
      "epoch": 1.8829688943640284,
      "grad_norm": 0.14322644472122192,
      "learning_rate": 7.451391027615236e-05,
      "loss": 0.1764,
      "step": 30570
    },
    {
      "epoch": 1.8835848475515862,
      "grad_norm": 0.17117100954055786,
      "learning_rate": 7.447284673031518e-05,
      "loss": 0.1785,
      "step": 30580
    },
    {
      "epoch": 1.8842008007391438,
      "grad_norm": 0.1604039967060089,
      "learning_rate": 7.443178318447798e-05,
      "loss": 0.1766,
      "step": 30590
    },
    {
      "epoch": 1.8848167539267016,
      "grad_norm": 0.16486874222755432,
      "learning_rate": 7.43907196386408e-05,
      "loss": 0.1781,
      "step": 30600
    },
    {
      "epoch": 1.8854327071142594,
      "grad_norm": 0.15382155776023865,
      "learning_rate": 7.434965609280362e-05,
      "loss": 0.1773,
      "step": 30610
    },
    {
      "epoch": 1.886048660301817,
      "grad_norm": 0.14373138546943665,
      "learning_rate": 7.430859254696644e-05,
      "loss": 0.1765,
      "step": 30620
    },
    {
      "epoch": 1.8866646134893748,
      "grad_norm": 0.1500755399465561,
      "learning_rate": 7.426752900112925e-05,
      "loss": 0.1764,
      "step": 30630
    },
    {
      "epoch": 1.8872805666769326,
      "grad_norm": 0.15094797313213348,
      "learning_rate": 7.422646545529207e-05,
      "loss": 0.1773,
      "step": 30640
    },
    {
      "epoch": 1.8878965198644901,
      "grad_norm": 0.160365030169487,
      "learning_rate": 7.418540190945489e-05,
      "loss": 0.1753,
      "step": 30650
    },
    {
      "epoch": 1.8885124730520482,
      "grad_norm": 0.1354246288537979,
      "learning_rate": 7.414433836361771e-05,
      "loss": 0.1782,
      "step": 30660
    },
    {
      "epoch": 1.8891284262396058,
      "grad_norm": 0.15324623882770538,
      "learning_rate": 7.410327481778052e-05,
      "loss": 0.1771,
      "step": 30670
    },
    {
      "epoch": 1.8897443794271636,
      "grad_norm": 0.14813902974128723,
      "learning_rate": 7.406221127194334e-05,
      "loss": 0.1738,
      "step": 30680
    },
    {
      "epoch": 1.8903603326147214,
      "grad_norm": 0.17507466673851013,
      "learning_rate": 7.402114772610616e-05,
      "loss": 0.1768,
      "step": 30690
    },
    {
      "epoch": 1.890976285802279,
      "grad_norm": 0.18438313901424408,
      "learning_rate": 7.398008418026898e-05,
      "loss": 0.178,
      "step": 30700
    },
    {
      "epoch": 1.8915922389898367,
      "grad_norm": 0.1439257264137268,
      "learning_rate": 7.393902063443178e-05,
      "loss": 0.1779,
      "step": 30710
    },
    {
      "epoch": 1.8922081921773946,
      "grad_norm": 0.17577652633190155,
      "learning_rate": 7.38979570885946e-05,
      "loss": 0.1776,
      "step": 30720
    },
    {
      "epoch": 1.8928241453649521,
      "grad_norm": 0.16683146357536316,
      "learning_rate": 7.385689354275742e-05,
      "loss": 0.1779,
      "step": 30730
    },
    {
      "epoch": 1.8934400985525102,
      "grad_norm": 0.17163462936878204,
      "learning_rate": 7.381582999692024e-05,
      "loss": 0.1764,
      "step": 30740
    },
    {
      "epoch": 1.8940560517400677,
      "grad_norm": 0.2561729848384857,
      "learning_rate": 7.377476645108305e-05,
      "loss": 0.1785,
      "step": 30750
    },
    {
      "epoch": 1.8946720049276256,
      "grad_norm": 0.12925009429454803,
      "learning_rate": 7.373370290524587e-05,
      "loss": 0.1768,
      "step": 30760
    },
    {
      "epoch": 1.8952879581151834,
      "grad_norm": 0.1326700747013092,
      "learning_rate": 7.369263935940869e-05,
      "loss": 0.1766,
      "step": 30770
    },
    {
      "epoch": 1.895903911302741,
      "grad_norm": 0.17250466346740723,
      "learning_rate": 7.365157581357151e-05,
      "loss": 0.1782,
      "step": 30780
    },
    {
      "epoch": 1.8965198644902987,
      "grad_norm": 0.1567503958940506,
      "learning_rate": 7.361051226773432e-05,
      "loss": 0.1764,
      "step": 30790
    },
    {
      "epoch": 1.8971358176778566,
      "grad_norm": 0.13647666573524475,
      "learning_rate": 7.356944872189714e-05,
      "loss": 0.1801,
      "step": 30800
    },
    {
      "epoch": 1.8977517708654141,
      "grad_norm": 0.14236898720264435,
      "learning_rate": 7.352838517605996e-05,
      "loss": 0.1749,
      "step": 30810
    },
    {
      "epoch": 1.898367724052972,
      "grad_norm": 0.14779028296470642,
      "learning_rate": 7.348732163022278e-05,
      "loss": 0.177,
      "step": 30820
    },
    {
      "epoch": 1.8989836772405297,
      "grad_norm": 0.14049752056598663,
      "learning_rate": 7.344625808438558e-05,
      "loss": 0.177,
      "step": 30830
    },
    {
      "epoch": 1.8995996304280873,
      "grad_norm": 0.16269829869270325,
      "learning_rate": 7.34051945385484e-05,
      "loss": 0.1773,
      "step": 30840
    },
    {
      "epoch": 1.9002155836156454,
      "grad_norm": 0.17572474479675293,
      "learning_rate": 7.336413099271122e-05,
      "loss": 0.1781,
      "step": 30850
    },
    {
      "epoch": 1.900831536803203,
      "grad_norm": 0.14912132918834686,
      "learning_rate": 7.332306744687404e-05,
      "loss": 0.1767,
      "step": 30860
    },
    {
      "epoch": 1.9014474899907607,
      "grad_norm": 0.1677876114845276,
      "learning_rate": 7.328200390103685e-05,
      "loss": 0.1784,
      "step": 30870
    },
    {
      "epoch": 1.9020634431783185,
      "grad_norm": 0.15920709073543549,
      "learning_rate": 7.324094035519967e-05,
      "loss": 0.176,
      "step": 30880
    },
    {
      "epoch": 1.9026793963658761,
      "grad_norm": 0.1558222472667694,
      "learning_rate": 7.319987680936249e-05,
      "loss": 0.1784,
      "step": 30890
    },
    {
      "epoch": 1.903295349553434,
      "grad_norm": 0.16279298067092896,
      "learning_rate": 7.315881326352531e-05,
      "loss": 0.178,
      "step": 30900
    },
    {
      "epoch": 1.9039113027409917,
      "grad_norm": 0.1296931654214859,
      "learning_rate": 7.311774971768812e-05,
      "loss": 0.1759,
      "step": 30910
    },
    {
      "epoch": 1.9045272559285493,
      "grad_norm": 0.1503046452999115,
      "learning_rate": 7.307668617185094e-05,
      "loss": 0.1783,
      "step": 30920
    },
    {
      "epoch": 1.9051432091161071,
      "grad_norm": 0.1481143981218338,
      "learning_rate": 7.303562262601376e-05,
      "loss": 0.1764,
      "step": 30930
    },
    {
      "epoch": 1.905759162303665,
      "grad_norm": 0.1318269670009613,
      "learning_rate": 7.299455908017658e-05,
      "loss": 0.1766,
      "step": 30940
    },
    {
      "epoch": 1.9063751154912225,
      "grad_norm": 0.15949995815753937,
      "learning_rate": 7.295349553433938e-05,
      "loss": 0.1769,
      "step": 30950
    },
    {
      "epoch": 1.9069910686787805,
      "grad_norm": 0.14956985414028168,
      "learning_rate": 7.29124319885022e-05,
      "loss": 0.1792,
      "step": 30960
    },
    {
      "epoch": 1.9076070218663381,
      "grad_norm": 0.17914795875549316,
      "learning_rate": 7.287136844266502e-05,
      "loss": 0.1783,
      "step": 30970
    },
    {
      "epoch": 1.908222975053896,
      "grad_norm": 0.17417673766613007,
      "learning_rate": 7.283030489682786e-05,
      "loss": 0.1782,
      "step": 30980
    },
    {
      "epoch": 1.9088389282414537,
      "grad_norm": 0.14727598428726196,
      "learning_rate": 7.278924135099066e-05,
      "loss": 0.1773,
      "step": 30990
    },
    {
      "epoch": 1.9094548814290113,
      "grad_norm": 0.18580706417560577,
      "learning_rate": 7.274817780515348e-05,
      "loss": 0.1776,
      "step": 31000
    },
    {
      "epoch": 1.9100708346165691,
      "grad_norm": 0.1411479115486145,
      "learning_rate": 7.27071142593163e-05,
      "loss": 0.1772,
      "step": 31010
    },
    {
      "epoch": 1.910686787804127,
      "grad_norm": 0.14607034623622894,
      "learning_rate": 7.266605071347912e-05,
      "loss": 0.178,
      "step": 31020
    },
    {
      "epoch": 1.9113027409916845,
      "grad_norm": 0.14841659367084503,
      "learning_rate": 7.262498716764193e-05,
      "loss": 0.176,
      "step": 31030
    },
    {
      "epoch": 1.9119186941792425,
      "grad_norm": 0.13749952614307404,
      "learning_rate": 7.258392362180475e-05,
      "loss": 0.1781,
      "step": 31040
    },
    {
      "epoch": 1.9125346473668001,
      "grad_norm": 0.23365554213523865,
      "learning_rate": 7.254286007596757e-05,
      "loss": 0.1781,
      "step": 31050
    },
    {
      "epoch": 1.9131506005543577,
      "grad_norm": 0.1512700468301773,
      "learning_rate": 7.250179653013039e-05,
      "loss": 0.1758,
      "step": 31060
    },
    {
      "epoch": 1.9137665537419157,
      "grad_norm": 0.148310124874115,
      "learning_rate": 7.24607329842932e-05,
      "loss": 0.1775,
      "step": 31070
    },
    {
      "epoch": 1.9143825069294733,
      "grad_norm": 0.14860013127326965,
      "learning_rate": 7.241966943845602e-05,
      "loss": 0.1757,
      "step": 31080
    },
    {
      "epoch": 1.9149984601170311,
      "grad_norm": 0.15417225658893585,
      "learning_rate": 7.237860589261884e-05,
      "loss": 0.1772,
      "step": 31090
    },
    {
      "epoch": 1.915614413304589,
      "grad_norm": 0.15707378089427948,
      "learning_rate": 7.233754234678166e-05,
      "loss": 0.1761,
      "step": 31100
    },
    {
      "epoch": 1.9162303664921465,
      "grad_norm": 0.16407029330730438,
      "learning_rate": 7.229647880094446e-05,
      "loss": 0.1763,
      "step": 31110
    },
    {
      "epoch": 1.9168463196797043,
      "grad_norm": 0.22526098787784576,
      "learning_rate": 7.225541525510728e-05,
      "loss": 0.1779,
      "step": 31120
    },
    {
      "epoch": 1.9174622728672621,
      "grad_norm": 0.15125179290771484,
      "learning_rate": 7.22143517092701e-05,
      "loss": 0.1771,
      "step": 31130
    },
    {
      "epoch": 1.9180782260548197,
      "grad_norm": 0.14587876200675964,
      "learning_rate": 7.217328816343292e-05,
      "loss": 0.1748,
      "step": 31140
    },
    {
      "epoch": 1.9186941792423777,
      "grad_norm": 0.14193613827228546,
      "learning_rate": 7.213222461759573e-05,
      "loss": 0.1772,
      "step": 31150
    },
    {
      "epoch": 1.9193101324299353,
      "grad_norm": 0.1707286834716797,
      "learning_rate": 7.209116107175855e-05,
      "loss": 0.1761,
      "step": 31160
    },
    {
      "epoch": 1.9199260856174931,
      "grad_norm": 0.16494803130626678,
      "learning_rate": 7.205009752592137e-05,
      "loss": 0.1777,
      "step": 31170
    },
    {
      "epoch": 1.920542038805051,
      "grad_norm": 0.17286249995231628,
      "learning_rate": 7.200903398008419e-05,
      "loss": 0.1765,
      "step": 31180
    },
    {
      "epoch": 1.9211579919926085,
      "grad_norm": 0.16672971844673157,
      "learning_rate": 7.1967970434247e-05,
      "loss": 0.177,
      "step": 31190
    },
    {
      "epoch": 1.9217739451801663,
      "grad_norm": 0.1790582537651062,
      "learning_rate": 7.192690688840982e-05,
      "loss": 0.1763,
      "step": 31200
    },
    {
      "epoch": 1.9223898983677241,
      "grad_norm": 0.15405724942684174,
      "learning_rate": 7.188584334257264e-05,
      "loss": 0.1751,
      "step": 31210
    },
    {
      "epoch": 1.9230058515552817,
      "grad_norm": 0.19544699788093567,
      "learning_rate": 7.184477979673546e-05,
      "loss": 0.1786,
      "step": 31220
    },
    {
      "epoch": 1.9236218047428395,
      "grad_norm": 0.16746105253696442,
      "learning_rate": 7.180371625089826e-05,
      "loss": 0.1775,
      "step": 31230
    },
    {
      "epoch": 1.9242377579303973,
      "grad_norm": 0.16049598157405853,
      "learning_rate": 7.176265270506108e-05,
      "loss": 0.178,
      "step": 31240
    },
    {
      "epoch": 1.924853711117955,
      "grad_norm": 0.15203030407428741,
      "learning_rate": 7.17215891592239e-05,
      "loss": 0.1789,
      "step": 31250
    },
    {
      "epoch": 1.925469664305513,
      "grad_norm": 0.15440250933170319,
      "learning_rate": 7.168052561338672e-05,
      "loss": 0.1776,
      "step": 31260
    },
    {
      "epoch": 1.9260856174930705,
      "grad_norm": 0.16048124432563782,
      "learning_rate": 7.163946206754953e-05,
      "loss": 0.1777,
      "step": 31270
    },
    {
      "epoch": 1.9267015706806283,
      "grad_norm": 0.13970531523227692,
      "learning_rate": 7.159839852171235e-05,
      "loss": 0.1785,
      "step": 31280
    },
    {
      "epoch": 1.927317523868186,
      "grad_norm": 0.14136408269405365,
      "learning_rate": 7.155733497587517e-05,
      "loss": 0.1771,
      "step": 31290
    },
    {
      "epoch": 1.9279334770557437,
      "grad_norm": 0.1360282003879547,
      "learning_rate": 7.151627143003799e-05,
      "loss": 0.175,
      "step": 31300
    },
    {
      "epoch": 1.9285494302433015,
      "grad_norm": 0.1506727635860443,
      "learning_rate": 7.14752078842008e-05,
      "loss": 0.1758,
      "step": 31310
    },
    {
      "epoch": 1.9291653834308593,
      "grad_norm": 0.13784977793693542,
      "learning_rate": 7.143414433836362e-05,
      "loss": 0.176,
      "step": 31320
    },
    {
      "epoch": 1.9297813366184169,
      "grad_norm": 0.145088329911232,
      "learning_rate": 7.139308079252644e-05,
      "loss": 0.1781,
      "step": 31330
    },
    {
      "epoch": 1.9303972898059747,
      "grad_norm": 0.14471368491649628,
      "learning_rate": 7.135201724668926e-05,
      "loss": 0.1765,
      "step": 31340
    },
    {
      "epoch": 1.9310132429935325,
      "grad_norm": 0.14036501944065094,
      "learning_rate": 7.131095370085206e-05,
      "loss": 0.178,
      "step": 31350
    },
    {
      "epoch": 1.93162919618109,
      "grad_norm": 0.15480229258537292,
      "learning_rate": 7.126989015501488e-05,
      "loss": 0.178,
      "step": 31360
    },
    {
      "epoch": 1.932245149368648,
      "grad_norm": 0.16120842099189758,
      "learning_rate": 7.12288266091777e-05,
      "loss": 0.1788,
      "step": 31370
    },
    {
      "epoch": 1.9328611025562057,
      "grad_norm": 0.13765370845794678,
      "learning_rate": 7.118776306334052e-05,
      "loss": 0.178,
      "step": 31380
    },
    {
      "epoch": 1.9334770557437635,
      "grad_norm": 0.15943622589111328,
      "learning_rate": 7.114669951750333e-05,
      "loss": 0.1766,
      "step": 31390
    },
    {
      "epoch": 1.9340930089313213,
      "grad_norm": 0.1625344604253769,
      "learning_rate": 7.110563597166615e-05,
      "loss": 0.1778,
      "step": 31400
    },
    {
      "epoch": 1.9347089621188789,
      "grad_norm": 0.1420508176088333,
      "learning_rate": 7.106457242582898e-05,
      "loss": 0.1768,
      "step": 31410
    },
    {
      "epoch": 1.9353249153064367,
      "grad_norm": 0.1671653538942337,
      "learning_rate": 7.102350887999179e-05,
      "loss": 0.1764,
      "step": 31420
    },
    {
      "epoch": 1.9359408684939945,
      "grad_norm": 0.15744014084339142,
      "learning_rate": 7.098244533415461e-05,
      "loss": 0.1763,
      "step": 31430
    },
    {
      "epoch": 1.936556821681552,
      "grad_norm": 0.1815863847732544,
      "learning_rate": 7.094138178831743e-05,
      "loss": 0.1774,
      "step": 31440
    },
    {
      "epoch": 1.93717277486911,
      "grad_norm": 0.14220666885375977,
      "learning_rate": 7.090031824248025e-05,
      "loss": 0.1768,
      "step": 31450
    },
    {
      "epoch": 1.9377887280566677,
      "grad_norm": 0.15904387831687927,
      "learning_rate": 7.085925469664307e-05,
      "loss": 0.1759,
      "step": 31460
    },
    {
      "epoch": 1.9384046812442255,
      "grad_norm": 0.14198826253414154,
      "learning_rate": 7.081819115080588e-05,
      "loss": 0.1766,
      "step": 31470
    },
    {
      "epoch": 1.9390206344317833,
      "grad_norm": 0.15087094902992249,
      "learning_rate": 7.07771276049687e-05,
      "loss": 0.1761,
      "step": 31480
    },
    {
      "epoch": 1.9396365876193409,
      "grad_norm": 0.15431132912635803,
      "learning_rate": 7.073606405913152e-05,
      "loss": 0.1771,
      "step": 31490
    },
    {
      "epoch": 1.9402525408068987,
      "grad_norm": 0.14750850200653076,
      "learning_rate": 7.069500051329434e-05,
      "loss": 0.177,
      "step": 31500
    },
    {
      "epoch": 1.9408684939944565,
      "grad_norm": 0.15212425589561462,
      "learning_rate": 7.065393696745714e-05,
      "loss": 0.1762,
      "step": 31510
    },
    {
      "epoch": 1.941484447182014,
      "grad_norm": 0.1499815434217453,
      "learning_rate": 7.061287342161996e-05,
      "loss": 0.1768,
      "step": 31520
    },
    {
      "epoch": 1.9421004003695719,
      "grad_norm": 0.14508406817913055,
      "learning_rate": 7.057180987578278e-05,
      "loss": 0.1745,
      "step": 31530
    },
    {
      "epoch": 1.9427163535571297,
      "grad_norm": 0.1325843781232834,
      "learning_rate": 7.05307463299456e-05,
      "loss": 0.1748,
      "step": 31540
    },
    {
      "epoch": 1.9433323067446873,
      "grad_norm": 0.14721642434597015,
      "learning_rate": 7.048968278410841e-05,
      "loss": 0.1766,
      "step": 31550
    },
    {
      "epoch": 1.9439482599322453,
      "grad_norm": 0.1411094069480896,
      "learning_rate": 7.044861923827123e-05,
      "loss": 0.1742,
      "step": 31560
    },
    {
      "epoch": 1.9445642131198029,
      "grad_norm": 0.18216179311275482,
      "learning_rate": 7.040755569243405e-05,
      "loss": 0.1774,
      "step": 31570
    },
    {
      "epoch": 1.9451801663073607,
      "grad_norm": 0.1649879664182663,
      "learning_rate": 7.036649214659687e-05,
      "loss": 0.1759,
      "step": 31580
    },
    {
      "epoch": 1.9457961194949185,
      "grad_norm": 0.17628629505634308,
      "learning_rate": 7.032542860075968e-05,
      "loss": 0.1754,
      "step": 31590
    },
    {
      "epoch": 1.946412072682476,
      "grad_norm": 0.16111795604228973,
      "learning_rate": 7.02843650549225e-05,
      "loss": 0.1785,
      "step": 31600
    },
    {
      "epoch": 1.9470280258700339,
      "grad_norm": 0.1691392958164215,
      "learning_rate": 7.024330150908532e-05,
      "loss": 0.1777,
      "step": 31610
    },
    {
      "epoch": 1.9476439790575917,
      "grad_norm": 0.12594649195671082,
      "learning_rate": 7.020223796324814e-05,
      "loss": 0.1766,
      "step": 31620
    },
    {
      "epoch": 1.9482599322451493,
      "grad_norm": 0.1434611827135086,
      "learning_rate": 7.016117441741094e-05,
      "loss": 0.1774,
      "step": 31630
    },
    {
      "epoch": 1.948875885432707,
      "grad_norm": 0.16076354682445526,
      "learning_rate": 7.012011087157376e-05,
      "loss": 0.1775,
      "step": 31640
    },
    {
      "epoch": 1.9494918386202649,
      "grad_norm": 0.1767851561307907,
      "learning_rate": 7.007904732573658e-05,
      "loss": 0.1773,
      "step": 31650
    },
    {
      "epoch": 1.9501077918078225,
      "grad_norm": 0.1464383602142334,
      "learning_rate": 7.00379837798994e-05,
      "loss": 0.1771,
      "step": 31660
    },
    {
      "epoch": 1.9507237449953805,
      "grad_norm": 0.13986264169216156,
      "learning_rate": 6.999692023406221e-05,
      "loss": 0.1765,
      "step": 31670
    },
    {
      "epoch": 1.951339698182938,
      "grad_norm": 0.1766398847103119,
      "learning_rate": 6.995585668822503e-05,
      "loss": 0.1778,
      "step": 31680
    },
    {
      "epoch": 1.9519556513704959,
      "grad_norm": 0.14324936270713806,
      "learning_rate": 6.991479314238785e-05,
      "loss": 0.177,
      "step": 31690
    },
    {
      "epoch": 1.9525716045580537,
      "grad_norm": 0.14306314289569855,
      "learning_rate": 6.987372959655067e-05,
      "loss": 0.1757,
      "step": 31700
    },
    {
      "epoch": 1.9531875577456113,
      "grad_norm": 0.14757533371448517,
      "learning_rate": 6.983266605071348e-05,
      "loss": 0.1746,
      "step": 31710
    },
    {
      "epoch": 1.953803510933169,
      "grad_norm": 0.14790640771389008,
      "learning_rate": 6.97916025048763e-05,
      "loss": 0.1784,
      "step": 31720
    },
    {
      "epoch": 1.9544194641207269,
      "grad_norm": 0.13752628862857819,
      "learning_rate": 6.975053895903912e-05,
      "loss": 0.1766,
      "step": 31730
    },
    {
      "epoch": 1.9550354173082845,
      "grad_norm": 0.14340482652187347,
      "learning_rate": 6.970947541320194e-05,
      "loss": 0.1769,
      "step": 31740
    },
    {
      "epoch": 1.9556513704958425,
      "grad_norm": 0.16242973506450653,
      "learning_rate": 6.966841186736474e-05,
      "loss": 0.1781,
      "step": 31750
    },
    {
      "epoch": 1.9562673236834,
      "grad_norm": 0.17141537368297577,
      "learning_rate": 6.962734832152756e-05,
      "loss": 0.1796,
      "step": 31760
    },
    {
      "epoch": 1.9568832768709576,
      "grad_norm": 0.15238197147846222,
      "learning_rate": 6.958628477569038e-05,
      "loss": 0.1761,
      "step": 31770
    },
    {
      "epoch": 1.9574992300585157,
      "grad_norm": 0.16348567605018616,
      "learning_rate": 6.95452212298532e-05,
      "loss": 0.1755,
      "step": 31780
    },
    {
      "epoch": 1.9581151832460733,
      "grad_norm": 0.13664834201335907,
      "learning_rate": 6.950415768401601e-05,
      "loss": 0.1773,
      "step": 31790
    },
    {
      "epoch": 1.958731136433631,
      "grad_norm": 0.13366150856018066,
      "learning_rate": 6.946309413817883e-05,
      "loss": 0.176,
      "step": 31800
    },
    {
      "epoch": 1.9593470896211889,
      "grad_norm": 0.1491934359073639,
      "learning_rate": 6.942203059234165e-05,
      "loss": 0.1751,
      "step": 31810
    },
    {
      "epoch": 1.9599630428087464,
      "grad_norm": 0.1451321691274643,
      "learning_rate": 6.938096704650447e-05,
      "loss": 0.1777,
      "step": 31820
    },
    {
      "epoch": 1.9605789959963043,
      "grad_norm": 0.16489183902740479,
      "learning_rate": 6.933990350066729e-05,
      "loss": 0.1762,
      "step": 31830
    },
    {
      "epoch": 1.961194949183862,
      "grad_norm": 0.15694938600063324,
      "learning_rate": 6.929883995483011e-05,
      "loss": 0.1765,
      "step": 31840
    },
    {
      "epoch": 1.9618109023714196,
      "grad_norm": 0.1483546644449234,
      "learning_rate": 6.925777640899293e-05,
      "loss": 0.1782,
      "step": 31850
    },
    {
      "epoch": 1.9624268555589777,
      "grad_norm": 0.13815569877624512,
      "learning_rate": 6.921671286315574e-05,
      "loss": 0.177,
      "step": 31860
    },
    {
      "epoch": 1.9630428087465353,
      "grad_norm": 0.13944077491760254,
      "learning_rate": 6.917564931731856e-05,
      "loss": 0.1762,
      "step": 31870
    },
    {
      "epoch": 1.963658761934093,
      "grad_norm": 0.131045401096344,
      "learning_rate": 6.913458577148138e-05,
      "loss": 0.1776,
      "step": 31880
    },
    {
      "epoch": 1.9642747151216509,
      "grad_norm": 0.14234717190265656,
      "learning_rate": 6.90935222256442e-05,
      "loss": 0.1765,
      "step": 31890
    },
    {
      "epoch": 1.9648906683092084,
      "grad_norm": 0.16366317868232727,
      "learning_rate": 6.9052458679807e-05,
      "loss": 0.1766,
      "step": 31900
    },
    {
      "epoch": 1.9655066214967662,
      "grad_norm": 0.14449159801006317,
      "learning_rate": 6.901139513396982e-05,
      "loss": 0.1768,
      "step": 31910
    },
    {
      "epoch": 1.966122574684324,
      "grad_norm": 0.16038484871387482,
      "learning_rate": 6.897033158813264e-05,
      "loss": 0.1762,
      "step": 31920
    },
    {
      "epoch": 1.9667385278718816,
      "grad_norm": 0.1593862771987915,
      "learning_rate": 6.892926804229546e-05,
      "loss": 0.1755,
      "step": 31930
    },
    {
      "epoch": 1.9673544810594394,
      "grad_norm": 0.15937641263008118,
      "learning_rate": 6.888820449645827e-05,
      "loss": 0.1753,
      "step": 31940
    },
    {
      "epoch": 1.9679704342469972,
      "grad_norm": 0.1552734375,
      "learning_rate": 6.884714095062109e-05,
      "loss": 0.179,
      "step": 31950
    },
    {
      "epoch": 1.9685863874345548,
      "grad_norm": 0.14151442050933838,
      "learning_rate": 6.880607740478391e-05,
      "loss": 0.1762,
      "step": 31960
    },
    {
      "epoch": 1.9692023406221129,
      "grad_norm": 0.14991052448749542,
      "learning_rate": 6.876501385894673e-05,
      "loss": 0.1776,
      "step": 31970
    },
    {
      "epoch": 1.9698182938096704,
      "grad_norm": 0.15947937965393066,
      "learning_rate": 6.872395031310955e-05,
      "loss": 0.1761,
      "step": 31980
    },
    {
      "epoch": 1.9704342469972282,
      "grad_norm": 0.13338971138000488,
      "learning_rate": 6.868288676727236e-05,
      "loss": 0.1765,
      "step": 31990
    },
    {
      "epoch": 1.971050200184786,
      "grad_norm": 0.1408388316631317,
      "learning_rate": 6.864182322143518e-05,
      "loss": 0.1767,
      "step": 32000
    },
    {
      "epoch": 1.9716661533723436,
      "grad_norm": 0.1465006023645401,
      "learning_rate": 6.8600759675598e-05,
      "loss": 0.1766,
      "step": 32010
    },
    {
      "epoch": 1.9722821065599014,
      "grad_norm": 0.13931654393672943,
      "learning_rate": 6.855969612976082e-05,
      "loss": 0.1772,
      "step": 32020
    },
    {
      "epoch": 1.9728980597474592,
      "grad_norm": 0.14124299585819244,
      "learning_rate": 6.851863258392362e-05,
      "loss": 0.1773,
      "step": 32030
    },
    {
      "epoch": 1.9735140129350168,
      "grad_norm": 0.15938062965869904,
      "learning_rate": 6.847756903808644e-05,
      "loss": 0.1766,
      "step": 32040
    },
    {
      "epoch": 1.9741299661225746,
      "grad_norm": 0.13633300364017487,
      "learning_rate": 6.843650549224926e-05,
      "loss": 0.1787,
      "step": 32050
    },
    {
      "epoch": 1.9747459193101324,
      "grad_norm": 0.1548052579164505,
      "learning_rate": 6.839544194641208e-05,
      "loss": 0.1756,
      "step": 32060
    },
    {
      "epoch": 1.97536187249769,
      "grad_norm": 0.12296455353498459,
      "learning_rate": 6.835437840057489e-05,
      "loss": 0.1782,
      "step": 32070
    },
    {
      "epoch": 1.975977825685248,
      "grad_norm": 0.14918172359466553,
      "learning_rate": 6.831331485473771e-05,
      "loss": 0.1757,
      "step": 32080
    },
    {
      "epoch": 1.9765937788728056,
      "grad_norm": 0.1554115116596222,
      "learning_rate": 6.827225130890053e-05,
      "loss": 0.1769,
      "step": 32090
    },
    {
      "epoch": 1.9772097320603634,
      "grad_norm": 0.16234634816646576,
      "learning_rate": 6.823118776306335e-05,
      "loss": 0.175,
      "step": 32100
    },
    {
      "epoch": 1.9778256852479212,
      "grad_norm": 0.14848095178604126,
      "learning_rate": 6.819012421722615e-05,
      "loss": 0.1784,
      "step": 32110
    },
    {
      "epoch": 1.9784416384354788,
      "grad_norm": 0.1342671662569046,
      "learning_rate": 6.814906067138897e-05,
      "loss": 0.1744,
      "step": 32120
    },
    {
      "epoch": 1.9790575916230366,
      "grad_norm": 0.14182806015014648,
      "learning_rate": 6.81079971255518e-05,
      "loss": 0.1783,
      "step": 32130
    },
    {
      "epoch": 1.9796735448105944,
      "grad_norm": 0.15382705628871918,
      "learning_rate": 6.806693357971461e-05,
      "loss": 0.1748,
      "step": 32140
    },
    {
      "epoch": 1.980289497998152,
      "grad_norm": 0.1346694827079773,
      "learning_rate": 6.802587003387742e-05,
      "loss": 0.1768,
      "step": 32150
    },
    {
      "epoch": 1.98090545118571,
      "grad_norm": 0.1421653777360916,
      "learning_rate": 6.798480648804024e-05,
      "loss": 0.1768,
      "step": 32160
    },
    {
      "epoch": 1.9815214043732676,
      "grad_norm": 0.1570432186126709,
      "learning_rate": 6.794374294220306e-05,
      "loss": 0.1758,
      "step": 32170
    },
    {
      "epoch": 1.9821373575608254,
      "grad_norm": 0.1619648039340973,
      "learning_rate": 6.790267939636588e-05,
      "loss": 0.1781,
      "step": 32180
    },
    {
      "epoch": 1.9827533107483832,
      "grad_norm": 0.1825234293937683,
      "learning_rate": 6.786161585052869e-05,
      "loss": 0.1753,
      "step": 32190
    },
    {
      "epoch": 1.9833692639359408,
      "grad_norm": 0.15477199852466583,
      "learning_rate": 6.782055230469151e-05,
      "loss": 0.1773,
      "step": 32200
    },
    {
      "epoch": 1.9839852171234986,
      "grad_norm": 0.17868101596832275,
      "learning_rate": 6.777948875885433e-05,
      "loss": 0.1786,
      "step": 32210
    },
    {
      "epoch": 1.9846011703110564,
      "grad_norm": 0.1325080841779709,
      "learning_rate": 6.773842521301715e-05,
      "loss": 0.1758,
      "step": 32220
    },
    {
      "epoch": 1.985217123498614,
      "grad_norm": 0.1491548866033554,
      "learning_rate": 6.769736166717995e-05,
      "loss": 0.1783,
      "step": 32230
    },
    {
      "epoch": 1.9858330766861718,
      "grad_norm": 0.16020016372203827,
      "learning_rate": 6.765629812134277e-05,
      "loss": 0.1748,
      "step": 32240
    },
    {
      "epoch": 1.9864490298737296,
      "grad_norm": 0.1347304731607437,
      "learning_rate": 6.76152345755056e-05,
      "loss": 0.1773,
      "step": 32250
    },
    {
      "epoch": 1.9870649830612872,
      "grad_norm": 0.1440238505601883,
      "learning_rate": 6.757417102966841e-05,
      "loss": 0.1754,
      "step": 32260
    },
    {
      "epoch": 1.9876809362488452,
      "grad_norm": 0.1677478849887848,
      "learning_rate": 6.753310748383123e-05,
      "loss": 0.1783,
      "step": 32270
    },
    {
      "epoch": 1.9882968894364028,
      "grad_norm": 0.1644906848669052,
      "learning_rate": 6.749204393799405e-05,
      "loss": 0.178,
      "step": 32280
    },
    {
      "epoch": 1.9889128426239606,
      "grad_norm": 0.1603187918663025,
      "learning_rate": 6.745098039215687e-05,
      "loss": 0.1774,
      "step": 32290
    },
    {
      "epoch": 1.9895287958115184,
      "grad_norm": 0.12499241530895233,
      "learning_rate": 6.740991684631968e-05,
      "loss": 0.1781,
      "step": 32300
    },
    {
      "epoch": 1.990144748999076,
      "grad_norm": 0.14241544902324677,
      "learning_rate": 6.73688533004825e-05,
      "loss": 0.1746,
      "step": 32310
    },
    {
      "epoch": 1.9907607021866338,
      "grad_norm": 0.1505858153104782,
      "learning_rate": 6.732778975464532e-05,
      "loss": 0.1774,
      "step": 32320
    },
    {
      "epoch": 1.9913766553741916,
      "grad_norm": 0.15721464157104492,
      "learning_rate": 6.728672620880814e-05,
      "loss": 0.1763,
      "step": 32330
    },
    {
      "epoch": 1.9919926085617492,
      "grad_norm": 0.13374795019626617,
      "learning_rate": 6.724566266297095e-05,
      "loss": 0.1764,
      "step": 32340
    },
    {
      "epoch": 1.992608561749307,
      "grad_norm": 0.1411515325307846,
      "learning_rate": 6.720459911713377e-05,
      "loss": 0.1768,
      "step": 32350
    },
    {
      "epoch": 1.9932245149368648,
      "grad_norm": 0.15126946568489075,
      "learning_rate": 6.716353557129659e-05,
      "loss": 0.1768,
      "step": 32360
    },
    {
      "epoch": 1.9938404681244224,
      "grad_norm": 0.1713605672121048,
      "learning_rate": 6.712247202545941e-05,
      "loss": 0.1773,
      "step": 32370
    },
    {
      "epoch": 1.9944564213119804,
      "grad_norm": 0.1647438257932663,
      "learning_rate": 6.708140847962221e-05,
      "loss": 0.1781,
      "step": 32380
    },
    {
      "epoch": 1.995072374499538,
      "grad_norm": 0.13725119829177856,
      "learning_rate": 6.704034493378503e-05,
      "loss": 0.1775,
      "step": 32390
    },
    {
      "epoch": 1.9956883276870958,
      "grad_norm": 0.15325041115283966,
      "learning_rate": 6.699928138794785e-05,
      "loss": 0.1772,
      "step": 32400
    },
    {
      "epoch": 1.9963042808746536,
      "grad_norm": 0.158436581492424,
      "learning_rate": 6.695821784211067e-05,
      "loss": 0.1747,
      "step": 32410
    },
    {
      "epoch": 1.9969202340622112,
      "grad_norm": 0.17596183717250824,
      "learning_rate": 6.691715429627348e-05,
      "loss": 0.1764,
      "step": 32420
    },
    {
      "epoch": 1.997536187249769,
      "grad_norm": 0.14573611319065094,
      "learning_rate": 6.68760907504363e-05,
      "loss": 0.1769,
      "step": 32430
    },
    {
      "epoch": 1.9981521404373268,
      "grad_norm": 0.17234335839748383,
      "learning_rate": 6.683502720459912e-05,
      "loss": 0.1764,
      "step": 32440
    },
    {
      "epoch": 1.9987680936248844,
      "grad_norm": 0.1843789666891098,
      "learning_rate": 6.679396365876194e-05,
      "loss": 0.1783,
      "step": 32450
    },
    {
      "epoch": 1.9993840468124424,
      "grad_norm": 0.16358429193496704,
      "learning_rate": 6.675290011292475e-05,
      "loss": 0.1788,
      "step": 32460
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.21145372092723846,
      "learning_rate": 6.671183656708757e-05,
      "loss": 0.1778,
      "step": 32470
    },
    {
      "epoch": 2.0006159531875576,
      "grad_norm": 0.12817616760730743,
      "learning_rate": 6.667077302125039e-05,
      "loss": 0.1764,
      "step": 32480
    },
    {
      "epoch": 2.0012319063751156,
      "grad_norm": 0.15841753780841827,
      "learning_rate": 6.662970947541321e-05,
      "loss": 0.1752,
      "step": 32490
    },
    {
      "epoch": 2.001847859562673,
      "grad_norm": 0.1245201826095581,
      "learning_rate": 6.658864592957603e-05,
      "loss": 0.1746,
      "step": 32500
    },
    {
      "epoch": 2.0024638127502308,
      "grad_norm": 0.13716110587120056,
      "learning_rate": 6.654758238373883e-05,
      "loss": 0.1758,
      "step": 32510
    },
    {
      "epoch": 2.003079765937789,
      "grad_norm": 0.1364455223083496,
      "learning_rate": 6.650651883790165e-05,
      "loss": 0.1748,
      "step": 32520
    },
    {
      "epoch": 2.0036957191253464,
      "grad_norm": 0.14197246730327606,
      "learning_rate": 6.646545529206447e-05,
      "loss": 0.1767,
      "step": 32530
    },
    {
      "epoch": 2.0043116723129044,
      "grad_norm": 0.13962675631046295,
      "learning_rate": 6.64243917462273e-05,
      "loss": 0.1748,
      "step": 32540
    },
    {
      "epoch": 2.004927625500462,
      "grad_norm": 0.15068893134593964,
      "learning_rate": 6.63833282003901e-05,
      "loss": 0.1765,
      "step": 32550
    },
    {
      "epoch": 2.0055435786880196,
      "grad_norm": 0.15776494145393372,
      "learning_rate": 6.634226465455292e-05,
      "loss": 0.1758,
      "step": 32560
    },
    {
      "epoch": 2.0061595318755776,
      "grad_norm": 0.15697944164276123,
      "learning_rate": 6.630120110871574e-05,
      "loss": 0.1774,
      "step": 32570
    },
    {
      "epoch": 2.006775485063135,
      "grad_norm": 0.16998516023159027,
      "learning_rate": 6.626013756287856e-05,
      "loss": 0.1772,
      "step": 32580
    },
    {
      "epoch": 2.0073914382506928,
      "grad_norm": 0.1614648699760437,
      "learning_rate": 6.621907401704137e-05,
      "loss": 0.1781,
      "step": 32590
    },
    {
      "epoch": 2.008007391438251,
      "grad_norm": 0.1489398032426834,
      "learning_rate": 6.617801047120419e-05,
      "loss": 0.177,
      "step": 32600
    },
    {
      "epoch": 2.0086233446258084,
      "grad_norm": 0.163407102227211,
      "learning_rate": 6.613694692536701e-05,
      "loss": 0.1757,
      "step": 32610
    },
    {
      "epoch": 2.009239297813366,
      "grad_norm": 0.18327555060386658,
      "learning_rate": 6.609588337952983e-05,
      "loss": 0.177,
      "step": 32620
    },
    {
      "epoch": 2.009855251000924,
      "grad_norm": 0.1736951470375061,
      "learning_rate": 6.605481983369263e-05,
      "loss": 0.1772,
      "step": 32630
    },
    {
      "epoch": 2.0104712041884816,
      "grad_norm": 0.17099790275096893,
      "learning_rate": 6.601375628785545e-05,
      "loss": 0.1784,
      "step": 32640
    },
    {
      "epoch": 2.0110871573760396,
      "grad_norm": 0.15553097426891327,
      "learning_rate": 6.597269274201827e-05,
      "loss": 0.1765,
      "step": 32650
    },
    {
      "epoch": 2.011703110563597,
      "grad_norm": 0.13377529382705688,
      "learning_rate": 6.59316291961811e-05,
      "loss": 0.1753,
      "step": 32660
    },
    {
      "epoch": 2.0123190637511548,
      "grad_norm": 0.18786540627479553,
      "learning_rate": 6.58905656503439e-05,
      "loss": 0.177,
      "step": 32670
    },
    {
      "epoch": 2.012935016938713,
      "grad_norm": 0.14495141804218292,
      "learning_rate": 6.584950210450673e-05,
      "loss": 0.1732,
      "step": 32680
    },
    {
      "epoch": 2.0135509701262704,
      "grad_norm": 0.15619371831417084,
      "learning_rate": 6.580843855866955e-05,
      "loss": 0.1753,
      "step": 32690
    },
    {
      "epoch": 2.014166923313828,
      "grad_norm": 0.14503836631774902,
      "learning_rate": 6.576737501283236e-05,
      "loss": 0.1774,
      "step": 32700
    },
    {
      "epoch": 2.014782876501386,
      "grad_norm": 0.13190165162086487,
      "learning_rate": 6.572631146699518e-05,
      "loss": 0.176,
      "step": 32710
    },
    {
      "epoch": 2.0153988296889436,
      "grad_norm": 0.14527879655361176,
      "learning_rate": 6.5685247921158e-05,
      "loss": 0.177,
      "step": 32720
    },
    {
      "epoch": 2.0160147828765016,
      "grad_norm": 0.21547198295593262,
      "learning_rate": 6.564418437532082e-05,
      "loss": 0.1756,
      "step": 32730
    },
    {
      "epoch": 2.016630736064059,
      "grad_norm": 0.15163679420948029,
      "learning_rate": 6.560312082948363e-05,
      "loss": 0.1786,
      "step": 32740
    },
    {
      "epoch": 2.0172466892516168,
      "grad_norm": 0.13416391611099243,
      "learning_rate": 6.556205728364645e-05,
      "loss": 0.1761,
      "step": 32750
    },
    {
      "epoch": 2.017862642439175,
      "grad_norm": 0.16618002951145172,
      "learning_rate": 6.552099373780927e-05,
      "loss": 0.1757,
      "step": 32760
    },
    {
      "epoch": 2.0184785956267324,
      "grad_norm": 0.16261816024780273,
      "learning_rate": 6.547993019197209e-05,
      "loss": 0.1765,
      "step": 32770
    },
    {
      "epoch": 2.01909454881429,
      "grad_norm": 0.15882407128810883,
      "learning_rate": 6.54388666461349e-05,
      "loss": 0.177,
      "step": 32780
    },
    {
      "epoch": 2.019710502001848,
      "grad_norm": 0.14622516930103302,
      "learning_rate": 6.539780310029771e-05,
      "loss": 0.1727,
      "step": 32790
    },
    {
      "epoch": 2.0203264551894056,
      "grad_norm": 0.13818007707595825,
      "learning_rate": 6.535673955446053e-05,
      "loss": 0.176,
      "step": 32800
    },
    {
      "epoch": 2.020942408376963,
      "grad_norm": 0.1594422459602356,
      "learning_rate": 6.531567600862335e-05,
      "loss": 0.1772,
      "step": 32810
    },
    {
      "epoch": 2.021558361564521,
      "grad_norm": 0.14579716324806213,
      "learning_rate": 6.527461246278616e-05,
      "loss": 0.1772,
      "step": 32820
    },
    {
      "epoch": 2.0221743147520788,
      "grad_norm": 0.14275209605693817,
      "learning_rate": 6.523354891694898e-05,
      "loss": 0.1778,
      "step": 32830
    },
    {
      "epoch": 2.022790267939637,
      "grad_norm": 0.1384073942899704,
      "learning_rate": 6.51924853711118e-05,
      "loss": 0.1765,
      "step": 32840
    },
    {
      "epoch": 2.0234062211271944,
      "grad_norm": 0.1379864662885666,
      "learning_rate": 6.515142182527462e-05,
      "loss": 0.1757,
      "step": 32850
    },
    {
      "epoch": 2.024022174314752,
      "grad_norm": 0.1434679925441742,
      "learning_rate": 6.511035827943743e-05,
      "loss": 0.1798,
      "step": 32860
    },
    {
      "epoch": 2.02463812750231,
      "grad_norm": 0.1309702843427658,
      "learning_rate": 6.506929473360025e-05,
      "loss": 0.1759,
      "step": 32870
    },
    {
      "epoch": 2.0252540806898676,
      "grad_norm": 0.12582886219024658,
      "learning_rate": 6.502823118776307e-05,
      "loss": 0.1756,
      "step": 32880
    },
    {
      "epoch": 2.025870033877425,
      "grad_norm": 0.22968807816505432,
      "learning_rate": 6.498716764192589e-05,
      "loss": 0.178,
      "step": 32890
    },
    {
      "epoch": 2.026485987064983,
      "grad_norm": 0.148912712931633,
      "learning_rate": 6.49461040960887e-05,
      "loss": 0.1788,
      "step": 32900
    },
    {
      "epoch": 2.0271019402525408,
      "grad_norm": 0.1558837592601776,
      "learning_rate": 6.490504055025151e-05,
      "loss": 0.1762,
      "step": 32910
    },
    {
      "epoch": 2.0277178934400983,
      "grad_norm": 0.14834460616111755,
      "learning_rate": 6.486397700441433e-05,
      "loss": 0.1767,
      "step": 32920
    },
    {
      "epoch": 2.0283338466276564,
      "grad_norm": 0.15115363895893097,
      "learning_rate": 6.482291345857715e-05,
      "loss": 0.1768,
      "step": 32930
    },
    {
      "epoch": 2.028949799815214,
      "grad_norm": 0.15287940204143524,
      "learning_rate": 6.478184991273996e-05,
      "loss": 0.1748,
      "step": 32940
    },
    {
      "epoch": 2.029565753002772,
      "grad_norm": 0.1769564300775528,
      "learning_rate": 6.474078636690278e-05,
      "loss": 0.1755,
      "step": 32950
    },
    {
      "epoch": 2.0301817061903296,
      "grad_norm": 0.1364963799715042,
      "learning_rate": 6.46997228210656e-05,
      "loss": 0.1763,
      "step": 32960
    },
    {
      "epoch": 2.030797659377887,
      "grad_norm": 0.1661844253540039,
      "learning_rate": 6.465865927522842e-05,
      "loss": 0.1765,
      "step": 32970
    },
    {
      "epoch": 2.031413612565445,
      "grad_norm": 0.1678575873374939,
      "learning_rate": 6.461759572939123e-05,
      "loss": 0.175,
      "step": 32980
    },
    {
      "epoch": 2.0320295657530028,
      "grad_norm": 0.15048305690288544,
      "learning_rate": 6.457653218355405e-05,
      "loss": 0.1769,
      "step": 32990
    },
    {
      "epoch": 2.0326455189405603,
      "grad_norm": 0.15049506723880768,
      "learning_rate": 6.453546863771687e-05,
      "loss": 0.176,
      "step": 33000
    },
    {
      "epoch": 2.0332614721281184,
      "grad_norm": 0.1545834243297577,
      "learning_rate": 6.449440509187969e-05,
      "loss": 0.1776,
      "step": 33010
    },
    {
      "epoch": 2.033877425315676,
      "grad_norm": 0.14399747550487518,
      "learning_rate": 6.445334154604251e-05,
      "loss": 0.1761,
      "step": 33020
    },
    {
      "epoch": 2.0344933785032335,
      "grad_norm": 0.14257857203483582,
      "learning_rate": 6.441227800020531e-05,
      "loss": 0.1771,
      "step": 33030
    },
    {
      "epoch": 2.0351093316907916,
      "grad_norm": 0.1600501388311386,
      "learning_rate": 6.437121445436813e-05,
      "loss": 0.1769,
      "step": 33040
    },
    {
      "epoch": 2.035725284878349,
      "grad_norm": 0.1467883437871933,
      "learning_rate": 6.433015090853095e-05,
      "loss": 0.1767,
      "step": 33050
    },
    {
      "epoch": 2.036341238065907,
      "grad_norm": 0.1521570384502411,
      "learning_rate": 6.428908736269377e-05,
      "loss": 0.1769,
      "step": 33060
    },
    {
      "epoch": 2.0369571912534647,
      "grad_norm": 0.14296869933605194,
      "learning_rate": 6.424802381685658e-05,
      "loss": 0.1775,
      "step": 33070
    },
    {
      "epoch": 2.0375731444410223,
      "grad_norm": 0.16224384307861328,
      "learning_rate": 6.42069602710194e-05,
      "loss": 0.1758,
      "step": 33080
    },
    {
      "epoch": 2.0381890976285804,
      "grad_norm": 0.13603278994560242,
      "learning_rate": 6.416589672518222e-05,
      "loss": 0.1767,
      "step": 33090
    },
    {
      "epoch": 2.038805050816138,
      "grad_norm": 0.14467297494411469,
      "learning_rate": 6.412483317934504e-05,
      "loss": 0.1768,
      "step": 33100
    },
    {
      "epoch": 2.0394210040036955,
      "grad_norm": 0.144292950630188,
      "learning_rate": 6.408376963350786e-05,
      "loss": 0.1757,
      "step": 33110
    },
    {
      "epoch": 2.0400369571912536,
      "grad_norm": 0.162292018532753,
      "learning_rate": 6.404270608767068e-05,
      "loss": 0.1783,
      "step": 33120
    },
    {
      "epoch": 2.040652910378811,
      "grad_norm": 0.16484317183494568,
      "learning_rate": 6.40016425418335e-05,
      "loss": 0.1771,
      "step": 33130
    },
    {
      "epoch": 2.041268863566369,
      "grad_norm": 0.16043367981910706,
      "learning_rate": 6.396057899599631e-05,
      "loss": 0.1767,
      "step": 33140
    },
    {
      "epoch": 2.0418848167539267,
      "grad_norm": 0.15095917880535126,
      "learning_rate": 6.391951545015913e-05,
      "loss": 0.1778,
      "step": 33150
    },
    {
      "epoch": 2.0425007699414843,
      "grad_norm": 0.14782437682151794,
      "learning_rate": 6.387845190432195e-05,
      "loss": 0.176,
      "step": 33160
    },
    {
      "epoch": 2.0431167231290424,
      "grad_norm": 0.1502925455570221,
      "learning_rate": 6.383738835848477e-05,
      "loss": 0.1788,
      "step": 33170
    },
    {
      "epoch": 2.0437326763166,
      "grad_norm": 0.14011543989181519,
      "learning_rate": 6.379632481264757e-05,
      "loss": 0.1762,
      "step": 33180
    },
    {
      "epoch": 2.0443486295041575,
      "grad_norm": 0.14160829782485962,
      "learning_rate": 6.37552612668104e-05,
      "loss": 0.1767,
      "step": 33190
    },
    {
      "epoch": 2.0449645826917155,
      "grad_norm": 0.15931051969528198,
      "learning_rate": 6.371419772097321e-05,
      "loss": 0.1773,
      "step": 33200
    },
    {
      "epoch": 2.045580535879273,
      "grad_norm": 0.15675319731235504,
      "learning_rate": 6.367313417513603e-05,
      "loss": 0.1787,
      "step": 33210
    },
    {
      "epoch": 2.0461964890668307,
      "grad_norm": 0.13624072074890137,
      "learning_rate": 6.363207062929884e-05,
      "loss": 0.1759,
      "step": 33220
    },
    {
      "epoch": 2.0468124422543887,
      "grad_norm": 0.15628020465373993,
      "learning_rate": 6.359100708346166e-05,
      "loss": 0.1765,
      "step": 33230
    },
    {
      "epoch": 2.0474283954419463,
      "grad_norm": 0.14080844819545746,
      "learning_rate": 6.354994353762448e-05,
      "loss": 0.1775,
      "step": 33240
    },
    {
      "epoch": 2.0480443486295044,
      "grad_norm": 0.1794189065694809,
      "learning_rate": 6.35088799917873e-05,
      "loss": 0.179,
      "step": 33250
    },
    {
      "epoch": 2.048660301817062,
      "grad_norm": 0.1697463095188141,
      "learning_rate": 6.346781644595011e-05,
      "loss": 0.1775,
      "step": 33260
    },
    {
      "epoch": 2.0492762550046195,
      "grad_norm": 0.13938620686531067,
      "learning_rate": 6.342675290011293e-05,
      "loss": 0.1763,
      "step": 33270
    },
    {
      "epoch": 2.0498922081921775,
      "grad_norm": 0.15176643431186676,
      "learning_rate": 6.338568935427575e-05,
      "loss": 0.178,
      "step": 33280
    },
    {
      "epoch": 2.050508161379735,
      "grad_norm": 0.18168741464614868,
      "learning_rate": 6.334462580843857e-05,
      "loss": 0.1765,
      "step": 33290
    },
    {
      "epoch": 2.0511241145672927,
      "grad_norm": 0.15248578786849976,
      "learning_rate": 6.330356226260137e-05,
      "loss": 0.1771,
      "step": 33300
    },
    {
      "epoch": 2.0517400677548507,
      "grad_norm": 0.1628560721874237,
      "learning_rate": 6.32624987167642e-05,
      "loss": 0.1753,
      "step": 33310
    },
    {
      "epoch": 2.0523560209424083,
      "grad_norm": 0.14472956955432892,
      "learning_rate": 6.322143517092701e-05,
      "loss": 0.1764,
      "step": 33320
    },
    {
      "epoch": 2.052971974129966,
      "grad_norm": 0.14510373771190643,
      "learning_rate": 6.318037162508983e-05,
      "loss": 0.1764,
      "step": 33330
    },
    {
      "epoch": 2.053587927317524,
      "grad_norm": 0.1561744511127472,
      "learning_rate": 6.313930807925264e-05,
      "loss": 0.1761,
      "step": 33340
    },
    {
      "epoch": 2.0542038805050815,
      "grad_norm": 0.16398490965366364,
      "learning_rate": 6.309824453341546e-05,
      "loss": 0.1768,
      "step": 33350
    },
    {
      "epoch": 2.0548198336926395,
      "grad_norm": 0.16645123064517975,
      "learning_rate": 6.305718098757828e-05,
      "loss": 0.1762,
      "step": 33360
    },
    {
      "epoch": 2.055435786880197,
      "grad_norm": 0.15637651085853577,
      "learning_rate": 6.30161174417411e-05,
      "loss": 0.1766,
      "step": 33370
    },
    {
      "epoch": 2.0560517400677547,
      "grad_norm": 0.13661566376686096,
      "learning_rate": 6.29750538959039e-05,
      "loss": 0.179,
      "step": 33380
    },
    {
      "epoch": 2.0566676932553127,
      "grad_norm": 0.14073854684829712,
      "learning_rate": 6.293399035006673e-05,
      "loss": 0.178,
      "step": 33390
    },
    {
      "epoch": 2.0572836464428703,
      "grad_norm": 0.15293091535568237,
      "learning_rate": 6.289292680422955e-05,
      "loss": 0.1771,
      "step": 33400
    },
    {
      "epoch": 2.057899599630428,
      "grad_norm": 0.1586216539144516,
      "learning_rate": 6.285186325839237e-05,
      "loss": 0.1765,
      "step": 33410
    },
    {
      "epoch": 2.058515552817986,
      "grad_norm": 0.12310894578695297,
      "learning_rate": 6.281079971255517e-05,
      "loss": 0.1749,
      "step": 33420
    },
    {
      "epoch": 2.0591315060055435,
      "grad_norm": 0.15639173984527588,
      "learning_rate": 6.2769736166718e-05,
      "loss": 0.1765,
      "step": 33430
    },
    {
      "epoch": 2.0597474591931015,
      "grad_norm": 0.14915436506271362,
      "learning_rate": 6.272867262088081e-05,
      "loss": 0.1773,
      "step": 33440
    },
    {
      "epoch": 2.060363412380659,
      "grad_norm": 0.16890573501586914,
      "learning_rate": 6.268760907504363e-05,
      "loss": 0.1767,
      "step": 33450
    },
    {
      "epoch": 2.0609793655682167,
      "grad_norm": 0.17920994758605957,
      "learning_rate": 6.264654552920644e-05,
      "loss": 0.1766,
      "step": 33460
    },
    {
      "epoch": 2.0615953187557747,
      "grad_norm": 0.14439059793949127,
      "learning_rate": 6.260548198336926e-05,
      "loss": 0.1756,
      "step": 33470
    },
    {
      "epoch": 2.0622112719433323,
      "grad_norm": 0.1435507982969284,
      "learning_rate": 6.256441843753208e-05,
      "loss": 0.1762,
      "step": 33480
    },
    {
      "epoch": 2.06282722513089,
      "grad_norm": 0.15337534248828888,
      "learning_rate": 6.25233548916949e-05,
      "loss": 0.1758,
      "step": 33490
    },
    {
      "epoch": 2.063443178318448,
      "grad_norm": 0.15464065968990326,
      "learning_rate": 6.24822913458577e-05,
      "loss": 0.1748,
      "step": 33500
    },
    {
      "epoch": 2.0640591315060055,
      "grad_norm": 0.15481369197368622,
      "learning_rate": 6.244122780002053e-05,
      "loss": 0.1767,
      "step": 33510
    },
    {
      "epoch": 2.064675084693563,
      "grad_norm": 0.15391921997070312,
      "learning_rate": 6.240016425418335e-05,
      "loss": 0.1776,
      "step": 33520
    },
    {
      "epoch": 2.065291037881121,
      "grad_norm": 0.16265396773815155,
      "learning_rate": 6.235910070834617e-05,
      "loss": 0.1758,
      "step": 33530
    },
    {
      "epoch": 2.0659069910686787,
      "grad_norm": 0.12413890659809113,
      "learning_rate": 6.231803716250899e-05,
      "loss": 0.1773,
      "step": 33540
    },
    {
      "epoch": 2.0665229442562367,
      "grad_norm": 0.1593216508626938,
      "learning_rate": 6.22769736166718e-05,
      "loss": 0.1771,
      "step": 33550
    },
    {
      "epoch": 2.0671388974437943,
      "grad_norm": 0.13368532061576843,
      "learning_rate": 6.223591007083463e-05,
      "loss": 0.1759,
      "step": 33560
    },
    {
      "epoch": 2.067754850631352,
      "grad_norm": 0.16026777029037476,
      "learning_rate": 6.219484652499745e-05,
      "loss": 0.177,
      "step": 33570
    },
    {
      "epoch": 2.06837080381891,
      "grad_norm": 0.12799297273159027,
      "learning_rate": 6.215378297916025e-05,
      "loss": 0.1796,
      "step": 33580
    },
    {
      "epoch": 2.0689867570064675,
      "grad_norm": 0.14701436460018158,
      "learning_rate": 6.211271943332307e-05,
      "loss": 0.176,
      "step": 33590
    },
    {
      "epoch": 2.069602710194025,
      "grad_norm": 0.13550904393196106,
      "learning_rate": 6.20716558874859e-05,
      "loss": 0.1766,
      "step": 33600
    },
    {
      "epoch": 2.070218663381583,
      "grad_norm": 0.1422344446182251,
      "learning_rate": 6.203059234164871e-05,
      "loss": 0.1766,
      "step": 33610
    },
    {
      "epoch": 2.0708346165691407,
      "grad_norm": 0.1373552680015564,
      "learning_rate": 6.198952879581152e-05,
      "loss": 0.175,
      "step": 33620
    },
    {
      "epoch": 2.0714505697566983,
      "grad_norm": 0.15372146666049957,
      "learning_rate": 6.194846524997434e-05,
      "loss": 0.1761,
      "step": 33630
    },
    {
      "epoch": 2.0720665229442563,
      "grad_norm": 0.16728386282920837,
      "learning_rate": 6.190740170413716e-05,
      "loss": 0.175,
      "step": 33640
    },
    {
      "epoch": 2.072682476131814,
      "grad_norm": 0.15989713370800018,
      "learning_rate": 6.187044451288368e-05,
      "loss": 0.1764,
      "step": 33650
    },
    {
      "epoch": 2.073298429319372,
      "grad_norm": 0.14222735166549683,
      "learning_rate": 6.182938096704652e-05,
      "loss": 0.1761,
      "step": 33660
    },
    {
      "epoch": 2.0739143825069295,
      "grad_norm": 0.14195197820663452,
      "learning_rate": 6.178831742120932e-05,
      "loss": 0.1771,
      "step": 33670
    },
    {
      "epoch": 2.074530335694487,
      "grad_norm": 0.14133810997009277,
      "learning_rate": 6.174725387537214e-05,
      "loss": 0.1764,
      "step": 33680
    },
    {
      "epoch": 2.075146288882045,
      "grad_norm": 0.1490897536277771,
      "learning_rate": 6.170619032953496e-05,
      "loss": 0.1751,
      "step": 33690
    },
    {
      "epoch": 2.0757622420696027,
      "grad_norm": 0.15609116852283478,
      "learning_rate": 6.166512678369778e-05,
      "loss": 0.1767,
      "step": 33700
    },
    {
      "epoch": 2.0763781952571603,
      "grad_norm": 0.13835018873214722,
      "learning_rate": 6.162406323786059e-05,
      "loss": 0.1753,
      "step": 33710
    },
    {
      "epoch": 2.0769941484447183,
      "grad_norm": 0.1286226361989975,
      "learning_rate": 6.158299969202341e-05,
      "loss": 0.1775,
      "step": 33720
    },
    {
      "epoch": 2.077610101632276,
      "grad_norm": 0.1556454747915268,
      "learning_rate": 6.154193614618623e-05,
      "loss": 0.1774,
      "step": 33730
    },
    {
      "epoch": 2.078226054819834,
      "grad_norm": 0.13969570398330688,
      "learning_rate": 6.150087260034905e-05,
      "loss": 0.1772,
      "step": 33740
    },
    {
      "epoch": 2.0788420080073915,
      "grad_norm": 0.1696348339319229,
      "learning_rate": 6.145980905451186e-05,
      "loss": 0.1757,
      "step": 33750
    },
    {
      "epoch": 2.079457961194949,
      "grad_norm": 0.19403080642223358,
      "learning_rate": 6.141874550867468e-05,
      "loss": 0.1773,
      "step": 33760
    },
    {
      "epoch": 2.080073914382507,
      "grad_norm": 0.17017610371112823,
      "learning_rate": 6.13776819628375e-05,
      "loss": 0.1764,
      "step": 33770
    },
    {
      "epoch": 2.0806898675700647,
      "grad_norm": 0.15855136513710022,
      "learning_rate": 6.133661841700032e-05,
      "loss": 0.1771,
      "step": 33780
    },
    {
      "epoch": 2.0813058207576223,
      "grad_norm": 0.1406763345003128,
      "learning_rate": 6.129555487116312e-05,
      "loss": 0.1758,
      "step": 33790
    },
    {
      "epoch": 2.0819217739451803,
      "grad_norm": 0.18404266238212585,
      "learning_rate": 6.125449132532594e-05,
      "loss": 0.1768,
      "step": 33800
    },
    {
      "epoch": 2.082537727132738,
      "grad_norm": 0.16564658284187317,
      "learning_rate": 6.121342777948876e-05,
      "loss": 0.176,
      "step": 33810
    },
    {
      "epoch": 2.0831536803202955,
      "grad_norm": 0.14316122233867645,
      "learning_rate": 6.117236423365158e-05,
      "loss": 0.1757,
      "step": 33820
    },
    {
      "epoch": 2.0837696335078535,
      "grad_norm": 0.18573002517223358,
      "learning_rate": 6.113130068781439e-05,
      "loss": 0.1749,
      "step": 33830
    },
    {
      "epoch": 2.084385586695411,
      "grad_norm": 0.14285452663898468,
      "learning_rate": 6.109023714197721e-05,
      "loss": 0.1766,
      "step": 33840
    },
    {
      "epoch": 2.085001539882969,
      "grad_norm": 0.1339879035949707,
      "learning_rate": 6.104917359614003e-05,
      "loss": 0.1773,
      "step": 33850
    },
    {
      "epoch": 2.0856174930705267,
      "grad_norm": 0.13752956688404083,
      "learning_rate": 6.1008110050302844e-05,
      "loss": 0.1764,
      "step": 33860
    },
    {
      "epoch": 2.0862334462580843,
      "grad_norm": 0.16781309247016907,
      "learning_rate": 6.0967046504465664e-05,
      "loss": 0.1776,
      "step": 33870
    },
    {
      "epoch": 2.0868493994456423,
      "grad_norm": 0.15470843017101288,
      "learning_rate": 6.092598295862848e-05,
      "loss": 0.1766,
      "step": 33880
    },
    {
      "epoch": 2.0874653526332,
      "grad_norm": 0.13486967980861664,
      "learning_rate": 6.08849194127913e-05,
      "loss": 0.1763,
      "step": 33890
    },
    {
      "epoch": 2.0880813058207575,
      "grad_norm": 0.1386694312095642,
      "learning_rate": 6.084385586695411e-05,
      "loss": 0.1768,
      "step": 33900
    },
    {
      "epoch": 2.0886972590083155,
      "grad_norm": 0.14167208969593048,
      "learning_rate": 6.080279232111693e-05,
      "loss": 0.1769,
      "step": 33910
    },
    {
      "epoch": 2.089313212195873,
      "grad_norm": 0.14881959557533264,
      "learning_rate": 6.0761728775279744e-05,
      "loss": 0.1757,
      "step": 33920
    },
    {
      "epoch": 2.0899291653834307,
      "grad_norm": 0.15131376683712006,
      "learning_rate": 6.0720665229442564e-05,
      "loss": 0.1774,
      "step": 33930
    },
    {
      "epoch": 2.0905451185709887,
      "grad_norm": 0.16282877326011658,
      "learning_rate": 6.067960168360538e-05,
      "loss": 0.1774,
      "step": 33940
    },
    {
      "epoch": 2.0911610717585463,
      "grad_norm": 0.15307435393333435,
      "learning_rate": 6.06385381377682e-05,
      "loss": 0.1771,
      "step": 33950
    },
    {
      "epoch": 2.0917770249461043,
      "grad_norm": 0.18624788522720337,
      "learning_rate": 6.059747459193101e-05,
      "loss": 0.1753,
      "step": 33960
    },
    {
      "epoch": 2.092392978133662,
      "grad_norm": 0.17027951776981354,
      "learning_rate": 6.055641104609383e-05,
      "loss": 0.1769,
      "step": 33970
    },
    {
      "epoch": 2.0930089313212195,
      "grad_norm": 0.15622299909591675,
      "learning_rate": 6.0515347500256644e-05,
      "loss": 0.1754,
      "step": 33980
    },
    {
      "epoch": 2.0936248845087775,
      "grad_norm": 0.14337752759456635,
      "learning_rate": 6.0474283954419464e-05,
      "loss": 0.1763,
      "step": 33990
    },
    {
      "epoch": 2.094240837696335,
      "grad_norm": 0.15517722070217133,
      "learning_rate": 6.0433220408582284e-05,
      "loss": 0.1765,
      "step": 34000
    },
    {
      "epoch": 2.0948567908838926,
      "grad_norm": 0.16294057667255402,
      "learning_rate": 6.03921568627451e-05,
      "loss": 0.1756,
      "step": 34010
    },
    {
      "epoch": 2.0954727440714507,
      "grad_norm": 0.1457994282245636,
      "learning_rate": 6.035109331690792e-05,
      "loss": 0.1772,
      "step": 34020
    },
    {
      "epoch": 2.0960886972590083,
      "grad_norm": 0.1455228626728058,
      "learning_rate": 6.031002977107073e-05,
      "loss": 0.1759,
      "step": 34030
    },
    {
      "epoch": 2.096704650446566,
      "grad_norm": 0.17022758722305298,
      "learning_rate": 6.026896622523355e-05,
      "loss": 0.1772,
      "step": 34040
    },
    {
      "epoch": 2.097320603634124,
      "grad_norm": 0.13793857395648956,
      "learning_rate": 6.0227902679396364e-05,
      "loss": 0.1758,
      "step": 34050
    },
    {
      "epoch": 2.0979365568216815,
      "grad_norm": 0.1501275897026062,
      "learning_rate": 6.0186839133559184e-05,
      "loss": 0.1763,
      "step": 34060
    },
    {
      "epoch": 2.0985525100092395,
      "grad_norm": 0.14438468217849731,
      "learning_rate": 6.0145775587722e-05,
      "loss": 0.1771,
      "step": 34070
    },
    {
      "epoch": 2.099168463196797,
      "grad_norm": 0.16416069865226746,
      "learning_rate": 6.010471204188482e-05,
      "loss": 0.177,
      "step": 34080
    },
    {
      "epoch": 2.0997844163843546,
      "grad_norm": 0.15606188774108887,
      "learning_rate": 6.0063648496047644e-05,
      "loss": 0.1768,
      "step": 34090
    },
    {
      "epoch": 2.1004003695719127,
      "grad_norm": 0.12651178240776062,
      "learning_rate": 6.002258495021046e-05,
      "loss": 0.1776,
      "step": 34100
    },
    {
      "epoch": 2.1010163227594703,
      "grad_norm": 0.14065013825893402,
      "learning_rate": 5.998562775895699e-05,
      "loss": 0.1773,
      "step": 34110
    },
    {
      "epoch": 2.101632275947028,
      "grad_norm": 0.12808722257614136,
      "learning_rate": 5.99445642131198e-05,
      "loss": 0.1772,
      "step": 34120
    },
    {
      "epoch": 2.102248229134586,
      "grad_norm": 0.1520022302865982,
      "learning_rate": 5.990350066728262e-05,
      "loss": 0.1763,
      "step": 34130
    },
    {
      "epoch": 2.1028641823221434,
      "grad_norm": 0.15816496312618256,
      "learning_rate": 5.9862437121445435e-05,
      "loss": 0.1765,
      "step": 34140
    },
    {
      "epoch": 2.1034801355097015,
      "grad_norm": 0.16146564483642578,
      "learning_rate": 5.9821373575608255e-05,
      "loss": 0.1774,
      "step": 34150
    },
    {
      "epoch": 2.104096088697259,
      "grad_norm": 0.1512453407049179,
      "learning_rate": 5.978031002977107e-05,
      "loss": 0.1752,
      "step": 34160
    },
    {
      "epoch": 2.1047120418848166,
      "grad_norm": 0.15677472949028015,
      "learning_rate": 5.973924648393389e-05,
      "loss": 0.1774,
      "step": 34170
    },
    {
      "epoch": 2.1053279950723747,
      "grad_norm": 0.16840454936027527,
      "learning_rate": 5.96981829380967e-05,
      "loss": 0.1768,
      "step": 34180
    },
    {
      "epoch": 2.1059439482599323,
      "grad_norm": 0.1360769271850586,
      "learning_rate": 5.965711939225952e-05,
      "loss": 0.1758,
      "step": 34190
    },
    {
      "epoch": 2.10655990144749,
      "grad_norm": 0.13859766721725464,
      "learning_rate": 5.9616055846422335e-05,
      "loss": 0.1763,
      "step": 34200
    },
    {
      "epoch": 2.107175854635048,
      "grad_norm": 0.13707491755485535,
      "learning_rate": 5.9574992300585155e-05,
      "loss": 0.1767,
      "step": 34210
    },
    {
      "epoch": 2.1077918078226054,
      "grad_norm": 0.14609771966934204,
      "learning_rate": 5.953392875474798e-05,
      "loss": 0.1747,
      "step": 34220
    },
    {
      "epoch": 2.108407761010163,
      "grad_norm": 0.15688079595565796,
      "learning_rate": 5.9492865208910795e-05,
      "loss": 0.1748,
      "step": 34230
    },
    {
      "epoch": 2.109023714197721,
      "grad_norm": 0.14810538291931152,
      "learning_rate": 5.9451801663073615e-05,
      "loss": 0.1765,
      "step": 34240
    },
    {
      "epoch": 2.1096396673852786,
      "grad_norm": 0.14363276958465576,
      "learning_rate": 5.941073811723643e-05,
      "loss": 0.1769,
      "step": 34250
    },
    {
      "epoch": 2.1102556205728367,
      "grad_norm": 0.12970763444900513,
      "learning_rate": 5.936967457139925e-05,
      "loss": 0.1792,
      "step": 34260
    },
    {
      "epoch": 2.1108715737603942,
      "grad_norm": 0.15005598962306976,
      "learning_rate": 5.932861102556206e-05,
      "loss": 0.1766,
      "step": 34270
    },
    {
      "epoch": 2.111487526947952,
      "grad_norm": 0.1583258956670761,
      "learning_rate": 5.928754747972488e-05,
      "loss": 0.175,
      "step": 34280
    },
    {
      "epoch": 2.11210348013551,
      "grad_norm": 0.12477146089076996,
      "learning_rate": 5.9246483933887695e-05,
      "loss": 0.1755,
      "step": 34290
    },
    {
      "epoch": 2.1127194333230674,
      "grad_norm": 0.1768050491809845,
      "learning_rate": 5.9205420388050515e-05,
      "loss": 0.1762,
      "step": 34300
    },
    {
      "epoch": 2.113335386510625,
      "grad_norm": 0.13743749260902405,
      "learning_rate": 5.916435684221333e-05,
      "loss": 0.1763,
      "step": 34310
    },
    {
      "epoch": 2.113951339698183,
      "grad_norm": 0.13854078948497772,
      "learning_rate": 5.912329329637615e-05,
      "loss": 0.1775,
      "step": 34320
    },
    {
      "epoch": 2.1145672928857406,
      "grad_norm": 0.1558571755886078,
      "learning_rate": 5.908222975053896e-05,
      "loss": 0.1764,
      "step": 34330
    },
    {
      "epoch": 2.115183246073298,
      "grad_norm": 0.14000840485095978,
      "learning_rate": 5.904116620470178e-05,
      "loss": 0.1756,
      "step": 34340
    },
    {
      "epoch": 2.1157991992608562,
      "grad_norm": 0.15264832973480225,
      "learning_rate": 5.9000102658864595e-05,
      "loss": 0.1759,
      "step": 34350
    },
    {
      "epoch": 2.116415152448414,
      "grad_norm": 0.15000009536743164,
      "learning_rate": 5.8959039113027415e-05,
      "loss": 0.1781,
      "step": 34360
    },
    {
      "epoch": 2.117031105635972,
      "grad_norm": 0.13015972077846527,
      "learning_rate": 5.891797556719023e-05,
      "loss": 0.1758,
      "step": 34370
    },
    {
      "epoch": 2.1176470588235294,
      "grad_norm": 0.1668989360332489,
      "learning_rate": 5.887691202135305e-05,
      "loss": 0.177,
      "step": 34380
    },
    {
      "epoch": 2.118263012011087,
      "grad_norm": 0.1528465449810028,
      "learning_rate": 5.883584847551586e-05,
      "loss": 0.1762,
      "step": 34390
    },
    {
      "epoch": 2.118878965198645,
      "grad_norm": 0.144870787858963,
      "learning_rate": 5.879478492967868e-05,
      "loss": 0.1765,
      "step": 34400
    },
    {
      "epoch": 2.1194949183862026,
      "grad_norm": 0.139651358127594,
      "learning_rate": 5.8753721383841495e-05,
      "loss": 0.1781,
      "step": 34410
    },
    {
      "epoch": 2.12011087157376,
      "grad_norm": 0.16014184057712555,
      "learning_rate": 5.8712657838004315e-05,
      "loss": 0.1773,
      "step": 34420
    },
    {
      "epoch": 2.1207268247613182,
      "grad_norm": 0.15210410952568054,
      "learning_rate": 5.867159429216713e-05,
      "loss": 0.1764,
      "step": 34430
    },
    {
      "epoch": 2.121342777948876,
      "grad_norm": 0.15099750459194183,
      "learning_rate": 5.863053074632995e-05,
      "loss": 0.1757,
      "step": 34440
    },
    {
      "epoch": 2.1219587311364334,
      "grad_norm": 0.14796051383018494,
      "learning_rate": 5.858946720049276e-05,
      "loss": 0.1769,
      "step": 34450
    },
    {
      "epoch": 2.1225746843239914,
      "grad_norm": 0.14597319066524506,
      "learning_rate": 5.854840365465558e-05,
      "loss": 0.1768,
      "step": 34460
    },
    {
      "epoch": 2.123190637511549,
      "grad_norm": 0.14422079920768738,
      "learning_rate": 5.85073401088184e-05,
      "loss": 0.1772,
      "step": 34470
    },
    {
      "epoch": 2.123806590699107,
      "grad_norm": 0.1464640498161316,
      "learning_rate": 5.8466276562981215e-05,
      "loss": 0.1768,
      "step": 34480
    },
    {
      "epoch": 2.1244225438866646,
      "grad_norm": 0.15552759170532227,
      "learning_rate": 5.8425213017144035e-05,
      "loss": 0.176,
      "step": 34490
    },
    {
      "epoch": 2.125038497074222,
      "grad_norm": 0.14579007029533386,
      "learning_rate": 5.838414947130685e-05,
      "loss": 0.1765,
      "step": 34500
    },
    {
      "epoch": 2.1256544502617802,
      "grad_norm": 0.1640084981918335,
      "learning_rate": 5.834308592546967e-05,
      "loss": 0.1767,
      "step": 34510
    },
    {
      "epoch": 2.126270403449338,
      "grad_norm": 0.1464080512523651,
      "learning_rate": 5.830202237963248e-05,
      "loss": 0.1764,
      "step": 34520
    },
    {
      "epoch": 2.1268863566368954,
      "grad_norm": 0.15615157783031464,
      "learning_rate": 5.82609588337953e-05,
      "loss": 0.1767,
      "step": 34530
    },
    {
      "epoch": 2.1275023098244534,
      "grad_norm": 0.13150279223918915,
      "learning_rate": 5.8219895287958115e-05,
      "loss": 0.1752,
      "step": 34540
    },
    {
      "epoch": 2.128118263012011,
      "grad_norm": 0.13824133574962616,
      "learning_rate": 5.8178831742120935e-05,
      "loss": 0.1754,
      "step": 34550
    },
    {
      "epoch": 2.128734216199569,
      "grad_norm": 0.1612313985824585,
      "learning_rate": 5.813776819628375e-05,
      "loss": 0.1773,
      "step": 34560
    },
    {
      "epoch": 2.1293501693871266,
      "grad_norm": 0.16557709872722626,
      "learning_rate": 5.809670465044657e-05,
      "loss": 0.1775,
      "step": 34570
    },
    {
      "epoch": 2.129966122574684,
      "grad_norm": 0.17056596279144287,
      "learning_rate": 5.805564110460938e-05,
      "loss": 0.1766,
      "step": 34580
    },
    {
      "epoch": 2.1305820757622422,
      "grad_norm": 0.16681024432182312,
      "learning_rate": 5.80145775587722e-05,
      "loss": 0.1764,
      "step": 34590
    },
    {
      "epoch": 2.1311980289498,
      "grad_norm": 0.16455797851085663,
      "learning_rate": 5.7973514012935015e-05,
      "loss": 0.175,
      "step": 34600
    },
    {
      "epoch": 2.1318139821373574,
      "grad_norm": 0.18689066171646118,
      "learning_rate": 5.7932450467097835e-05,
      "loss": 0.1765,
      "step": 34610
    },
    {
      "epoch": 2.1324299353249154,
      "grad_norm": 0.14519453048706055,
      "learning_rate": 5.789138692126065e-05,
      "loss": 0.1775,
      "step": 34620
    },
    {
      "epoch": 2.133045888512473,
      "grad_norm": 0.16397543251514435,
      "learning_rate": 5.785032337542347e-05,
      "loss": 0.1769,
      "step": 34630
    },
    {
      "epoch": 2.1336618417000306,
      "grad_norm": 0.1661037802696228,
      "learning_rate": 5.7809259829586295e-05,
      "loss": 0.1771,
      "step": 34640
    },
    {
      "epoch": 2.1342777948875886,
      "grad_norm": 0.13574644923210144,
      "learning_rate": 5.776819628374911e-05,
      "loss": 0.1765,
      "step": 34650
    },
    {
      "epoch": 2.134893748075146,
      "grad_norm": 0.14478997886180878,
      "learning_rate": 5.772713273791193e-05,
      "loss": 0.1767,
      "step": 34660
    },
    {
      "epoch": 2.1355097012627042,
      "grad_norm": 0.15428707003593445,
      "learning_rate": 5.768606919207474e-05,
      "loss": 0.1763,
      "step": 34670
    },
    {
      "epoch": 2.136125654450262,
      "grad_norm": 0.1530151665210724,
      "learning_rate": 5.764500564623756e-05,
      "loss": 0.1774,
      "step": 34680
    },
    {
      "epoch": 2.1367416076378194,
      "grad_norm": 0.14030441641807556,
      "learning_rate": 5.7603942100400375e-05,
      "loss": 0.1758,
      "step": 34690
    },
    {
      "epoch": 2.1373575608253774,
      "grad_norm": 0.17477741837501526,
      "learning_rate": 5.7562878554563195e-05,
      "loss": 0.1777,
      "step": 34700
    },
    {
      "epoch": 2.137973514012935,
      "grad_norm": 0.15306103229522705,
      "learning_rate": 5.752181500872601e-05,
      "loss": 0.1768,
      "step": 34710
    },
    {
      "epoch": 2.1385894672004926,
      "grad_norm": 0.1678733229637146,
      "learning_rate": 5.748075146288883e-05,
      "loss": 0.1742,
      "step": 34720
    },
    {
      "epoch": 2.1392054203880506,
      "grad_norm": 0.15896882116794586,
      "learning_rate": 5.743968791705164e-05,
      "loss": 0.174,
      "step": 34730
    },
    {
      "epoch": 2.139821373575608,
      "grad_norm": 0.16660957038402557,
      "learning_rate": 5.739862437121446e-05,
      "loss": 0.1774,
      "step": 34740
    },
    {
      "epoch": 2.1404373267631662,
      "grad_norm": 0.1350625902414322,
      "learning_rate": 5.7357560825377274e-05,
      "loss": 0.1763,
      "step": 34750
    },
    {
      "epoch": 2.141053279950724,
      "grad_norm": 0.1744842380285263,
      "learning_rate": 5.7316497279540094e-05,
      "loss": 0.1759,
      "step": 34760
    },
    {
      "epoch": 2.1416692331382814,
      "grad_norm": 0.17604056000709534,
      "learning_rate": 5.727543373370291e-05,
      "loss": 0.1758,
      "step": 34770
    },
    {
      "epoch": 2.1422851863258394,
      "grad_norm": 0.16359888017177582,
      "learning_rate": 5.723437018786573e-05,
      "loss": 0.1758,
      "step": 34780
    },
    {
      "epoch": 2.142901139513397,
      "grad_norm": 0.1879001259803772,
      "learning_rate": 5.719330664202854e-05,
      "loss": 0.1763,
      "step": 34790
    },
    {
      "epoch": 2.1435170927009546,
      "grad_norm": 0.156101256608963,
      "learning_rate": 5.715224309619136e-05,
      "loss": 0.1781,
      "step": 34800
    },
    {
      "epoch": 2.1441330458885126,
      "grad_norm": 0.14536932110786438,
      "learning_rate": 5.7111179550354174e-05,
      "loss": 0.1768,
      "step": 34810
    },
    {
      "epoch": 2.14474899907607,
      "grad_norm": 0.16360433399677277,
      "learning_rate": 5.7070116004516994e-05,
      "loss": 0.1763,
      "step": 34820
    },
    {
      "epoch": 2.145364952263628,
      "grad_norm": 0.16810385882854462,
      "learning_rate": 5.702905245867981e-05,
      "loss": 0.1756,
      "step": 34830
    },
    {
      "epoch": 2.145980905451186,
      "grad_norm": 0.13710519671440125,
      "learning_rate": 5.698798891284263e-05,
      "loss": 0.1763,
      "step": 34840
    },
    {
      "epoch": 2.1465968586387434,
      "grad_norm": 0.14124171435832977,
      "learning_rate": 5.694692536700544e-05,
      "loss": 0.1767,
      "step": 34850
    },
    {
      "epoch": 2.147212811826301,
      "grad_norm": 0.15805770456790924,
      "learning_rate": 5.690586182116826e-05,
      "loss": 0.1771,
      "step": 34860
    },
    {
      "epoch": 2.147828765013859,
      "grad_norm": 0.14526642858982086,
      "learning_rate": 5.6864798275331074e-05,
      "loss": 0.1759,
      "step": 34870
    },
    {
      "epoch": 2.1484447182014166,
      "grad_norm": 0.15775123238563538,
      "learning_rate": 5.6823734729493894e-05,
      "loss": 0.1771,
      "step": 34880
    },
    {
      "epoch": 2.1490606713889746,
      "grad_norm": 0.14899975061416626,
      "learning_rate": 5.678267118365671e-05,
      "loss": 0.1756,
      "step": 34890
    },
    {
      "epoch": 2.149676624576532,
      "grad_norm": 0.1635810285806656,
      "learning_rate": 5.674160763781953e-05,
      "loss": 0.1766,
      "step": 34900
    },
    {
      "epoch": 2.1502925777640898,
      "grad_norm": 0.14814898371696472,
      "learning_rate": 5.670054409198234e-05,
      "loss": 0.1774,
      "step": 34910
    },
    {
      "epoch": 2.150908530951648,
      "grad_norm": 0.1433587223291397,
      "learning_rate": 5.665948054614516e-05,
      "loss": 0.1778,
      "step": 34920
    },
    {
      "epoch": 2.1515244841392054,
      "grad_norm": 0.1579388976097107,
      "learning_rate": 5.6618417000307974e-05,
      "loss": 0.1776,
      "step": 34930
    },
    {
      "epoch": 2.152140437326763,
      "grad_norm": 0.15478487312793732,
      "learning_rate": 5.6577353454470794e-05,
      "loss": 0.1756,
      "step": 34940
    },
    {
      "epoch": 2.152756390514321,
      "grad_norm": 0.13672581315040588,
      "learning_rate": 5.653628990863361e-05,
      "loss": 0.1757,
      "step": 34950
    },
    {
      "epoch": 2.1533723437018786,
      "grad_norm": 0.17356689274311066,
      "learning_rate": 5.649522636279643e-05,
      "loss": 0.1784,
      "step": 34960
    },
    {
      "epoch": 2.1539882968894366,
      "grad_norm": 0.1512250006198883,
      "learning_rate": 5.645416281695924e-05,
      "loss": 0.1758,
      "step": 34970
    },
    {
      "epoch": 2.154604250076994,
      "grad_norm": 0.12930436432361603,
      "learning_rate": 5.641309927112206e-05,
      "loss": 0.1765,
      "step": 34980
    },
    {
      "epoch": 2.1552202032645518,
      "grad_norm": 0.13804161548614502,
      "learning_rate": 5.637203572528488e-05,
      "loss": 0.1748,
      "step": 34990
    },
    {
      "epoch": 2.15583615645211,
      "grad_norm": 0.14487434923648834,
      "learning_rate": 5.6330972179447694e-05,
      "loss": 0.1752,
      "step": 35000
    },
    {
      "epoch": 2.1564521096396674,
      "grad_norm": 0.13991758227348328,
      "learning_rate": 5.6289908633610514e-05,
      "loss": 0.1763,
      "step": 35010
    },
    {
      "epoch": 2.157068062827225,
      "grad_norm": 0.17648892104625702,
      "learning_rate": 5.624884508777333e-05,
      "loss": 0.1772,
      "step": 35020
    },
    {
      "epoch": 2.157684016014783,
      "grad_norm": 0.18674397468566895,
      "learning_rate": 5.620778154193615e-05,
      "loss": 0.1762,
      "step": 35030
    },
    {
      "epoch": 2.1582999692023406,
      "grad_norm": 0.15266761183738708,
      "learning_rate": 5.616671799609896e-05,
      "loss": 0.1768,
      "step": 35040
    },
    {
      "epoch": 2.158915922389898,
      "grad_norm": 0.1376529037952423,
      "learning_rate": 5.612565445026178e-05,
      "loss": 0.1764,
      "step": 35050
    },
    {
      "epoch": 2.159531875577456,
      "grad_norm": 0.1628776341676712,
      "learning_rate": 5.6084590904424594e-05,
      "loss": 0.1767,
      "step": 35060
    },
    {
      "epoch": 2.1601478287650138,
      "grad_norm": 0.1569824069738388,
      "learning_rate": 5.604352735858742e-05,
      "loss": 0.1777,
      "step": 35070
    },
    {
      "epoch": 2.160763781952572,
      "grad_norm": 0.1731528639793396,
      "learning_rate": 5.600246381275024e-05,
      "loss": 0.1763,
      "step": 35080
    },
    {
      "epoch": 2.1613797351401294,
      "grad_norm": 0.14547891914844513,
      "learning_rate": 5.5961400266913054e-05,
      "loss": 0.1762,
      "step": 35090
    },
    {
      "epoch": 2.161995688327687,
      "grad_norm": 0.14334766566753387,
      "learning_rate": 5.5920336721075874e-05,
      "loss": 0.176,
      "step": 35100
    },
    {
      "epoch": 2.162611641515245,
      "grad_norm": 0.1467721164226532,
      "learning_rate": 5.587927317523869e-05,
      "loss": 0.1778,
      "step": 35110
    },
    {
      "epoch": 2.1632275947028026,
      "grad_norm": 0.16377471387386322,
      "learning_rate": 5.583820962940151e-05,
      "loss": 0.1764,
      "step": 35120
    },
    {
      "epoch": 2.16384354789036,
      "grad_norm": 0.15046896040439606,
      "learning_rate": 5.579714608356432e-05,
      "loss": 0.178,
      "step": 35130
    },
    {
      "epoch": 2.164459501077918,
      "grad_norm": 0.16456489264965057,
      "learning_rate": 5.575608253772714e-05,
      "loss": 0.177,
      "step": 35140
    },
    {
      "epoch": 2.1650754542654758,
      "grad_norm": 0.1496908962726593,
      "learning_rate": 5.5715018991889954e-05,
      "loss": 0.1768,
      "step": 35150
    },
    {
      "epoch": 2.165691407453034,
      "grad_norm": 0.17954011261463165,
      "learning_rate": 5.5673955446052774e-05,
      "loss": 0.177,
      "step": 35160
    },
    {
      "epoch": 2.1663073606405914,
      "grad_norm": 0.1345408409833908,
      "learning_rate": 5.563289190021559e-05,
      "loss": 0.1754,
      "step": 35170
    },
    {
      "epoch": 2.166923313828149,
      "grad_norm": 0.15735246241092682,
      "learning_rate": 5.559182835437841e-05,
      "loss": 0.177,
      "step": 35180
    },
    {
      "epoch": 2.167539267015707,
      "grad_norm": 0.17666541039943695,
      "learning_rate": 5.555076480854122e-05,
      "loss": 0.1759,
      "step": 35190
    },
    {
      "epoch": 2.1681552202032646,
      "grad_norm": 0.15216033160686493,
      "learning_rate": 5.550970126270404e-05,
      "loss": 0.1759,
      "step": 35200
    },
    {
      "epoch": 2.168771173390822,
      "grad_norm": 0.1331188678741455,
      "learning_rate": 5.5468637716866854e-05,
      "loss": 0.1753,
      "step": 35210
    },
    {
      "epoch": 2.16938712657838,
      "grad_norm": 0.16449807584285736,
      "learning_rate": 5.5427574171029674e-05,
      "loss": 0.1767,
      "step": 35220
    },
    {
      "epoch": 2.1700030797659378,
      "grad_norm": 0.15702638030052185,
      "learning_rate": 5.538651062519249e-05,
      "loss": 0.176,
      "step": 35230
    },
    {
      "epoch": 2.1706190329534953,
      "grad_norm": 0.1524866819381714,
      "learning_rate": 5.534544707935531e-05,
      "loss": 0.1774,
      "step": 35240
    },
    {
      "epoch": 2.1712349861410534,
      "grad_norm": 0.12049951404333115,
      "learning_rate": 5.530438353351812e-05,
      "loss": 0.1758,
      "step": 35250
    },
    {
      "epoch": 2.171850939328611,
      "grad_norm": 0.16030564904212952,
      "learning_rate": 5.526331998768094e-05,
      "loss": 0.1771,
      "step": 35260
    },
    {
      "epoch": 2.172466892516169,
      "grad_norm": 0.13870015740394592,
      "learning_rate": 5.5222256441843754e-05,
      "loss": 0.1759,
      "step": 35270
    },
    {
      "epoch": 2.1730828457037266,
      "grad_norm": 0.14740648865699768,
      "learning_rate": 5.5181192896006574e-05,
      "loss": 0.176,
      "step": 35280
    },
    {
      "epoch": 2.173698798891284,
      "grad_norm": 0.15971216559410095,
      "learning_rate": 5.514012935016939e-05,
      "loss": 0.1777,
      "step": 35290
    },
    {
      "epoch": 2.174314752078842,
      "grad_norm": 0.1808551847934723,
      "learning_rate": 5.509906580433221e-05,
      "loss": 0.1766,
      "step": 35300
    },
    {
      "epoch": 2.1749307052663998,
      "grad_norm": 0.1592179834842682,
      "learning_rate": 5.505800225849502e-05,
      "loss": 0.1779,
      "step": 35310
    },
    {
      "epoch": 2.1755466584539573,
      "grad_norm": 0.15604649484157562,
      "learning_rate": 5.501693871265784e-05,
      "loss": 0.1756,
      "step": 35320
    },
    {
      "epoch": 2.1761626116415154,
      "grad_norm": 0.14804303646087646,
      "learning_rate": 5.4975875166820654e-05,
      "loss": 0.1757,
      "step": 35330
    },
    {
      "epoch": 2.176778564829073,
      "grad_norm": 0.16516143083572388,
      "learning_rate": 5.4934811620983474e-05,
      "loss": 0.1773,
      "step": 35340
    },
    {
      "epoch": 2.1773945180166305,
      "grad_norm": 0.1466822773218155,
      "learning_rate": 5.489374807514629e-05,
      "loss": 0.1769,
      "step": 35350
    },
    {
      "epoch": 2.1780104712041886,
      "grad_norm": 0.14868375658988953,
      "learning_rate": 5.485268452930911e-05,
      "loss": 0.1754,
      "step": 35360
    },
    {
      "epoch": 2.178626424391746,
      "grad_norm": 0.13479240238666534,
      "learning_rate": 5.481162098347192e-05,
      "loss": 0.1748,
      "step": 35370
    },
    {
      "epoch": 2.179242377579304,
      "grad_norm": 0.14174166321754456,
      "learning_rate": 5.477055743763474e-05,
      "loss": 0.1771,
      "step": 35380
    },
    {
      "epoch": 2.1798583307668618,
      "grad_norm": 0.14783111214637756,
      "learning_rate": 5.4729493891797554e-05,
      "loss": 0.1766,
      "step": 35390
    },
    {
      "epoch": 2.1804742839544193,
      "grad_norm": 0.17026759684085846,
      "learning_rate": 5.4688430345960374e-05,
      "loss": 0.1757,
      "step": 35400
    },
    {
      "epoch": 2.1810902371419774,
      "grad_norm": 0.16508130729198456,
      "learning_rate": 5.464736680012319e-05,
      "loss": 0.1765,
      "step": 35410
    },
    {
      "epoch": 2.181706190329535,
      "grad_norm": 0.16186632215976715,
      "learning_rate": 5.460630325428601e-05,
      "loss": 0.1757,
      "step": 35420
    },
    {
      "epoch": 2.1823221435170925,
      "grad_norm": 0.16407816112041473,
      "learning_rate": 5.456523970844882e-05,
      "loss": 0.1745,
      "step": 35430
    },
    {
      "epoch": 2.1829380967046506,
      "grad_norm": 0.17175771296024323,
      "learning_rate": 5.452417616261164e-05,
      "loss": 0.1783,
      "step": 35440
    },
    {
      "epoch": 2.183554049892208,
      "grad_norm": 0.1392885446548462,
      "learning_rate": 5.4483112616774453e-05,
      "loss": 0.1755,
      "step": 35450
    },
    {
      "epoch": 2.1841700030797657,
      "grad_norm": 0.13599388301372528,
      "learning_rate": 5.4442049070937273e-05,
      "loss": 0.1764,
      "step": 35460
    },
    {
      "epoch": 2.1847859562673237,
      "grad_norm": 0.15641221404075623,
      "learning_rate": 5.440098552510009e-05,
      "loss": 0.1758,
      "step": 35470
    },
    {
      "epoch": 2.1854019094548813,
      "grad_norm": 0.15429389476776123,
      "learning_rate": 5.435992197926291e-05,
      "loss": 0.1765,
      "step": 35480
    },
    {
      "epoch": 2.1860178626424394,
      "grad_norm": 0.15530918538570404,
      "learning_rate": 5.4318858433425734e-05,
      "loss": 0.1762,
      "step": 35490
    },
    {
      "epoch": 2.186633815829997,
      "grad_norm": 0.1848413646221161,
      "learning_rate": 5.4277794887588554e-05,
      "loss": 0.1752,
      "step": 35500
    },
    {
      "epoch": 2.1872497690175545,
      "grad_norm": 0.13724663853645325,
      "learning_rate": 5.423673134175137e-05,
      "loss": 0.1748,
      "step": 35510
    },
    {
      "epoch": 2.1878657222051126,
      "grad_norm": 0.15656380355358124,
      "learning_rate": 5.419566779591419e-05,
      "loss": 0.1765,
      "step": 35520
    },
    {
      "epoch": 2.18848167539267,
      "grad_norm": 0.12787118554115295,
      "learning_rate": 5.4154604250077e-05,
      "loss": 0.1766,
      "step": 35530
    },
    {
      "epoch": 2.1890976285802277,
      "grad_norm": 0.12775273621082306,
      "learning_rate": 5.411354070423982e-05,
      "loss": 0.1752,
      "step": 35540
    },
    {
      "epoch": 2.1897135817677857,
      "grad_norm": 0.20161758363246918,
      "learning_rate": 5.4072477158402633e-05,
      "loss": 0.1773,
      "step": 35550
    },
    {
      "epoch": 2.1903295349553433,
      "grad_norm": 0.23073910176753998,
      "learning_rate": 5.4031413612565454e-05,
      "loss": 0.177,
      "step": 35560
    },
    {
      "epoch": 2.1909454881429014,
      "grad_norm": 0.144985169172287,
      "learning_rate": 5.399035006672827e-05,
      "loss": 0.1781,
      "step": 35570
    },
    {
      "epoch": 2.191561441330459,
      "grad_norm": 0.1314794421195984,
      "learning_rate": 5.394928652089109e-05,
      "loss": 0.1764,
      "step": 35580
    },
    {
      "epoch": 2.1921773945180165,
      "grad_norm": 0.14026731252670288,
      "learning_rate": 5.39082229750539e-05,
      "loss": 0.1763,
      "step": 35590
    },
    {
      "epoch": 2.1927933477055745,
      "grad_norm": 0.14535808563232422,
      "learning_rate": 5.386715942921672e-05,
      "loss": 0.1774,
      "step": 35600
    },
    {
      "epoch": 2.193409300893132,
      "grad_norm": 0.15596671402454376,
      "learning_rate": 5.382609588337953e-05,
      "loss": 0.1772,
      "step": 35610
    },
    {
      "epoch": 2.1940252540806897,
      "grad_norm": 0.1573379635810852,
      "learning_rate": 5.3785032337542353e-05,
      "loss": 0.1763,
      "step": 35620
    },
    {
      "epoch": 2.1946412072682477,
      "grad_norm": 0.14711017906665802,
      "learning_rate": 5.374396879170517e-05,
      "loss": 0.1771,
      "step": 35630
    },
    {
      "epoch": 2.1952571604558053,
      "grad_norm": 0.14243467152118683,
      "learning_rate": 5.370290524586799e-05,
      "loss": 0.1748,
      "step": 35640
    },
    {
      "epoch": 2.195873113643363,
      "grad_norm": 0.16063956916332245,
      "learning_rate": 5.36618417000308e-05,
      "loss": 0.1742,
      "step": 35650
    },
    {
      "epoch": 2.196489066830921,
      "grad_norm": 0.15539881587028503,
      "learning_rate": 5.362077815419362e-05,
      "loss": 0.1761,
      "step": 35660
    },
    {
      "epoch": 2.1971050200184785,
      "grad_norm": 0.17022357881069183,
      "learning_rate": 5.357971460835643e-05,
      "loss": 0.1778,
      "step": 35670
    },
    {
      "epoch": 2.1977209732060365,
      "grad_norm": 0.1956547051668167,
      "learning_rate": 5.353865106251925e-05,
      "loss": 0.1769,
      "step": 35680
    },
    {
      "epoch": 2.198336926393594,
      "grad_norm": 0.15524806082248688,
      "learning_rate": 5.3497587516682067e-05,
      "loss": 0.1762,
      "step": 35690
    },
    {
      "epoch": 2.1989528795811517,
      "grad_norm": 0.14897535741329193,
      "learning_rate": 5.345652397084489e-05,
      "loss": 0.1759,
      "step": 35700
    },
    {
      "epoch": 2.1995688327687097,
      "grad_norm": 0.13970953226089478,
      "learning_rate": 5.34154604250077e-05,
      "loss": 0.175,
      "step": 35710
    },
    {
      "epoch": 2.2001847859562673,
      "grad_norm": 0.1716291755437851,
      "learning_rate": 5.337439687917052e-05,
      "loss": 0.1775,
      "step": 35720
    },
    {
      "epoch": 2.200800739143825,
      "grad_norm": 0.17419391870498657,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.1759,
      "step": 35730
    },
    {
      "epoch": 2.201416692331383,
      "grad_norm": 0.14713381230831146,
      "learning_rate": 5.329226978749615e-05,
      "loss": 0.176,
      "step": 35740
    },
    {
      "epoch": 2.2020326455189405,
      "grad_norm": 0.13962730765342712,
      "learning_rate": 5.3251206241658966e-05,
      "loss": 0.1773,
      "step": 35750
    },
    {
      "epoch": 2.2026485987064985,
      "grad_norm": 0.15407589077949524,
      "learning_rate": 5.3210142695821787e-05,
      "loss": 0.1764,
      "step": 35760
    },
    {
      "epoch": 2.203264551894056,
      "grad_norm": 0.1648305207490921,
      "learning_rate": 5.31690791499846e-05,
      "loss": 0.1761,
      "step": 35770
    },
    {
      "epoch": 2.2038805050816137,
      "grad_norm": 0.16431322693824768,
      "learning_rate": 5.312801560414742e-05,
      "loss": 0.1756,
      "step": 35780
    },
    {
      "epoch": 2.2044964582691717,
      "grad_norm": 0.1563662737607956,
      "learning_rate": 5.308695205831023e-05,
      "loss": 0.1777,
      "step": 35790
    },
    {
      "epoch": 2.2051124114567293,
      "grad_norm": 0.14401760697364807,
      "learning_rate": 5.304588851247305e-05,
      "loss": 0.1748,
      "step": 35800
    },
    {
      "epoch": 2.205728364644287,
      "grad_norm": 0.1874181479215622,
      "learning_rate": 5.3004824966635866e-05,
      "loss": 0.1763,
      "step": 35810
    },
    {
      "epoch": 2.206344317831845,
      "grad_norm": 0.13915330171585083,
      "learning_rate": 5.2963761420798686e-05,
      "loss": 0.1745,
      "step": 35820
    },
    {
      "epoch": 2.2069602710194025,
      "grad_norm": 0.15401318669319153,
      "learning_rate": 5.29226978749615e-05,
      "loss": 0.1769,
      "step": 35830
    },
    {
      "epoch": 2.20757622420696,
      "grad_norm": 0.1494794636964798,
      "learning_rate": 5.288163432912432e-05,
      "loss": 0.176,
      "step": 35840
    },
    {
      "epoch": 2.208192177394518,
      "grad_norm": 0.15480974316596985,
      "learning_rate": 5.284057078328713e-05,
      "loss": 0.1756,
      "step": 35850
    },
    {
      "epoch": 2.2088081305820757,
      "grad_norm": 0.15774616599082947,
      "learning_rate": 5.279950723744995e-05,
      "loss": 0.1742,
      "step": 35860
    },
    {
      "epoch": 2.2094240837696333,
      "grad_norm": 0.13824434578418732,
      "learning_rate": 5.2758443691612766e-05,
      "loss": 0.1764,
      "step": 35870
    },
    {
      "epoch": 2.2100400369571913,
      "grad_norm": 0.14675840735435486,
      "learning_rate": 5.2717380145775586e-05,
      "loss": 0.1744,
      "step": 35880
    },
    {
      "epoch": 2.210655990144749,
      "grad_norm": 0.1494799703359604,
      "learning_rate": 5.26763165999384e-05,
      "loss": 0.1777,
      "step": 35890
    },
    {
      "epoch": 2.211271943332307,
      "grad_norm": 0.14609095454216003,
      "learning_rate": 5.263525305410122e-05,
      "loss": 0.1776,
      "step": 35900
    },
    {
      "epoch": 2.2118878965198645,
      "grad_norm": 0.15495222806930542,
      "learning_rate": 5.259418950826403e-05,
      "loss": 0.1766,
      "step": 35910
    },
    {
      "epoch": 2.212503849707422,
      "grad_norm": 0.13517716526985168,
      "learning_rate": 5.2553125962426866e-05,
      "loss": 0.1751,
      "step": 35920
    },
    {
      "epoch": 2.21311980289498,
      "grad_norm": 0.13840346038341522,
      "learning_rate": 5.251206241658968e-05,
      "loss": 0.1746,
      "step": 35930
    },
    {
      "epoch": 2.2137357560825377,
      "grad_norm": 0.1655094176530838,
      "learning_rate": 5.24709988707525e-05,
      "loss": 0.1772,
      "step": 35940
    },
    {
      "epoch": 2.2143517092700953,
      "grad_norm": 0.14308004081249237,
      "learning_rate": 5.242993532491531e-05,
      "loss": 0.1766,
      "step": 35950
    },
    {
      "epoch": 2.2149676624576533,
      "grad_norm": 0.1554858535528183,
      "learning_rate": 5.238887177907813e-05,
      "loss": 0.175,
      "step": 35960
    },
    {
      "epoch": 2.215583615645211,
      "grad_norm": 0.1640559732913971,
      "learning_rate": 5.2347808233240946e-05,
      "loss": 0.1772,
      "step": 35970
    },
    {
      "epoch": 2.216199568832769,
      "grad_norm": 0.1427091658115387,
      "learning_rate": 5.2306744687403766e-05,
      "loss": 0.175,
      "step": 35980
    },
    {
      "epoch": 2.2168155220203265,
      "grad_norm": 0.1435343623161316,
      "learning_rate": 5.226568114156658e-05,
      "loss": 0.1783,
      "step": 35990
    },
    {
      "epoch": 2.217431475207884,
      "grad_norm": 0.14683672785758972,
      "learning_rate": 5.22246175957294e-05,
      "loss": 0.177,
      "step": 36000
    },
    {
      "epoch": 2.218047428395442,
      "grad_norm": 0.16169516742229462,
      "learning_rate": 5.218355404989221e-05,
      "loss": 0.1774,
      "step": 36010
    },
    {
      "epoch": 2.2186633815829997,
      "grad_norm": 0.13745063543319702,
      "learning_rate": 5.214249050405503e-05,
      "loss": 0.1769,
      "step": 36020
    },
    {
      "epoch": 2.2192793347705573,
      "grad_norm": 0.15760502219200134,
      "learning_rate": 5.2101426958217846e-05,
      "loss": 0.1767,
      "step": 36030
    },
    {
      "epoch": 2.2198952879581153,
      "grad_norm": 0.1290411800146103,
      "learning_rate": 5.2060363412380666e-05,
      "loss": 0.1765,
      "step": 36040
    },
    {
      "epoch": 2.220511241145673,
      "grad_norm": 0.14594638347625732,
      "learning_rate": 5.201929986654348e-05,
      "loss": 0.1769,
      "step": 36050
    },
    {
      "epoch": 2.2211271943332305,
      "grad_norm": 0.1376592069864273,
      "learning_rate": 5.19782363207063e-05,
      "loss": 0.1755,
      "step": 36060
    },
    {
      "epoch": 2.2217431475207885,
      "grad_norm": 0.16321997344493866,
      "learning_rate": 5.193717277486911e-05,
      "loss": 0.178,
      "step": 36070
    },
    {
      "epoch": 2.222359100708346,
      "grad_norm": 0.16803815960884094,
      "learning_rate": 5.189610922903193e-05,
      "loss": 0.176,
      "step": 36080
    },
    {
      "epoch": 2.222975053895904,
      "grad_norm": 0.15696945786476135,
      "learning_rate": 5.1855045683194746e-05,
      "loss": 0.175,
      "step": 36090
    },
    {
      "epoch": 2.2235910070834617,
      "grad_norm": 0.15705150365829468,
      "learning_rate": 5.1813982137357566e-05,
      "loss": 0.1769,
      "step": 36100
    },
    {
      "epoch": 2.2242069602710193,
      "grad_norm": 0.12987202405929565,
      "learning_rate": 5.177291859152038e-05,
      "loss": 0.1773,
      "step": 36110
    },
    {
      "epoch": 2.2248229134585773,
      "grad_norm": 0.17148971557617188,
      "learning_rate": 5.17318550456832e-05,
      "loss": 0.1752,
      "step": 36120
    },
    {
      "epoch": 2.225438866646135,
      "grad_norm": 0.15477775037288666,
      "learning_rate": 5.169079149984601e-05,
      "loss": 0.1762,
      "step": 36130
    },
    {
      "epoch": 2.2260548198336925,
      "grad_norm": 0.12931525707244873,
      "learning_rate": 5.164972795400883e-05,
      "loss": 0.176,
      "step": 36140
    },
    {
      "epoch": 2.2266707730212505,
      "grad_norm": 0.15062302350997925,
      "learning_rate": 5.1608664408171646e-05,
      "loss": 0.1775,
      "step": 36150
    },
    {
      "epoch": 2.227286726208808,
      "grad_norm": 0.1512831598520279,
      "learning_rate": 5.1567600862334466e-05,
      "loss": 0.1755,
      "step": 36160
    },
    {
      "epoch": 2.227902679396366,
      "grad_norm": 0.15546323359012604,
      "learning_rate": 5.152653731649728e-05,
      "loss": 0.1767,
      "step": 36170
    },
    {
      "epoch": 2.2285186325839237,
      "grad_norm": 0.18167510628700256,
      "learning_rate": 5.14854737706601e-05,
      "loss": 0.1742,
      "step": 36180
    },
    {
      "epoch": 2.2291345857714813,
      "grad_norm": 0.14889580011367798,
      "learning_rate": 5.144441022482291e-05,
      "loss": 0.1757,
      "step": 36190
    },
    {
      "epoch": 2.2297505389590393,
      "grad_norm": 0.14629870653152466,
      "learning_rate": 5.140334667898573e-05,
      "loss": 0.1775,
      "step": 36200
    },
    {
      "epoch": 2.230366492146597,
      "grad_norm": 0.16996973752975464,
      "learning_rate": 5.1362283133148546e-05,
      "loss": 0.1774,
      "step": 36210
    },
    {
      "epoch": 2.2309824453341545,
      "grad_norm": 0.17353011667728424,
      "learning_rate": 5.1321219587311366e-05,
      "loss": 0.174,
      "step": 36220
    },
    {
      "epoch": 2.2315983985217125,
      "grad_norm": 0.16716785728931427,
      "learning_rate": 5.128015604147418e-05,
      "loss": 0.177,
      "step": 36230
    },
    {
      "epoch": 2.23221435170927,
      "grad_norm": 0.1516430526971817,
      "learning_rate": 5.1239092495637e-05,
      "loss": 0.1757,
      "step": 36240
    },
    {
      "epoch": 2.2328303048968277,
      "grad_norm": 0.1349625587463379,
      "learning_rate": 5.119802894979981e-05,
      "loss": 0.1751,
      "step": 36250
    },
    {
      "epoch": 2.2334462580843857,
      "grad_norm": 0.13667576014995575,
      "learning_rate": 5.115696540396263e-05,
      "loss": 0.1749,
      "step": 36260
    },
    {
      "epoch": 2.2340622112719433,
      "grad_norm": 0.14470335841178894,
      "learning_rate": 5.1115901858125446e-05,
      "loss": 0.1752,
      "step": 36270
    },
    {
      "epoch": 2.234678164459501,
      "grad_norm": 0.17130114138126373,
      "learning_rate": 5.1074838312288266e-05,
      "loss": 0.1782,
      "step": 36280
    },
    {
      "epoch": 2.235294117647059,
      "grad_norm": 0.16096077859401703,
      "learning_rate": 5.103377476645108e-05,
      "loss": 0.1762,
      "step": 36290
    },
    {
      "epoch": 2.2359100708346165,
      "grad_norm": 0.14572159945964813,
      "learning_rate": 5.09927112206139e-05,
      "loss": 0.1751,
      "step": 36300
    },
    {
      "epoch": 2.2365260240221745,
      "grad_norm": 0.15906713902950287,
      "learning_rate": 5.095164767477671e-05,
      "loss": 0.1751,
      "step": 36310
    },
    {
      "epoch": 2.237141977209732,
      "grad_norm": 0.1612459123134613,
      "learning_rate": 5.091058412893953e-05,
      "loss": 0.177,
      "step": 36320
    },
    {
      "epoch": 2.2377579303972897,
      "grad_norm": 0.1425350308418274,
      "learning_rate": 5.0869520583102346e-05,
      "loss": 0.1766,
      "step": 36330
    },
    {
      "epoch": 2.2383738835848477,
      "grad_norm": 0.15072405338287354,
      "learning_rate": 5.082845703726517e-05,
      "loss": 0.1765,
      "step": 36340
    },
    {
      "epoch": 2.2389898367724053,
      "grad_norm": 0.13477590680122375,
      "learning_rate": 5.078739349142799e-05,
      "loss": 0.1758,
      "step": 36350
    },
    {
      "epoch": 2.239605789959963,
      "grad_norm": 0.14050090312957764,
      "learning_rate": 5.074632994559081e-05,
      "loss": 0.1749,
      "step": 36360
    },
    {
      "epoch": 2.240221743147521,
      "grad_norm": 0.1519635021686554,
      "learning_rate": 5.0705266399753626e-05,
      "loss": 0.1759,
      "step": 36370
    },
    {
      "epoch": 2.2408376963350785,
      "grad_norm": 0.18101809918880463,
      "learning_rate": 5.0664202853916446e-05,
      "loss": 0.1743,
      "step": 36380
    },
    {
      "epoch": 2.2414536495226365,
      "grad_norm": 0.16334888339042664,
      "learning_rate": 5.062313930807926e-05,
      "loss": 0.1762,
      "step": 36390
    },
    {
      "epoch": 2.242069602710194,
      "grad_norm": 0.18176156282424927,
      "learning_rate": 5.058207576224208e-05,
      "loss": 0.1743,
      "step": 36400
    },
    {
      "epoch": 2.2426855558977516,
      "grad_norm": 0.19045329093933105,
      "learning_rate": 5.054101221640489e-05,
      "loss": 0.1752,
      "step": 36410
    },
    {
      "epoch": 2.2433015090853097,
      "grad_norm": 0.16221269965171814,
      "learning_rate": 5.049994867056771e-05,
      "loss": 0.1764,
      "step": 36420
    },
    {
      "epoch": 2.2439174622728673,
      "grad_norm": 0.17014190554618835,
      "learning_rate": 5.0458885124730526e-05,
      "loss": 0.1771,
      "step": 36430
    },
    {
      "epoch": 2.244533415460425,
      "grad_norm": 0.14755026996135712,
      "learning_rate": 5.0417821578893346e-05,
      "loss": 0.1791,
      "step": 36440
    },
    {
      "epoch": 2.245149368647983,
      "grad_norm": 0.17967358231544495,
      "learning_rate": 5.037675803305616e-05,
      "loss": 0.176,
      "step": 36450
    },
    {
      "epoch": 2.2457653218355405,
      "grad_norm": 0.15493519604206085,
      "learning_rate": 5.033569448721898e-05,
      "loss": 0.1764,
      "step": 36460
    },
    {
      "epoch": 2.246381275023098,
      "grad_norm": 0.1366247534751892,
      "learning_rate": 5.029463094138179e-05,
      "loss": 0.1772,
      "step": 36470
    },
    {
      "epoch": 2.246997228210656,
      "grad_norm": 0.1732586771249771,
      "learning_rate": 5.025356739554461e-05,
      "loss": 0.1745,
      "step": 36480
    },
    {
      "epoch": 2.2476131813982136,
      "grad_norm": 0.15640221536159515,
      "learning_rate": 5.0212503849707426e-05,
      "loss": 0.1755,
      "step": 36490
    },
    {
      "epoch": 2.2482291345857717,
      "grad_norm": 0.21082814037799835,
      "learning_rate": 5.0171440303870246e-05,
      "loss": 0.1754,
      "step": 36500
    },
    {
      "epoch": 2.2488450877733293,
      "grad_norm": 0.15746726095676422,
      "learning_rate": 5.013037675803306e-05,
      "loss": 0.1754,
      "step": 36510
    },
    {
      "epoch": 2.249461040960887,
      "grad_norm": 0.16684098541736603,
      "learning_rate": 5.008931321219588e-05,
      "loss": 0.177,
      "step": 36520
    },
    {
      "epoch": 2.250076994148445,
      "grad_norm": 0.144817516207695,
      "learning_rate": 5.004824966635869e-05,
      "loss": 0.1763,
      "step": 36530
    },
    {
      "epoch": 2.2506929473360024,
      "grad_norm": 0.15031617879867554,
      "learning_rate": 5.000718612052151e-05,
      "loss": 0.1768,
      "step": 36540
    },
    {
      "epoch": 2.25130890052356,
      "grad_norm": 0.1748426854610443,
      "learning_rate": 4.9966122574684326e-05,
      "loss": 0.1766,
      "step": 36550
    },
    {
      "epoch": 2.251924853711118,
      "grad_norm": 0.13656498491764069,
      "learning_rate": 4.9925059028847146e-05,
      "loss": 0.1757,
      "step": 36560
    },
    {
      "epoch": 2.2525408068986756,
      "grad_norm": 0.14556081593036652,
      "learning_rate": 4.988399548300996e-05,
      "loss": 0.1798,
      "step": 36570
    },
    {
      "epoch": 2.2531567600862337,
      "grad_norm": 0.1472507268190384,
      "learning_rate": 4.984293193717278e-05,
      "loss": 0.1761,
      "step": 36580
    },
    {
      "epoch": 2.2537727132737913,
      "grad_norm": 0.1450212150812149,
      "learning_rate": 4.980186839133559e-05,
      "loss": 0.1758,
      "step": 36590
    },
    {
      "epoch": 2.254388666461349,
      "grad_norm": 0.1632014811038971,
      "learning_rate": 4.976080484549841e-05,
      "loss": 0.1743,
      "step": 36600
    },
    {
      "epoch": 2.255004619648907,
      "grad_norm": 0.17585095763206482,
      "learning_rate": 4.9719741299661225e-05,
      "loss": 0.1758,
      "step": 36610
    },
    {
      "epoch": 2.2556205728364644,
      "grad_norm": 0.18018986284732819,
      "learning_rate": 4.9678677753824045e-05,
      "loss": 0.1764,
      "step": 36620
    },
    {
      "epoch": 2.256236526024022,
      "grad_norm": 0.152835413813591,
      "learning_rate": 4.963761420798686e-05,
      "loss": 0.1756,
      "step": 36630
    },
    {
      "epoch": 2.25685247921158,
      "grad_norm": 0.1679217517375946,
      "learning_rate": 4.959655066214968e-05,
      "loss": 0.1784,
      "step": 36640
    },
    {
      "epoch": 2.2574684323991376,
      "grad_norm": 0.1571609228849411,
      "learning_rate": 4.955548711631249e-05,
      "loss": 0.177,
      "step": 36650
    },
    {
      "epoch": 2.258084385586695,
      "grad_norm": 0.1579408496618271,
      "learning_rate": 4.951442357047532e-05,
      "loss": 0.1757,
      "step": 36660
    },
    {
      "epoch": 2.2587003387742532,
      "grad_norm": 0.12523211538791656,
      "learning_rate": 4.947336002463813e-05,
      "loss": 0.1752,
      "step": 36670
    },
    {
      "epoch": 2.259316291961811,
      "grad_norm": 0.15566137433052063,
      "learning_rate": 4.943229647880095e-05,
      "loss": 0.1761,
      "step": 36680
    },
    {
      "epoch": 2.2599322451493684,
      "grad_norm": 0.15547846257686615,
      "learning_rate": 4.9391232932963765e-05,
      "loss": 0.1764,
      "step": 36690
    },
    {
      "epoch": 2.2605481983369264,
      "grad_norm": 0.1488555520772934,
      "learning_rate": 4.9350169387126585e-05,
      "loss": 0.1744,
      "step": 36700
    },
    {
      "epoch": 2.261164151524484,
      "grad_norm": 0.16462968289852142,
      "learning_rate": 4.93091058412894e-05,
      "loss": 0.177,
      "step": 36710
    },
    {
      "epoch": 2.261780104712042,
      "grad_norm": 0.14874976873397827,
      "learning_rate": 4.926804229545222e-05,
      "loss": 0.1754,
      "step": 36720
    },
    {
      "epoch": 2.2623960578995996,
      "grad_norm": 0.16280880570411682,
      "learning_rate": 4.922697874961503e-05,
      "loss": 0.1751,
      "step": 36730
    },
    {
      "epoch": 2.263012011087157,
      "grad_norm": 0.1628107875585556,
      "learning_rate": 4.918591520377785e-05,
      "loss": 0.1757,
      "step": 36740
    },
    {
      "epoch": 2.2636279642747152,
      "grad_norm": 0.16045179963111877,
      "learning_rate": 4.9144851657940665e-05,
      "loss": 0.1768,
      "step": 36750
    },
    {
      "epoch": 2.264243917462273,
      "grad_norm": 0.13568846881389618,
      "learning_rate": 4.9103788112103485e-05,
      "loss": 0.1764,
      "step": 36760
    },
    {
      "epoch": 2.264859870649831,
      "grad_norm": 0.1492331176996231,
      "learning_rate": 4.90627245662663e-05,
      "loss": 0.1764,
      "step": 36770
    },
    {
      "epoch": 2.2654758238373884,
      "grad_norm": 0.14991968870162964,
      "learning_rate": 4.902166102042912e-05,
      "loss": 0.1765,
      "step": 36780
    },
    {
      "epoch": 2.266091777024946,
      "grad_norm": 0.144642174243927,
      "learning_rate": 4.898059747459193e-05,
      "loss": 0.177,
      "step": 36790
    },
    {
      "epoch": 2.266707730212504,
      "grad_norm": 0.14634571969509125,
      "learning_rate": 4.893953392875475e-05,
      "loss": 0.1752,
      "step": 36800
    },
    {
      "epoch": 2.2673236834000616,
      "grad_norm": 0.17853227257728577,
      "learning_rate": 4.8898470382917565e-05,
      "loss": 0.1769,
      "step": 36810
    },
    {
      "epoch": 2.267939636587619,
      "grad_norm": 0.15315501391887665,
      "learning_rate": 4.8857406837080385e-05,
      "loss": 0.1764,
      "step": 36820
    },
    {
      "epoch": 2.2685555897751772,
      "grad_norm": 0.15052196383476257,
      "learning_rate": 4.88163432912432e-05,
      "loss": 0.1773,
      "step": 36830
    },
    {
      "epoch": 2.269171542962735,
      "grad_norm": 0.1392212212085724,
      "learning_rate": 4.877527974540602e-05,
      "loss": 0.1779,
      "step": 36840
    },
    {
      "epoch": 2.2697874961502924,
      "grad_norm": 0.15345387160778046,
      "learning_rate": 4.873421619956883e-05,
      "loss": 0.1751,
      "step": 36850
    },
    {
      "epoch": 2.2704034493378504,
      "grad_norm": 0.14369535446166992,
      "learning_rate": 4.869315265373165e-05,
      "loss": 0.1767,
      "step": 36860
    },
    {
      "epoch": 2.271019402525408,
      "grad_norm": 0.16204212605953217,
      "learning_rate": 4.8652089107894465e-05,
      "loss": 0.1766,
      "step": 36870
    },
    {
      "epoch": 2.2716353557129656,
      "grad_norm": 0.15160341560840607,
      "learning_rate": 4.861102556205729e-05,
      "loss": 0.1762,
      "step": 36880
    },
    {
      "epoch": 2.2722513089005236,
      "grad_norm": 0.12356609851121902,
      "learning_rate": 4.8569962016220105e-05,
      "loss": 0.1783,
      "step": 36890
    },
    {
      "epoch": 2.272867262088081,
      "grad_norm": 0.15284816920757294,
      "learning_rate": 4.8528898470382925e-05,
      "loss": 0.1756,
      "step": 36900
    },
    {
      "epoch": 2.2734832152756392,
      "grad_norm": 0.15416865050792694,
      "learning_rate": 4.848783492454574e-05,
      "loss": 0.1774,
      "step": 36910
    },
    {
      "epoch": 2.274099168463197,
      "grad_norm": 0.15935924649238586,
      "learning_rate": 4.844677137870856e-05,
      "loss": 0.175,
      "step": 36920
    },
    {
      "epoch": 2.2747151216507544,
      "grad_norm": 0.16883650422096252,
      "learning_rate": 4.840570783287137e-05,
      "loss": 0.1764,
      "step": 36930
    },
    {
      "epoch": 2.2753310748383124,
      "grad_norm": 0.14821867644786835,
      "learning_rate": 4.836464428703419e-05,
      "loss": 0.176,
      "step": 36940
    },
    {
      "epoch": 2.27594702802587,
      "grad_norm": 0.15102720260620117,
      "learning_rate": 4.8323580741197005e-05,
      "loss": 0.1769,
      "step": 36950
    },
    {
      "epoch": 2.2765629812134276,
      "grad_norm": 0.1682322770357132,
      "learning_rate": 4.8282517195359825e-05,
      "loss": 0.1756,
      "step": 36960
    },
    {
      "epoch": 2.2771789344009856,
      "grad_norm": 0.18598799407482147,
      "learning_rate": 4.824145364952264e-05,
      "loss": 0.1743,
      "step": 36970
    },
    {
      "epoch": 2.277794887588543,
      "grad_norm": 0.1553846299648285,
      "learning_rate": 4.820039010368546e-05,
      "loss": 0.1761,
      "step": 36980
    },
    {
      "epoch": 2.2784108407761012,
      "grad_norm": 0.17043989896774292,
      "learning_rate": 4.815932655784827e-05,
      "loss": 0.1752,
      "step": 36990
    },
    {
      "epoch": 2.279026793963659,
      "grad_norm": 0.1689026653766632,
      "learning_rate": 4.811826301201109e-05,
      "loss": 0.175,
      "step": 37000
    },
    {
      "epoch": 2.2796427471512164,
      "grad_norm": 0.1577652096748352,
      "learning_rate": 4.8077199466173905e-05,
      "loss": 0.1777,
      "step": 37010
    },
    {
      "epoch": 2.2802587003387744,
      "grad_norm": 0.1634288877248764,
      "learning_rate": 4.8036135920336725e-05,
      "loss": 0.178,
      "step": 37020
    },
    {
      "epoch": 2.280874653526332,
      "grad_norm": 0.14189770817756653,
      "learning_rate": 4.799507237449954e-05,
      "loss": 0.1759,
      "step": 37030
    },
    {
      "epoch": 2.2814906067138896,
      "grad_norm": 0.16575002670288086,
      "learning_rate": 4.795400882866236e-05,
      "loss": 0.1755,
      "step": 37040
    },
    {
      "epoch": 2.2821065599014476,
      "grad_norm": 0.14466798305511475,
      "learning_rate": 4.791294528282517e-05,
      "loss": 0.1743,
      "step": 37050
    },
    {
      "epoch": 2.282722513089005,
      "grad_norm": 0.1883470118045807,
      "learning_rate": 4.787188173698799e-05,
      "loss": 0.1759,
      "step": 37060
    },
    {
      "epoch": 2.283338466276563,
      "grad_norm": 0.14066970348358154,
      "learning_rate": 4.7830818191150805e-05,
      "loss": 0.1779,
      "step": 37070
    },
    {
      "epoch": 2.283954419464121,
      "grad_norm": 0.147752046585083,
      "learning_rate": 4.7789754645313625e-05,
      "loss": 0.1762,
      "step": 37080
    },
    {
      "epoch": 2.2845703726516784,
      "grad_norm": 0.1665664166212082,
      "learning_rate": 4.7748691099476445e-05,
      "loss": 0.1776,
      "step": 37090
    },
    {
      "epoch": 2.285186325839236,
      "grad_norm": 0.15081565082073212,
      "learning_rate": 4.770762755363926e-05,
      "loss": 0.1761,
      "step": 37100
    },
    {
      "epoch": 2.285802279026794,
      "grad_norm": 0.16994546353816986,
      "learning_rate": 4.766656400780208e-05,
      "loss": 0.1759,
      "step": 37110
    },
    {
      "epoch": 2.2864182322143516,
      "grad_norm": 0.16435010731220245,
      "learning_rate": 4.762550046196489e-05,
      "loss": 0.1773,
      "step": 37120
    },
    {
      "epoch": 2.2870341854019096,
      "grad_norm": 0.16392260789871216,
      "learning_rate": 4.758443691612771e-05,
      "loss": 0.1771,
      "step": 37130
    },
    {
      "epoch": 2.287650138589467,
      "grad_norm": 0.14957314729690552,
      "learning_rate": 4.7543373370290525e-05,
      "loss": 0.1756,
      "step": 37140
    },
    {
      "epoch": 2.288266091777025,
      "grad_norm": 0.15227267146110535,
      "learning_rate": 4.7502309824453345e-05,
      "loss": 0.1781,
      "step": 37150
    },
    {
      "epoch": 2.288882044964583,
      "grad_norm": 0.15671373903751373,
      "learning_rate": 4.7461246278616165e-05,
      "loss": 0.1752,
      "step": 37160
    },
    {
      "epoch": 2.2894979981521404,
      "grad_norm": 0.18029606342315674,
      "learning_rate": 4.742018273277898e-05,
      "loss": 0.1753,
      "step": 37170
    },
    {
      "epoch": 2.2901139513396984,
      "grad_norm": 0.16126245260238647,
      "learning_rate": 4.73791191869418e-05,
      "loss": 0.1779,
      "step": 37180
    },
    {
      "epoch": 2.290729904527256,
      "grad_norm": 0.1819537729024887,
      "learning_rate": 4.733805564110461e-05,
      "loss": 0.1739,
      "step": 37190
    },
    {
      "epoch": 2.2913458577148136,
      "grad_norm": 0.16910402476787567,
      "learning_rate": 4.729699209526743e-05,
      "loss": 0.1765,
      "step": 37200
    },
    {
      "epoch": 2.2919618109023716,
      "grad_norm": 0.18371184170246124,
      "learning_rate": 4.7255928549430245e-05,
      "loss": 0.177,
      "step": 37210
    },
    {
      "epoch": 2.292577764089929,
      "grad_norm": 0.1340274214744568,
      "learning_rate": 4.7214865003593065e-05,
      "loss": 0.1768,
      "step": 37220
    },
    {
      "epoch": 2.2931937172774868,
      "grad_norm": 0.16558098793029785,
      "learning_rate": 4.717380145775588e-05,
      "loss": 0.1769,
      "step": 37230
    },
    {
      "epoch": 2.293809670465045,
      "grad_norm": 0.15403532981872559,
      "learning_rate": 4.71327379119187e-05,
      "loss": 0.1754,
      "step": 37240
    },
    {
      "epoch": 2.2944256236526024,
      "grad_norm": 0.1412900686264038,
      "learning_rate": 4.709167436608151e-05,
      "loss": 0.178,
      "step": 37250
    },
    {
      "epoch": 2.29504157684016,
      "grad_norm": 0.1546398103237152,
      "learning_rate": 4.705061082024433e-05,
      "loss": 0.1759,
      "step": 37260
    },
    {
      "epoch": 2.295657530027718,
      "grad_norm": 0.15447406470775604,
      "learning_rate": 4.7009547274407145e-05,
      "loss": 0.1757,
      "step": 37270
    },
    {
      "epoch": 2.2962734832152756,
      "grad_norm": 0.15769566595554352,
      "learning_rate": 4.6968483728569965e-05,
      "loss": 0.176,
      "step": 37280
    },
    {
      "epoch": 2.296889436402833,
      "grad_norm": 0.15777355432510376,
      "learning_rate": 4.692742018273278e-05,
      "loss": 0.1755,
      "step": 37290
    },
    {
      "epoch": 2.297505389590391,
      "grad_norm": 0.15093480050563812,
      "learning_rate": 4.68863566368956e-05,
      "loss": 0.1745,
      "step": 37300
    },
    {
      "epoch": 2.2981213427779488,
      "grad_norm": 0.15390892326831818,
      "learning_rate": 4.684529309105842e-05,
      "loss": 0.1762,
      "step": 37310
    },
    {
      "epoch": 2.298737295965507,
      "grad_norm": 0.16844134032726288,
      "learning_rate": 4.680422954522123e-05,
      "loss": 0.1781,
      "step": 37320
    },
    {
      "epoch": 2.2993532491530644,
      "grad_norm": 0.14501561224460602,
      "learning_rate": 4.676316599938405e-05,
      "loss": 0.1761,
      "step": 37330
    },
    {
      "epoch": 2.299969202340622,
      "grad_norm": 0.14907437562942505,
      "learning_rate": 4.6722102453546865e-05,
      "loss": 0.1758,
      "step": 37340
    },
    {
      "epoch": 2.30058515552818,
      "grad_norm": 0.129893958568573,
      "learning_rate": 4.6681038907709685e-05,
      "loss": 0.1746,
      "step": 37350
    },
    {
      "epoch": 2.3012011087157376,
      "grad_norm": 0.12568847835063934,
      "learning_rate": 4.66399753618725e-05,
      "loss": 0.1737,
      "step": 37360
    },
    {
      "epoch": 2.301817061903295,
      "grad_norm": 0.1409233957529068,
      "learning_rate": 4.659891181603532e-05,
      "loss": 0.1741,
      "step": 37370
    },
    {
      "epoch": 2.302433015090853,
      "grad_norm": 0.1454286277294159,
      "learning_rate": 4.655784827019813e-05,
      "loss": 0.1775,
      "step": 37380
    },
    {
      "epoch": 2.3030489682784108,
      "grad_norm": 0.1309642791748047,
      "learning_rate": 4.651678472436095e-05,
      "loss": 0.1754,
      "step": 37390
    },
    {
      "epoch": 2.303664921465969,
      "grad_norm": 0.1482725590467453,
      "learning_rate": 4.6475721178523764e-05,
      "loss": 0.1765,
      "step": 37400
    },
    {
      "epoch": 2.3042808746535264,
      "grad_norm": 0.15697813034057617,
      "learning_rate": 4.6434657632686584e-05,
      "loss": 0.1796,
      "step": 37410
    },
    {
      "epoch": 2.304896827841084,
      "grad_norm": 0.14094942808151245,
      "learning_rate": 4.6393594086849405e-05,
      "loss": 0.1763,
      "step": 37420
    },
    {
      "epoch": 2.305512781028642,
      "grad_norm": 0.1388067603111267,
      "learning_rate": 4.635253054101222e-05,
      "loss": 0.1763,
      "step": 37430
    },
    {
      "epoch": 2.3061287342161996,
      "grad_norm": 0.15261182188987732,
      "learning_rate": 4.631146699517504e-05,
      "loss": 0.1772,
      "step": 37440
    },
    {
      "epoch": 2.306744687403757,
      "grad_norm": 0.1661868542432785,
      "learning_rate": 4.627040344933785e-05,
      "loss": 0.1752,
      "step": 37450
    },
    {
      "epoch": 2.307360640591315,
      "grad_norm": 0.15568655729293823,
      "learning_rate": 4.622933990350067e-05,
      "loss": 0.1773,
      "step": 37460
    },
    {
      "epoch": 2.3079765937788728,
      "grad_norm": 0.15434829890727997,
      "learning_rate": 4.6188276357663484e-05,
      "loss": 0.1763,
      "step": 37470
    },
    {
      "epoch": 2.3085925469664303,
      "grad_norm": 0.1524645835161209,
      "learning_rate": 4.6147212811826304e-05,
      "loss": 0.1764,
      "step": 37480
    },
    {
      "epoch": 2.3092085001539884,
      "grad_norm": 0.1610127091407776,
      "learning_rate": 4.610614926598912e-05,
      "loss": 0.1767,
      "step": 37490
    },
    {
      "epoch": 2.309824453341546,
      "grad_norm": 0.16252270340919495,
      "learning_rate": 4.606508572015194e-05,
      "loss": 0.1767,
      "step": 37500
    },
    {
      "epoch": 2.3104404065291035,
      "grad_norm": 0.14706812798976898,
      "learning_rate": 4.602402217431476e-05,
      "loss": 0.1769,
      "step": 37510
    },
    {
      "epoch": 2.3110563597166616,
      "grad_norm": 0.14017720520496368,
      "learning_rate": 4.598295862847757e-05,
      "loss": 0.1755,
      "step": 37520
    },
    {
      "epoch": 2.311672312904219,
      "grad_norm": 0.1617245376110077,
      "learning_rate": 4.594189508264039e-05,
      "loss": 0.1747,
      "step": 37530
    },
    {
      "epoch": 2.312288266091777,
      "grad_norm": 0.14229591190814972,
      "learning_rate": 4.5900831536803204e-05,
      "loss": 0.1734,
      "step": 37540
    },
    {
      "epoch": 2.3129042192793348,
      "grad_norm": 0.14300091564655304,
      "learning_rate": 4.5859767990966024e-05,
      "loss": 0.1763,
      "step": 37550
    },
    {
      "epoch": 2.3135201724668923,
      "grad_norm": 0.1653110533952713,
      "learning_rate": 4.581870444512884e-05,
      "loss": 0.1766,
      "step": 37560
    },
    {
      "epoch": 2.3141361256544504,
      "grad_norm": 0.1510099321603775,
      "learning_rate": 4.577764089929166e-05,
      "loss": 0.1777,
      "step": 37570
    },
    {
      "epoch": 2.314752078842008,
      "grad_norm": 0.1756991446018219,
      "learning_rate": 4.573657735345447e-05,
      "loss": 0.1757,
      "step": 37580
    },
    {
      "epoch": 2.315368032029566,
      "grad_norm": 0.15694722533226013,
      "learning_rate": 4.569551380761729e-05,
      "loss": 0.1755,
      "step": 37590
    },
    {
      "epoch": 2.3159839852171236,
      "grad_norm": 0.1505332589149475,
      "learning_rate": 4.5654450261780104e-05,
      "loss": 0.1756,
      "step": 37600
    },
    {
      "epoch": 2.316599938404681,
      "grad_norm": 0.2034226804971695,
      "learning_rate": 4.5613386715942924e-05,
      "loss": 0.1758,
      "step": 37610
    },
    {
      "epoch": 2.317215891592239,
      "grad_norm": 0.15730443596839905,
      "learning_rate": 4.5576429524689456e-05,
      "loss": 0.1774,
      "step": 37620
    },
    {
      "epoch": 2.3178318447797968,
      "grad_norm": 0.16447092592716217,
      "learning_rate": 4.5535365978852276e-05,
      "loss": 0.1764,
      "step": 37630
    },
    {
      "epoch": 2.3184477979673543,
      "grad_norm": 0.16744102537631989,
      "learning_rate": 4.5494302433015096e-05,
      "loss": 0.1754,
      "step": 37640
    },
    {
      "epoch": 2.3190637511549124,
      "grad_norm": 0.1798933446407318,
      "learning_rate": 4.5453238887177916e-05,
      "loss": 0.1748,
      "step": 37650
    },
    {
      "epoch": 2.31967970434247,
      "grad_norm": 0.1563781350851059,
      "learning_rate": 4.541217534134073e-05,
      "loss": 0.1773,
      "step": 37660
    },
    {
      "epoch": 2.3202956575300275,
      "grad_norm": 0.1457768976688385,
      "learning_rate": 4.537111179550355e-05,
      "loss": 0.1771,
      "step": 37670
    },
    {
      "epoch": 2.3209116107175856,
      "grad_norm": 0.15437474846839905,
      "learning_rate": 4.533004824966636e-05,
      "loss": 0.1762,
      "step": 37680
    },
    {
      "epoch": 2.321527563905143,
      "grad_norm": 0.15359942615032196,
      "learning_rate": 4.528898470382918e-05,
      "loss": 0.1771,
      "step": 37690
    },
    {
      "epoch": 2.3221435170927007,
      "grad_norm": 0.18723171949386597,
      "learning_rate": 4.5247921157991995e-05,
      "loss": 0.1795,
      "step": 37700
    },
    {
      "epoch": 2.3227594702802588,
      "grad_norm": 0.15266747772693634,
      "learning_rate": 4.5206857612154816e-05,
      "loss": 0.1752,
      "step": 37710
    },
    {
      "epoch": 2.3233754234678163,
      "grad_norm": 0.14959368109703064,
      "learning_rate": 4.516579406631763e-05,
      "loss": 0.177,
      "step": 37720
    },
    {
      "epoch": 2.3239913766553744,
      "grad_norm": 0.1483941674232483,
      "learning_rate": 4.512473052048045e-05,
      "loss": 0.1778,
      "step": 37730
    },
    {
      "epoch": 2.324607329842932,
      "grad_norm": 0.147257000207901,
      "learning_rate": 4.508366697464326e-05,
      "loss": 0.1761,
      "step": 37740
    },
    {
      "epoch": 2.3252232830304895,
      "grad_norm": 0.1680525690317154,
      "learning_rate": 4.504260342880608e-05,
      "loss": 0.1763,
      "step": 37750
    },
    {
      "epoch": 2.3258392362180476,
      "grad_norm": 0.15034429728984833,
      "learning_rate": 4.5001539882968895e-05,
      "loss": 0.176,
      "step": 37760
    },
    {
      "epoch": 2.326455189405605,
      "grad_norm": 0.20652571320533752,
      "learning_rate": 4.4960476337131715e-05,
      "loss": 0.1785,
      "step": 37770
    },
    {
      "epoch": 2.327071142593163,
      "grad_norm": 0.14263302087783813,
      "learning_rate": 4.491941279129453e-05,
      "loss": 0.1783,
      "step": 37780
    },
    {
      "epoch": 2.3276870957807207,
      "grad_norm": 0.14870823919773102,
      "learning_rate": 4.487834924545735e-05,
      "loss": 0.1757,
      "step": 37790
    },
    {
      "epoch": 2.3283030489682783,
      "grad_norm": 0.16711658239364624,
      "learning_rate": 4.483728569962016e-05,
      "loss": 0.1772,
      "step": 37800
    },
    {
      "epoch": 2.3289190021558364,
      "grad_norm": 0.15951137244701385,
      "learning_rate": 4.479622215378298e-05,
      "loss": 0.177,
      "step": 37810
    },
    {
      "epoch": 2.329534955343394,
      "grad_norm": 0.1491081565618515,
      "learning_rate": 4.4755158607945795e-05,
      "loss": 0.1746,
      "step": 37820
    },
    {
      "epoch": 2.3301509085309515,
      "grad_norm": 0.1494886428117752,
      "learning_rate": 4.4714095062108615e-05,
      "loss": 0.1748,
      "step": 37830
    },
    {
      "epoch": 2.3307668617185096,
      "grad_norm": 0.14477768540382385,
      "learning_rate": 4.467303151627143e-05,
      "loss": 0.1759,
      "step": 37840
    },
    {
      "epoch": 2.331382814906067,
      "grad_norm": 0.17473196983337402,
      "learning_rate": 4.463196797043425e-05,
      "loss": 0.1762,
      "step": 37850
    },
    {
      "epoch": 2.3319987680936247,
      "grad_norm": 0.15328039228916168,
      "learning_rate": 4.459090442459707e-05,
      "loss": 0.1756,
      "step": 37860
    },
    {
      "epoch": 2.3326147212811827,
      "grad_norm": 0.14606790244579315,
      "learning_rate": 4.454984087875989e-05,
      "loss": 0.177,
      "step": 37870
    },
    {
      "epoch": 2.3332306744687403,
      "grad_norm": 0.14434608817100525,
      "learning_rate": 4.45087773329227e-05,
      "loss": 0.1771,
      "step": 37880
    },
    {
      "epoch": 2.333846627656298,
      "grad_norm": 0.23842674493789673,
      "learning_rate": 4.446771378708552e-05,
      "loss": 0.1756,
      "step": 37890
    },
    {
      "epoch": 2.334462580843856,
      "grad_norm": 0.14214374125003815,
      "learning_rate": 4.4426650241248335e-05,
      "loss": 0.176,
      "step": 37900
    },
    {
      "epoch": 2.3350785340314135,
      "grad_norm": 0.14111977815628052,
      "learning_rate": 4.4385586695411155e-05,
      "loss": 0.1761,
      "step": 37910
    },
    {
      "epoch": 2.3356944872189715,
      "grad_norm": 0.15715089440345764,
      "learning_rate": 4.434452314957397e-05,
      "loss": 0.1768,
      "step": 37920
    },
    {
      "epoch": 2.336310440406529,
      "grad_norm": 0.1576632708311081,
      "learning_rate": 4.430345960373679e-05,
      "loss": 0.1751,
      "step": 37930
    },
    {
      "epoch": 2.3369263935940867,
      "grad_norm": 0.14290715754032135,
      "learning_rate": 4.42623960578996e-05,
      "loss": 0.1773,
      "step": 37940
    },
    {
      "epoch": 2.3375423467816447,
      "grad_norm": 0.1583661288022995,
      "learning_rate": 4.422133251206242e-05,
      "loss": 0.1752,
      "step": 37950
    },
    {
      "epoch": 2.3381582999692023,
      "grad_norm": 0.16568200290203094,
      "learning_rate": 4.4180268966225235e-05,
      "loss": 0.1749,
      "step": 37960
    },
    {
      "epoch": 2.33877425315676,
      "grad_norm": 0.1481979638338089,
      "learning_rate": 4.4139205420388055e-05,
      "loss": 0.1757,
      "step": 37970
    },
    {
      "epoch": 2.339390206344318,
      "grad_norm": 0.1486002802848816,
      "learning_rate": 4.409814187455087e-05,
      "loss": 0.1774,
      "step": 37980
    },
    {
      "epoch": 2.3400061595318755,
      "grad_norm": 0.15707999467849731,
      "learning_rate": 4.405707832871369e-05,
      "loss": 0.1764,
      "step": 37990
    },
    {
      "epoch": 2.3406221127194335,
      "grad_norm": 0.14770081639289856,
      "learning_rate": 4.40160147828765e-05,
      "loss": 0.1769,
      "step": 38000
    },
    {
      "epoch": 2.341238065906991,
      "grad_norm": 0.12983278930187225,
      "learning_rate": 4.397495123703932e-05,
      "loss": 0.175,
      "step": 38010
    },
    {
      "epoch": 2.3418540190945487,
      "grad_norm": 0.14471113681793213,
      "learning_rate": 4.3933887691202135e-05,
      "loss": 0.1757,
      "step": 38020
    },
    {
      "epoch": 2.3424699722821067,
      "grad_norm": 0.19091416895389557,
      "learning_rate": 4.3892824145364955e-05,
      "loss": 0.1767,
      "step": 38030
    },
    {
      "epoch": 2.3430859254696643,
      "grad_norm": 0.1576400250196457,
      "learning_rate": 4.385176059952777e-05,
      "loss": 0.1764,
      "step": 38040
    },
    {
      "epoch": 2.343701878657222,
      "grad_norm": 0.1626579463481903,
      "learning_rate": 4.381069705369059e-05,
      "loss": 0.1759,
      "step": 38050
    },
    {
      "epoch": 2.34431783184478,
      "grad_norm": 0.149819478392601,
      "learning_rate": 4.37696335078534e-05,
      "loss": 0.1751,
      "step": 38060
    },
    {
      "epoch": 2.3449337850323375,
      "grad_norm": 0.16347850859165192,
      "learning_rate": 4.372856996201622e-05,
      "loss": 0.1768,
      "step": 38070
    },
    {
      "epoch": 2.345549738219895,
      "grad_norm": 0.16289155185222626,
      "learning_rate": 4.368750641617904e-05,
      "loss": 0.1755,
      "step": 38080
    },
    {
      "epoch": 2.346165691407453,
      "grad_norm": 0.16846109926700592,
      "learning_rate": 4.3646442870341855e-05,
      "loss": 0.1772,
      "step": 38090
    },
    {
      "epoch": 2.3467816445950107,
      "grad_norm": 0.15490089356899261,
      "learning_rate": 4.3605379324504675e-05,
      "loss": 0.1772,
      "step": 38100
    },
    {
      "epoch": 2.3473975977825683,
      "grad_norm": 0.13988350331783295,
      "learning_rate": 4.356431577866749e-05,
      "loss": 0.1753,
      "step": 38110
    },
    {
      "epoch": 2.3480135509701263,
      "grad_norm": 0.14812149107456207,
      "learning_rate": 4.352325223283031e-05,
      "loss": 0.177,
      "step": 38120
    },
    {
      "epoch": 2.348629504157684,
      "grad_norm": 0.15933100879192352,
      "learning_rate": 4.348218868699313e-05,
      "loss": 0.1762,
      "step": 38130
    },
    {
      "epoch": 2.349245457345242,
      "grad_norm": 0.1402917057275772,
      "learning_rate": 4.344112514115594e-05,
      "loss": 0.1759,
      "step": 38140
    },
    {
      "epoch": 2.3498614105327995,
      "grad_norm": 0.15595482289791107,
      "learning_rate": 4.340006159531876e-05,
      "loss": 0.1754,
      "step": 38150
    },
    {
      "epoch": 2.350477363720357,
      "grad_norm": 0.153878316283226,
      "learning_rate": 4.3358998049481575e-05,
      "loss": 0.1761,
      "step": 38160
    },
    {
      "epoch": 2.351093316907915,
      "grad_norm": 0.15784062445163727,
      "learning_rate": 4.3317934503644395e-05,
      "loss": 0.1765,
      "step": 38170
    },
    {
      "epoch": 2.3517092700954727,
      "grad_norm": 0.1510549783706665,
      "learning_rate": 4.327687095780721e-05,
      "loss": 0.1773,
      "step": 38180
    },
    {
      "epoch": 2.3523252232830307,
      "grad_norm": 0.13830670714378357,
      "learning_rate": 4.323580741197003e-05,
      "loss": 0.1768,
      "step": 38190
    },
    {
      "epoch": 2.3529411764705883,
      "grad_norm": 0.15156599879264832,
      "learning_rate": 4.319474386613284e-05,
      "loss": 0.1757,
      "step": 38200
    },
    {
      "epoch": 2.353557129658146,
      "grad_norm": 0.14648215472698212,
      "learning_rate": 4.315368032029566e-05,
      "loss": 0.1752,
      "step": 38210
    },
    {
      "epoch": 2.354173082845704,
      "grad_norm": 0.13808627426624298,
      "learning_rate": 4.3112616774458475e-05,
      "loss": 0.1756,
      "step": 38220
    },
    {
      "epoch": 2.3547890360332615,
      "grad_norm": 0.17207735776901245,
      "learning_rate": 4.3071553228621295e-05,
      "loss": 0.1768,
      "step": 38230
    },
    {
      "epoch": 2.355404989220819,
      "grad_norm": 0.16193509101867676,
      "learning_rate": 4.303048968278411e-05,
      "loss": 0.1742,
      "step": 38240
    },
    {
      "epoch": 2.356020942408377,
      "grad_norm": 0.15756601095199585,
      "learning_rate": 4.298942613694693e-05,
      "loss": 0.1767,
      "step": 38250
    },
    {
      "epoch": 2.3566368955959347,
      "grad_norm": 0.16970455646514893,
      "learning_rate": 4.294836259110974e-05,
      "loss": 0.1775,
      "step": 38260
    },
    {
      "epoch": 2.3572528487834923,
      "grad_norm": 0.14530527591705322,
      "learning_rate": 4.290729904527256e-05,
      "loss": 0.1753,
      "step": 38270
    },
    {
      "epoch": 2.3578688019710503,
      "grad_norm": 0.15950211882591248,
      "learning_rate": 4.286623549943538e-05,
      "loss": 0.1763,
      "step": 38280
    },
    {
      "epoch": 2.358484755158608,
      "grad_norm": 0.15646885335445404,
      "learning_rate": 4.2825171953598195e-05,
      "loss": 0.1748,
      "step": 38290
    },
    {
      "epoch": 2.3591007083461655,
      "grad_norm": 0.16604962944984436,
      "learning_rate": 4.2784108407761015e-05,
      "loss": 0.1757,
      "step": 38300
    },
    {
      "epoch": 2.3597166615337235,
      "grad_norm": 0.15616518259048462,
      "learning_rate": 4.274304486192383e-05,
      "loss": 0.1761,
      "step": 38310
    },
    {
      "epoch": 2.360332614721281,
      "grad_norm": 0.1376827508211136,
      "learning_rate": 4.270198131608665e-05,
      "loss": 0.1752,
      "step": 38320
    },
    {
      "epoch": 2.360948567908839,
      "grad_norm": 0.14600802958011627,
      "learning_rate": 4.266091777024946e-05,
      "loss": 0.1771,
      "step": 38330
    },
    {
      "epoch": 2.3615645210963967,
      "grad_norm": 0.15427643060684204,
      "learning_rate": 4.261985422441228e-05,
      "loss": 0.176,
      "step": 38340
    },
    {
      "epoch": 2.3621804742839543,
      "grad_norm": 0.1630258560180664,
      "learning_rate": 4.2578790678575095e-05,
      "loss": 0.1759,
      "step": 38350
    },
    {
      "epoch": 2.3627964274715123,
      "grad_norm": 0.16006769239902496,
      "learning_rate": 4.2537727132737915e-05,
      "loss": 0.175,
      "step": 38360
    },
    {
      "epoch": 2.36341238065907,
      "grad_norm": 0.16226665675640106,
      "learning_rate": 4.249666358690073e-05,
      "loss": 0.1772,
      "step": 38370
    },
    {
      "epoch": 2.3640283338466275,
      "grad_norm": 0.16559739410877228,
      "learning_rate": 4.245560004106355e-05,
      "loss": 0.1766,
      "step": 38380
    },
    {
      "epoch": 2.3646442870341855,
      "grad_norm": 0.14176416397094727,
      "learning_rate": 4.241453649522636e-05,
      "loss": 0.1755,
      "step": 38390
    },
    {
      "epoch": 2.365260240221743,
      "grad_norm": 0.14918099343776703,
      "learning_rate": 4.237347294938918e-05,
      "loss": 0.1771,
      "step": 38400
    },
    {
      "epoch": 2.365876193409301,
      "grad_norm": 0.1502959430217743,
      "learning_rate": 4.2332409403552e-05,
      "loss": 0.1773,
      "step": 38410
    },
    {
      "epoch": 2.3664921465968587,
      "grad_norm": 0.1664413958787918,
      "learning_rate": 4.2291345857714815e-05,
      "loss": 0.1784,
      "step": 38420
    },
    {
      "epoch": 2.3671080997844163,
      "grad_norm": 0.12458426505327225,
      "learning_rate": 4.2250282311877635e-05,
      "loss": 0.1759,
      "step": 38430
    },
    {
      "epoch": 2.3677240529719743,
      "grad_norm": 0.14650380611419678,
      "learning_rate": 4.220921876604045e-05,
      "loss": 0.1777,
      "step": 38440
    },
    {
      "epoch": 2.368340006159532,
      "grad_norm": 0.14930598437786102,
      "learning_rate": 4.216815522020327e-05,
      "loss": 0.1743,
      "step": 38450
    },
    {
      "epoch": 2.3689559593470895,
      "grad_norm": 0.1417573094367981,
      "learning_rate": 4.212709167436608e-05,
      "loss": 0.1769,
      "step": 38460
    },
    {
      "epoch": 2.3695719125346475,
      "grad_norm": 0.15611505508422852,
      "learning_rate": 4.20860281285289e-05,
      "loss": 0.1761,
      "step": 38470
    },
    {
      "epoch": 2.370187865722205,
      "grad_norm": 0.1378881186246872,
      "learning_rate": 4.2044964582691714e-05,
      "loss": 0.1758,
      "step": 38480
    },
    {
      "epoch": 2.3708038189097627,
      "grad_norm": 0.14762836694717407,
      "learning_rate": 4.2003901036854535e-05,
      "loss": 0.1758,
      "step": 38490
    },
    {
      "epoch": 2.3714197720973207,
      "grad_norm": 0.14179293811321259,
      "learning_rate": 4.1962837491017355e-05,
      "loss": 0.1767,
      "step": 38500
    },
    {
      "epoch": 2.3720357252848783,
      "grad_norm": 0.13675037026405334,
      "learning_rate": 4.192177394518017e-05,
      "loss": 0.1752,
      "step": 38510
    },
    {
      "epoch": 2.372651678472436,
      "grad_norm": 0.14888805150985718,
      "learning_rate": 4.188071039934299e-05,
      "loss": 0.1769,
      "step": 38520
    },
    {
      "epoch": 2.373267631659994,
      "grad_norm": 0.15346196293830872,
      "learning_rate": 4.18396468535058e-05,
      "loss": 0.1751,
      "step": 38530
    },
    {
      "epoch": 2.3738835848475515,
      "grad_norm": 0.14991623163223267,
      "learning_rate": 4.179858330766862e-05,
      "loss": 0.1741,
      "step": 38540
    },
    {
      "epoch": 2.3744995380351095,
      "grad_norm": 0.15066073834896088,
      "learning_rate": 4.1757519761831434e-05,
      "loss": 0.1764,
      "step": 38550
    },
    {
      "epoch": 2.375115491222667,
      "grad_norm": 0.14764995872974396,
      "learning_rate": 4.1716456215994254e-05,
      "loss": 0.175,
      "step": 38560
    },
    {
      "epoch": 2.3757314444102247,
      "grad_norm": 0.15509556233882904,
      "learning_rate": 4.167539267015707e-05,
      "loss": 0.1755,
      "step": 38570
    },
    {
      "epoch": 2.3763473975977827,
      "grad_norm": 0.14271996915340424,
      "learning_rate": 4.163432912431989e-05,
      "loss": 0.1751,
      "step": 38580
    },
    {
      "epoch": 2.3769633507853403,
      "grad_norm": 0.15607453882694244,
      "learning_rate": 4.15932655784827e-05,
      "loss": 0.178,
      "step": 38590
    },
    {
      "epoch": 2.3775793039728983,
      "grad_norm": 0.14911282062530518,
      "learning_rate": 4.155220203264552e-05,
      "loss": 0.1743,
      "step": 38600
    },
    {
      "epoch": 2.378195257160456,
      "grad_norm": 0.1375204175710678,
      "learning_rate": 4.1511138486808334e-05,
      "loss": 0.1754,
      "step": 38610
    },
    {
      "epoch": 2.3788112103480135,
      "grad_norm": 0.19414985179901123,
      "learning_rate": 4.1470074940971154e-05,
      "loss": 0.178,
      "step": 38620
    },
    {
      "epoch": 2.3794271635355715,
      "grad_norm": 0.1387588083744049,
      "learning_rate": 4.142901139513397e-05,
      "loss": 0.1755,
      "step": 38630
    },
    {
      "epoch": 2.380043116723129,
      "grad_norm": 0.15105271339416504,
      "learning_rate": 4.138794784929679e-05,
      "loss": 0.1768,
      "step": 38640
    },
    {
      "epoch": 2.3806590699106867,
      "grad_norm": 0.13701333105564117,
      "learning_rate": 4.13468843034596e-05,
      "loss": 0.1758,
      "step": 38650
    },
    {
      "epoch": 2.3812750230982447,
      "grad_norm": 0.14064256846904755,
      "learning_rate": 4.130582075762242e-05,
      "loss": 0.175,
      "step": 38660
    },
    {
      "epoch": 2.3818909762858023,
      "grad_norm": 0.15730486810207367,
      "learning_rate": 4.126475721178524e-05,
      "loss": 0.176,
      "step": 38670
    },
    {
      "epoch": 2.38250692947336,
      "grad_norm": 0.14256605505943298,
      "learning_rate": 4.1223693665948054e-05,
      "loss": 0.176,
      "step": 38680
    },
    {
      "epoch": 2.383122882660918,
      "grad_norm": 0.1592540293931961,
      "learning_rate": 4.1182630120110874e-05,
      "loss": 0.1754,
      "step": 38690
    },
    {
      "epoch": 2.3837388358484755,
      "grad_norm": 0.16090025007724762,
      "learning_rate": 4.114156657427369e-05,
      "loss": 0.1769,
      "step": 38700
    },
    {
      "epoch": 2.384354789036033,
      "grad_norm": 0.1578965187072754,
      "learning_rate": 4.110050302843651e-05,
      "loss": 0.1766,
      "step": 38710
    },
    {
      "epoch": 2.384970742223591,
      "grad_norm": 0.16379383206367493,
      "learning_rate": 4.105943948259933e-05,
      "loss": 0.1757,
      "step": 38720
    },
    {
      "epoch": 2.3855866954111486,
      "grad_norm": 0.16381746530532837,
      "learning_rate": 4.101837593676214e-05,
      "loss": 0.1752,
      "step": 38730
    },
    {
      "epoch": 2.3862026485987067,
      "grad_norm": 0.16845740377902985,
      "learning_rate": 4.097731239092496e-05,
      "loss": 0.1763,
      "step": 38740
    },
    {
      "epoch": 2.3868186017862643,
      "grad_norm": 0.13379095494747162,
      "learning_rate": 4.0936248845087774e-05,
      "loss": 0.1754,
      "step": 38750
    },
    {
      "epoch": 2.387434554973822,
      "grad_norm": 0.15856163203716278,
      "learning_rate": 4.0895185299250594e-05,
      "loss": 0.1738,
      "step": 38760
    },
    {
      "epoch": 2.38805050816138,
      "grad_norm": 0.13217003643512726,
      "learning_rate": 4.085412175341341e-05,
      "loss": 0.1751,
      "step": 38770
    },
    {
      "epoch": 2.3886664613489375,
      "grad_norm": 0.1453995555639267,
      "learning_rate": 4.081305820757623e-05,
      "loss": 0.1743,
      "step": 38780
    },
    {
      "epoch": 2.389282414536495,
      "grad_norm": 0.15101708471775055,
      "learning_rate": 4.077199466173904e-05,
      "loss": 0.1759,
      "step": 38790
    },
    {
      "epoch": 2.389898367724053,
      "grad_norm": 0.15078523755073547,
      "learning_rate": 4.073093111590186e-05,
      "loss": 0.1749,
      "step": 38800
    },
    {
      "epoch": 2.3905143209116106,
      "grad_norm": 0.14701427519321442,
      "learning_rate": 4.0689867570064674e-05,
      "loss": 0.1761,
      "step": 38810
    },
    {
      "epoch": 2.3911302740991687,
      "grad_norm": 0.14683757722377777,
      "learning_rate": 4.0648804024227494e-05,
      "loss": 0.1763,
      "step": 38820
    },
    {
      "epoch": 2.3917462272867263,
      "grad_norm": 0.15207456052303314,
      "learning_rate": 4.060774047839031e-05,
      "loss": 0.1761,
      "step": 38830
    },
    {
      "epoch": 2.392362180474284,
      "grad_norm": 0.14970095455646515,
      "learning_rate": 4.056667693255313e-05,
      "loss": 0.1769,
      "step": 38840
    },
    {
      "epoch": 2.392978133661842,
      "grad_norm": 0.148675799369812,
      "learning_rate": 4.052561338671594e-05,
      "loss": 0.1735,
      "step": 38850
    },
    {
      "epoch": 2.3935940868493994,
      "grad_norm": 0.13451489806175232,
      "learning_rate": 4.048454984087876e-05,
      "loss": 0.1755,
      "step": 38860
    },
    {
      "epoch": 2.394210040036957,
      "grad_norm": 0.15388287603855133,
      "learning_rate": 4.0443486295041574e-05,
      "loss": 0.1771,
      "step": 38870
    },
    {
      "epoch": 2.394825993224515,
      "grad_norm": 0.1512860208749771,
      "learning_rate": 4.0402422749204394e-05,
      "loss": 0.1769,
      "step": 38880
    },
    {
      "epoch": 2.3954419464120726,
      "grad_norm": 0.17843873798847198,
      "learning_rate": 4.036135920336721e-05,
      "loss": 0.1787,
      "step": 38890
    },
    {
      "epoch": 2.3960578995996302,
      "grad_norm": 0.15394021570682526,
      "learning_rate": 4.032029565753003e-05,
      "loss": 0.1756,
      "step": 38900
    },
    {
      "epoch": 2.3966738527871883,
      "grad_norm": 0.1695067286491394,
      "learning_rate": 4.027923211169284e-05,
      "loss": 0.1766,
      "step": 38910
    },
    {
      "epoch": 2.397289805974746,
      "grad_norm": 0.176316037774086,
      "learning_rate": 4.023816856585567e-05,
      "loss": 0.1757,
      "step": 38920
    },
    {
      "epoch": 2.3979057591623034,
      "grad_norm": 0.13350172340869904,
      "learning_rate": 4.019710502001848e-05,
      "loss": 0.1756,
      "step": 38930
    },
    {
      "epoch": 2.3985217123498614,
      "grad_norm": 0.13783279061317444,
      "learning_rate": 4.01560414741813e-05,
      "loss": 0.1745,
      "step": 38940
    },
    {
      "epoch": 2.399137665537419,
      "grad_norm": 0.15702380239963531,
      "learning_rate": 4.0114977928344114e-05,
      "loss": 0.1756,
      "step": 38950
    },
    {
      "epoch": 2.399753618724977,
      "grad_norm": 0.14347124099731445,
      "learning_rate": 4.0073914382506934e-05,
      "loss": 0.1747,
      "step": 38960
    },
    {
      "epoch": 2.4003695719125346,
      "grad_norm": 0.15417622029781342,
      "learning_rate": 4.003285083666975e-05,
      "loss": 0.1772,
      "step": 38970
    },
    {
      "epoch": 2.400985525100092,
      "grad_norm": 0.13676103949546814,
      "learning_rate": 3.999178729083257e-05,
      "loss": 0.1762,
      "step": 38980
    },
    {
      "epoch": 2.4016014782876502,
      "grad_norm": 0.1554720103740692,
      "learning_rate": 3.995072374499538e-05,
      "loss": 0.1759,
      "step": 38990
    },
    {
      "epoch": 2.402217431475208,
      "grad_norm": 0.13751836121082306,
      "learning_rate": 3.99096601991582e-05,
      "loss": 0.1749,
      "step": 39000
    },
    {
      "epoch": 2.402833384662766,
      "grad_norm": 0.13713158667087555,
      "learning_rate": 3.9868596653321014e-05,
      "loss": 0.1766,
      "step": 39010
    },
    {
      "epoch": 2.4034493378503234,
      "grad_norm": 0.16093309223651886,
      "learning_rate": 3.9827533107483834e-05,
      "loss": 0.1784,
      "step": 39020
    },
    {
      "epoch": 2.404065291037881,
      "grad_norm": 0.12932337820529938,
      "learning_rate": 3.978646956164665e-05,
      "loss": 0.1754,
      "step": 39030
    },
    {
      "epoch": 2.404681244225439,
      "grad_norm": 0.1548624485731125,
      "learning_rate": 3.974540601580947e-05,
      "loss": 0.1753,
      "step": 39040
    },
    {
      "epoch": 2.4052971974129966,
      "grad_norm": 0.13587968051433563,
      "learning_rate": 3.970434246997228e-05,
      "loss": 0.1772,
      "step": 39050
    },
    {
      "epoch": 2.405913150600554,
      "grad_norm": 0.14125868678092957,
      "learning_rate": 3.96632789241351e-05,
      "loss": 0.1762,
      "step": 39060
    },
    {
      "epoch": 2.4065291037881122,
      "grad_norm": 0.17131729423999786,
      "learning_rate": 3.9622215378297914e-05,
      "loss": 0.1748,
      "step": 39070
    },
    {
      "epoch": 2.40714505697567,
      "grad_norm": 0.15862949192523956,
      "learning_rate": 3.9581151832460734e-05,
      "loss": 0.1765,
      "step": 39080
    },
    {
      "epoch": 2.4077610101632274,
      "grad_norm": 0.15003494918346405,
      "learning_rate": 3.954008828662355e-05,
      "loss": 0.1765,
      "step": 39090
    },
    {
      "epoch": 2.4083769633507854,
      "grad_norm": 0.14879679679870605,
      "learning_rate": 3.949902474078637e-05,
      "loss": 0.1744,
      "step": 39100
    },
    {
      "epoch": 2.408992916538343,
      "grad_norm": 0.16255728900432587,
      "learning_rate": 3.945796119494918e-05,
      "loss": 0.1773,
      "step": 39110
    },
    {
      "epoch": 2.4096088697259006,
      "grad_norm": 0.14005430042743683,
      "learning_rate": 3.9416897649112e-05,
      "loss": 0.1772,
      "step": 39120
    },
    {
      "epoch": 2.4102248229134586,
      "grad_norm": 0.15191690623760223,
      "learning_rate": 3.937583410327482e-05,
      "loss": 0.1765,
      "step": 39130
    },
    {
      "epoch": 2.410840776101016,
      "grad_norm": 0.14968915283679962,
      "learning_rate": 3.933477055743764e-05,
      "loss": 0.1766,
      "step": 39140
    },
    {
      "epoch": 2.4114567292885742,
      "grad_norm": 0.13745145499706268,
      "learning_rate": 3.9293707011600454e-05,
      "loss": 0.1756,
      "step": 39150
    },
    {
      "epoch": 2.412072682476132,
      "grad_norm": 0.15950845181941986,
      "learning_rate": 3.9252643465763274e-05,
      "loss": 0.1766,
      "step": 39160
    },
    {
      "epoch": 2.4126886356636894,
      "grad_norm": 0.13114312291145325,
      "learning_rate": 3.921157991992609e-05,
      "loss": 0.1757,
      "step": 39170
    },
    {
      "epoch": 2.4133045888512474,
      "grad_norm": 0.1479123830795288,
      "learning_rate": 3.917051637408891e-05,
      "loss": 0.1754,
      "step": 39180
    },
    {
      "epoch": 2.413920542038805,
      "grad_norm": 0.13374315202236176,
      "learning_rate": 3.912945282825172e-05,
      "loss": 0.177,
      "step": 39190
    },
    {
      "epoch": 2.414536495226363,
      "grad_norm": 0.1490931212902069,
      "learning_rate": 3.908838928241454e-05,
      "loss": 0.1761,
      "step": 39200
    },
    {
      "epoch": 2.4151524484139206,
      "grad_norm": 0.155513733625412,
      "learning_rate": 3.9047325736577354e-05,
      "loss": 0.1758,
      "step": 39210
    },
    {
      "epoch": 2.415768401601478,
      "grad_norm": 0.14676134288311005,
      "learning_rate": 3.9006262190740174e-05,
      "loss": 0.1754,
      "step": 39220
    },
    {
      "epoch": 2.4163843547890362,
      "grad_norm": 0.1716591715812683,
      "learning_rate": 3.896519864490299e-05,
      "loss": 0.1761,
      "step": 39230
    },
    {
      "epoch": 2.417000307976594,
      "grad_norm": 0.15675313770771027,
      "learning_rate": 3.892413509906581e-05,
      "loss": 0.1761,
      "step": 39240
    },
    {
      "epoch": 2.4176162611641514,
      "grad_norm": 0.1538383811712265,
      "learning_rate": 3.888307155322862e-05,
      "loss": 0.1754,
      "step": 39250
    },
    {
      "epoch": 2.4182322143517094,
      "grad_norm": 0.14158257842063904,
      "learning_rate": 3.884200800739144e-05,
      "loss": 0.1774,
      "step": 39260
    },
    {
      "epoch": 2.418848167539267,
      "grad_norm": 0.14495083689689636,
      "learning_rate": 3.8800944461554254e-05,
      "loss": 0.1751,
      "step": 39270
    },
    {
      "epoch": 2.4194641207268246,
      "grad_norm": 0.13481271266937256,
      "learning_rate": 3.8759880915717074e-05,
      "loss": 0.1735,
      "step": 39280
    },
    {
      "epoch": 2.4200800739143826,
      "grad_norm": 0.13998118042945862,
      "learning_rate": 3.871881736987989e-05,
      "loss": 0.1754,
      "step": 39290
    },
    {
      "epoch": 2.42069602710194,
      "grad_norm": 0.14061355590820312,
      "learning_rate": 3.867775382404271e-05,
      "loss": 0.1762,
      "step": 39300
    },
    {
      "epoch": 2.421311980289498,
      "grad_norm": 0.13503848016262054,
      "learning_rate": 3.863669027820552e-05,
      "loss": 0.1769,
      "step": 39310
    },
    {
      "epoch": 2.421927933477056,
      "grad_norm": 0.15798768401145935,
      "learning_rate": 3.859562673236834e-05,
      "loss": 0.1738,
      "step": 39320
    },
    {
      "epoch": 2.4225438866646134,
      "grad_norm": 0.15758831799030304,
      "learning_rate": 3.8554563186531153e-05,
      "loss": 0.1758,
      "step": 39330
    },
    {
      "epoch": 2.4231598398521714,
      "grad_norm": 0.1571352779865265,
      "learning_rate": 3.851349964069398e-05,
      "loss": 0.1762,
      "step": 39340
    },
    {
      "epoch": 2.423775793039729,
      "grad_norm": 0.13968467712402344,
      "learning_rate": 3.8472436094856793e-05,
      "loss": 0.1759,
      "step": 39350
    },
    {
      "epoch": 2.4243917462272866,
      "grad_norm": 0.14538580179214478,
      "learning_rate": 3.8431372549019614e-05,
      "loss": 0.1759,
      "step": 39360
    },
    {
      "epoch": 2.4250076994148446,
      "grad_norm": 0.17811256647109985,
      "learning_rate": 3.839030900318243e-05,
      "loss": 0.1768,
      "step": 39370
    },
    {
      "epoch": 2.425623652602402,
      "grad_norm": 0.14465691149234772,
      "learning_rate": 3.834924545734525e-05,
      "loss": 0.1756,
      "step": 39380
    },
    {
      "epoch": 2.42623960578996,
      "grad_norm": 0.1392429918050766,
      "learning_rate": 3.830818191150806e-05,
      "loss": 0.1771,
      "step": 39390
    },
    {
      "epoch": 2.426855558977518,
      "grad_norm": 0.13029269874095917,
      "learning_rate": 3.826711836567088e-05,
      "loss": 0.1744,
      "step": 39400
    },
    {
      "epoch": 2.4274715121650754,
      "grad_norm": 0.15828837454319,
      "learning_rate": 3.822605481983369e-05,
      "loss": 0.1767,
      "step": 39410
    },
    {
      "epoch": 2.4280874653526334,
      "grad_norm": 0.1379520446062088,
      "learning_rate": 3.8184991273996513e-05,
      "loss": 0.1747,
      "step": 39420
    },
    {
      "epoch": 2.428703418540191,
      "grad_norm": 0.156297504901886,
      "learning_rate": 3.814392772815933e-05,
      "loss": 0.176,
      "step": 39430
    },
    {
      "epoch": 2.4293193717277486,
      "grad_norm": 0.13030369579792023,
      "learning_rate": 3.810286418232215e-05,
      "loss": 0.1769,
      "step": 39440
    },
    {
      "epoch": 2.4299353249153066,
      "grad_norm": 0.15216003358364105,
      "learning_rate": 3.806180063648496e-05,
      "loss": 0.1758,
      "step": 39450
    },
    {
      "epoch": 2.430551278102864,
      "grad_norm": 0.15398350358009338,
      "learning_rate": 3.802073709064778e-05,
      "loss": 0.176,
      "step": 39460
    },
    {
      "epoch": 2.431167231290422,
      "grad_norm": 0.17458023130893707,
      "learning_rate": 3.797967354481059e-05,
      "loss": 0.1764,
      "step": 39470
    },
    {
      "epoch": 2.43178318447798,
      "grad_norm": 0.15928316116333008,
      "learning_rate": 3.793860999897341e-05,
      "loss": 0.1761,
      "step": 39480
    },
    {
      "epoch": 2.4323991376655374,
      "grad_norm": 0.13738352060317993,
      "learning_rate": 3.7897546453136227e-05,
      "loss": 0.1763,
      "step": 39490
    },
    {
      "epoch": 2.433015090853095,
      "grad_norm": 0.15233229100704193,
      "learning_rate": 3.7856482907299047e-05,
      "loss": 0.1761,
      "step": 39500
    },
    {
      "epoch": 2.433631044040653,
      "grad_norm": 0.1428753137588501,
      "learning_rate": 3.781541936146186e-05,
      "loss": 0.1759,
      "step": 39510
    },
    {
      "epoch": 2.4342469972282106,
      "grad_norm": 0.16505806148052216,
      "learning_rate": 3.777435581562468e-05,
      "loss": 0.1774,
      "step": 39520
    },
    {
      "epoch": 2.434862950415768,
      "grad_norm": 0.14027464389801025,
      "learning_rate": 3.773329226978749e-05,
      "loss": 0.1749,
      "step": 39530
    },
    {
      "epoch": 2.435478903603326,
      "grad_norm": 0.14478495717048645,
      "learning_rate": 3.769222872395031e-05,
      "loss": 0.1766,
      "step": 39540
    },
    {
      "epoch": 2.436094856790884,
      "grad_norm": 0.14299936592578888,
      "learning_rate": 3.7651165178113126e-05,
      "loss": 0.178,
      "step": 39550
    },
    {
      "epoch": 2.436710809978442,
      "grad_norm": 0.15112964808940887,
      "learning_rate": 3.761010163227595e-05,
      "loss": 0.1752,
      "step": 39560
    },
    {
      "epoch": 2.4373267631659994,
      "grad_norm": 0.14015910029411316,
      "learning_rate": 3.7569038086438767e-05,
      "loss": 0.1743,
      "step": 39570
    },
    {
      "epoch": 2.437942716353557,
      "grad_norm": 0.14649821817874908,
      "learning_rate": 3.7527974540601587e-05,
      "loss": 0.177,
      "step": 39580
    },
    {
      "epoch": 2.438558669541115,
      "grad_norm": 0.16100747883319855,
      "learning_rate": 3.74869109947644e-05,
      "loss": 0.1751,
      "step": 39590
    },
    {
      "epoch": 2.4391746227286726,
      "grad_norm": 0.1723460555076599,
      "learning_rate": 3.744584744892722e-05,
      "loss": 0.1765,
      "step": 39600
    },
    {
      "epoch": 2.4397905759162306,
      "grad_norm": 0.1390155851840973,
      "learning_rate": 3.740478390309003e-05,
      "loss": 0.1756,
      "step": 39610
    },
    {
      "epoch": 2.440406529103788,
      "grad_norm": 0.14999032020568848,
      "learning_rate": 3.736372035725285e-05,
      "loss": 0.1753,
      "step": 39620
    },
    {
      "epoch": 2.4410224822913458,
      "grad_norm": 0.16756723821163177,
      "learning_rate": 3.7322656811415666e-05,
      "loss": 0.178,
      "step": 39630
    },
    {
      "epoch": 2.441638435478904,
      "grad_norm": 0.13313348591327667,
      "learning_rate": 3.7281593265578486e-05,
      "loss": 0.1738,
      "step": 39640
    },
    {
      "epoch": 2.4422543886664614,
      "grad_norm": 0.14790792763233185,
      "learning_rate": 3.72405297197413e-05,
      "loss": 0.1763,
      "step": 39650
    },
    {
      "epoch": 2.442870341854019,
      "grad_norm": 0.14355041086673737,
      "learning_rate": 3.719946617390412e-05,
      "loss": 0.1759,
      "step": 39660
    },
    {
      "epoch": 2.443486295041577,
      "grad_norm": 0.1530490517616272,
      "learning_rate": 3.715840262806693e-05,
      "loss": 0.1781,
      "step": 39670
    },
    {
      "epoch": 2.4441022482291346,
      "grad_norm": 0.14373061060905457,
      "learning_rate": 3.711733908222975e-05,
      "loss": 0.1761,
      "step": 39680
    },
    {
      "epoch": 2.444718201416692,
      "grad_norm": 0.17253276705741882,
      "learning_rate": 3.7076275536392566e-05,
      "loss": 0.1768,
      "step": 39690
    },
    {
      "epoch": 2.44533415460425,
      "grad_norm": 0.1546674519777298,
      "learning_rate": 3.7035211990555386e-05,
      "loss": 0.1769,
      "step": 39700
    },
    {
      "epoch": 2.4459501077918078,
      "grad_norm": 0.14526678621768951,
      "learning_rate": 3.69941484447182e-05,
      "loss": 0.1754,
      "step": 39710
    },
    {
      "epoch": 2.4465660609793654,
      "grad_norm": 0.13978612422943115,
      "learning_rate": 3.695308489888102e-05,
      "loss": 0.1769,
      "step": 39720
    },
    {
      "epoch": 2.4471820141669234,
      "grad_norm": 0.17348994314670563,
      "learning_rate": 3.691202135304383e-05,
      "loss": 0.1743,
      "step": 39730
    },
    {
      "epoch": 2.447797967354481,
      "grad_norm": 0.16957879066467285,
      "learning_rate": 3.687095780720665e-05,
      "loss": 0.1758,
      "step": 39740
    },
    {
      "epoch": 2.448413920542039,
      "grad_norm": 0.14460451900959015,
      "learning_rate": 3.6829894261369466e-05,
      "loss": 0.1762,
      "step": 39750
    },
    {
      "epoch": 2.4490298737295966,
      "grad_norm": 0.15625905990600586,
      "learning_rate": 3.6788830715532286e-05,
      "loss": 0.1751,
      "step": 39760
    },
    {
      "epoch": 2.449645826917154,
      "grad_norm": 0.13773207366466522,
      "learning_rate": 3.6747767169695106e-05,
      "loss": 0.1759,
      "step": 39770
    },
    {
      "epoch": 2.450261780104712,
      "grad_norm": 0.15733702480793,
      "learning_rate": 3.6706703623857926e-05,
      "loss": 0.1757,
      "step": 39780
    },
    {
      "epoch": 2.4508777332922698,
      "grad_norm": 0.14372341334819794,
      "learning_rate": 3.666564007802074e-05,
      "loss": 0.1752,
      "step": 39790
    },
    {
      "epoch": 2.4514936864798273,
      "grad_norm": 0.15109704434871674,
      "learning_rate": 3.662457653218356e-05,
      "loss": 0.1752,
      "step": 39800
    },
    {
      "epoch": 2.4521096396673854,
      "grad_norm": 0.14924803376197815,
      "learning_rate": 3.658351298634637e-05,
      "loss": 0.1743,
      "step": 39810
    },
    {
      "epoch": 2.452725592854943,
      "grad_norm": 0.1392984539270401,
      "learning_rate": 3.654244944050919e-05,
      "loss": 0.1767,
      "step": 39820
    },
    {
      "epoch": 2.453341546042501,
      "grad_norm": 0.14645890891551971,
      "learning_rate": 3.6501385894672006e-05,
      "loss": 0.1753,
      "step": 39830
    },
    {
      "epoch": 2.4539574992300586,
      "grad_norm": 0.17410454154014587,
      "learning_rate": 3.6460322348834826e-05,
      "loss": 0.1772,
      "step": 39840
    },
    {
      "epoch": 2.454573452417616,
      "grad_norm": 0.1634814441204071,
      "learning_rate": 3.641925880299764e-05,
      "loss": 0.1772,
      "step": 39850
    },
    {
      "epoch": 2.455189405605174,
      "grad_norm": 0.1583043336868286,
      "learning_rate": 3.637819525716046e-05,
      "loss": 0.1759,
      "step": 39860
    },
    {
      "epoch": 2.4558053587927318,
      "grad_norm": 0.1440555602312088,
      "learning_rate": 3.633713171132327e-05,
      "loss": 0.1769,
      "step": 39870
    },
    {
      "epoch": 2.4564213119802893,
      "grad_norm": 0.1546144187450409,
      "learning_rate": 3.629606816548609e-05,
      "loss": 0.1757,
      "step": 39880
    },
    {
      "epoch": 2.4570372651678474,
      "grad_norm": 0.15054132044315338,
      "learning_rate": 3.6255004619648906e-05,
      "loss": 0.1754,
      "step": 39890
    },
    {
      "epoch": 2.457653218355405,
      "grad_norm": 0.14515598118305206,
      "learning_rate": 3.6213941073811726e-05,
      "loss": 0.1755,
      "step": 39900
    },
    {
      "epoch": 2.4582691715429625,
      "grad_norm": 0.13683468103408813,
      "learning_rate": 3.617287752797454e-05,
      "loss": 0.1762,
      "step": 39910
    },
    {
      "epoch": 2.4588851247305206,
      "grad_norm": 0.15182740986347198,
      "learning_rate": 3.613181398213736e-05,
      "loss": 0.1748,
      "step": 39920
    },
    {
      "epoch": 2.459501077918078,
      "grad_norm": 0.14941810071468353,
      "learning_rate": 3.609075043630017e-05,
      "loss": 0.1765,
      "step": 39930
    },
    {
      "epoch": 2.4601170311056357,
      "grad_norm": 0.14450089633464813,
      "learning_rate": 3.604968689046299e-05,
      "loss": 0.1764,
      "step": 39940
    },
    {
      "epoch": 2.4607329842931938,
      "grad_norm": 0.1483171582221985,
      "learning_rate": 3.6008623344625806e-05,
      "loss": 0.1765,
      "step": 39950
    },
    {
      "epoch": 2.4613489374807513,
      "grad_norm": 0.14022380113601685,
      "learning_rate": 3.5967559798788626e-05,
      "loss": 0.1768,
      "step": 39960
    },
    {
      "epoch": 2.4619648906683094,
      "grad_norm": 0.14298714697360992,
      "learning_rate": 3.592649625295144e-05,
      "loss": 0.1765,
      "step": 39970
    },
    {
      "epoch": 2.462580843855867,
      "grad_norm": 0.17715390026569366,
      "learning_rate": 3.5885432707114266e-05,
      "loss": 0.1761,
      "step": 39980
    },
    {
      "epoch": 2.4631967970434245,
      "grad_norm": 0.148204505443573,
      "learning_rate": 3.584436916127708e-05,
      "loss": 0.1758,
      "step": 39990
    },
    {
      "epoch": 2.4638127502309826,
      "grad_norm": 0.14528897404670715,
      "learning_rate": 3.58033056154399e-05,
      "loss": 0.1771,
      "step": 40000
    },
    {
      "epoch": 2.46442870341854,
      "grad_norm": 0.13504581153392792,
      "learning_rate": 3.576224206960271e-05,
      "loss": 0.1739,
      "step": 40010
    },
    {
      "epoch": 2.465044656606098,
      "grad_norm": 0.17231659591197968,
      "learning_rate": 3.572117852376553e-05,
      "loss": 0.1762,
      "step": 40020
    },
    {
      "epoch": 2.4656606097936558,
      "grad_norm": 0.13669010996818542,
      "learning_rate": 3.5680114977928346e-05,
      "loss": 0.1768,
      "step": 40030
    },
    {
      "epoch": 2.4662765629812133,
      "grad_norm": 0.12809377908706665,
      "learning_rate": 3.5639051432091166e-05,
      "loss": 0.1745,
      "step": 40040
    },
    {
      "epoch": 2.4668925161687714,
      "grad_norm": 0.13242806494235992,
      "learning_rate": 3.559798788625398e-05,
      "loss": 0.175,
      "step": 40050
    },
    {
      "epoch": 2.467508469356329,
      "grad_norm": 0.14965371787548065,
      "learning_rate": 3.55569243404168e-05,
      "loss": 0.1765,
      "step": 40060
    },
    {
      "epoch": 2.4681244225438865,
      "grad_norm": 0.14803719520568848,
      "learning_rate": 3.551586079457961e-05,
      "loss": 0.1754,
      "step": 40070
    },
    {
      "epoch": 2.4687403757314446,
      "grad_norm": 0.14960306882858276,
      "learning_rate": 3.547479724874243e-05,
      "loss": 0.1757,
      "step": 40080
    },
    {
      "epoch": 2.469356328919002,
      "grad_norm": 0.14921711385250092,
      "learning_rate": 3.5433733702905246e-05,
      "loss": 0.1754,
      "step": 40090
    },
    {
      "epoch": 2.4699722821065597,
      "grad_norm": 0.12837623059749603,
      "learning_rate": 3.5392670157068066e-05,
      "loss": 0.1742,
      "step": 40100
    },
    {
      "epoch": 2.4705882352941178,
      "grad_norm": 0.14111420512199402,
      "learning_rate": 3.535160661123088e-05,
      "loss": 0.176,
      "step": 40110
    },
    {
      "epoch": 2.4712041884816753,
      "grad_norm": 0.1574840545654297,
      "learning_rate": 3.53105430653937e-05,
      "loss": 0.1759,
      "step": 40120
    },
    {
      "epoch": 2.471820141669233,
      "grad_norm": 0.1639832854270935,
      "learning_rate": 3.526947951955651e-05,
      "loss": 0.1772,
      "step": 40130
    },
    {
      "epoch": 2.472436094856791,
      "grad_norm": 0.16494794189929962,
      "learning_rate": 3.522841597371933e-05,
      "loss": 0.1765,
      "step": 40140
    },
    {
      "epoch": 2.4730520480443485,
      "grad_norm": 0.16179728507995605,
      "learning_rate": 3.5187352427882146e-05,
      "loss": 0.1763,
      "step": 40150
    },
    {
      "epoch": 2.4736680012319066,
      "grad_norm": 0.19527311623096466,
      "learning_rate": 3.5146288882044966e-05,
      "loss": 0.1768,
      "step": 40160
    },
    {
      "epoch": 2.474283954419464,
      "grad_norm": 0.16890855133533478,
      "learning_rate": 3.510522533620778e-05,
      "loss": 0.1747,
      "step": 40170
    },
    {
      "epoch": 2.4748999076070217,
      "grad_norm": 0.15916483104228973,
      "learning_rate": 3.50641617903706e-05,
      "loss": 0.1757,
      "step": 40180
    },
    {
      "epoch": 2.4755158607945797,
      "grad_norm": 0.15362732112407684,
      "learning_rate": 3.502309824453341e-05,
      "loss": 0.1771,
      "step": 40190
    },
    {
      "epoch": 2.4761318139821373,
      "grad_norm": 0.1344437301158905,
      "learning_rate": 3.498203469869624e-05,
      "loss": 0.1754,
      "step": 40200
    },
    {
      "epoch": 2.476747767169695,
      "grad_norm": 0.13826307654380798,
      "learning_rate": 3.494097115285905e-05,
      "loss": 0.1749,
      "step": 40210
    },
    {
      "epoch": 2.477363720357253,
      "grad_norm": 0.1851799041032791,
      "learning_rate": 3.489990760702187e-05,
      "loss": 0.1767,
      "step": 40220
    },
    {
      "epoch": 2.4779796735448105,
      "grad_norm": 0.17150923609733582,
      "learning_rate": 3.4858844061184686e-05,
      "loss": 0.1758,
      "step": 40230
    },
    {
      "epoch": 2.4785956267323686,
      "grad_norm": 0.14791204035282135,
      "learning_rate": 3.4817780515347506e-05,
      "loss": 0.1755,
      "step": 40240
    },
    {
      "epoch": 2.479211579919926,
      "grad_norm": 0.1656348556280136,
      "learning_rate": 3.477671696951032e-05,
      "loss": 0.176,
      "step": 40250
    },
    {
      "epoch": 2.4798275331074837,
      "grad_norm": 0.14768081903457642,
      "learning_rate": 3.473565342367314e-05,
      "loss": 0.177,
      "step": 40260
    },
    {
      "epoch": 2.4804434862950417,
      "grad_norm": 0.14625193178653717,
      "learning_rate": 3.469458987783595e-05,
      "loss": 0.1755,
      "step": 40270
    },
    {
      "epoch": 2.4810594394825993,
      "grad_norm": 0.153716579079628,
      "learning_rate": 3.465352633199877e-05,
      "loss": 0.1758,
      "step": 40280
    },
    {
      "epoch": 2.481675392670157,
      "grad_norm": 0.13245488703250885,
      "learning_rate": 3.4612462786161586e-05,
      "loss": 0.1757,
      "step": 40290
    },
    {
      "epoch": 2.482291345857715,
      "grad_norm": 0.16814479231834412,
      "learning_rate": 3.4571399240324406e-05,
      "loss": 0.1761,
      "step": 40300
    },
    {
      "epoch": 2.4829072990452725,
      "grad_norm": 0.13615728914737701,
      "learning_rate": 3.453033569448722e-05,
      "loss": 0.1777,
      "step": 40310
    },
    {
      "epoch": 2.48352325223283,
      "grad_norm": 0.15278196334838867,
      "learning_rate": 3.448927214865004e-05,
      "loss": 0.1755,
      "step": 40320
    },
    {
      "epoch": 2.484139205420388,
      "grad_norm": 0.1545032262802124,
      "learning_rate": 3.444820860281285e-05,
      "loss": 0.1781,
      "step": 40330
    },
    {
      "epoch": 2.4847551586079457,
      "grad_norm": 0.16205912828445435,
      "learning_rate": 3.440714505697567e-05,
      "loss": 0.1768,
      "step": 40340
    },
    {
      "epoch": 2.4853711117955033,
      "grad_norm": 0.1449863463640213,
      "learning_rate": 3.4366081511138486e-05,
      "loss": 0.1758,
      "step": 40350
    },
    {
      "epoch": 2.4859870649830613,
      "grad_norm": 0.14851133525371552,
      "learning_rate": 3.4325017965301306e-05,
      "loss": 0.1771,
      "step": 40360
    },
    {
      "epoch": 2.486603018170619,
      "grad_norm": 0.14695285260677338,
      "learning_rate": 3.428395441946412e-05,
      "loss": 0.1761,
      "step": 40370
    },
    {
      "epoch": 2.487218971358177,
      "grad_norm": 0.16134192049503326,
      "learning_rate": 3.424289087362694e-05,
      "loss": 0.1757,
      "step": 40380
    },
    {
      "epoch": 2.4878349245457345,
      "grad_norm": 0.14864365756511688,
      "learning_rate": 3.420182732778975e-05,
      "loss": 0.1773,
      "step": 40390
    },
    {
      "epoch": 2.488450877733292,
      "grad_norm": 0.13872835040092468,
      "learning_rate": 3.416076378195257e-05,
      "loss": 0.1757,
      "step": 40400
    },
    {
      "epoch": 2.48906683092085,
      "grad_norm": 0.15144234895706177,
      "learning_rate": 3.411970023611539e-05,
      "loss": 0.176,
      "step": 40410
    },
    {
      "epoch": 2.4896827841084077,
      "grad_norm": 0.1477377861738205,
      "learning_rate": 3.407863669027821e-05,
      "loss": 0.175,
      "step": 40420
    },
    {
      "epoch": 2.4902987372959657,
      "grad_norm": 0.13607220351696014,
      "learning_rate": 3.4037573144441025e-05,
      "loss": 0.1755,
      "step": 40430
    },
    {
      "epoch": 2.4909146904835233,
      "grad_norm": 0.15930573642253876,
      "learning_rate": 3.3996509598603846e-05,
      "loss": 0.1767,
      "step": 40440
    },
    {
      "epoch": 2.491530643671081,
      "grad_norm": 0.16170379519462585,
      "learning_rate": 3.395544605276666e-05,
      "loss": 0.1764,
      "step": 40450
    },
    {
      "epoch": 2.492146596858639,
      "grad_norm": 0.1500505656003952,
      "learning_rate": 3.391438250692948e-05,
      "loss": 0.1753,
      "step": 40460
    },
    {
      "epoch": 2.4927625500461965,
      "grad_norm": 0.1686060130596161,
      "learning_rate": 3.387331896109229e-05,
      "loss": 0.1753,
      "step": 40470
    },
    {
      "epoch": 2.493378503233754,
      "grad_norm": 0.15800893306732178,
      "learning_rate": 3.383225541525511e-05,
      "loss": 0.1745,
      "step": 40480
    },
    {
      "epoch": 2.493994456421312,
      "grad_norm": 0.14110177755355835,
      "learning_rate": 3.3791191869417925e-05,
      "loss": 0.1759,
      "step": 40490
    },
    {
      "epoch": 2.4946104096088697,
      "grad_norm": 0.14968524873256683,
      "learning_rate": 3.3750128323580745e-05,
      "loss": 0.1764,
      "step": 40500
    },
    {
      "epoch": 2.4952263627964273,
      "grad_norm": 0.15459465980529785,
      "learning_rate": 3.370906477774356e-05,
      "loss": 0.176,
      "step": 40510
    },
    {
      "epoch": 2.4958423159839853,
      "grad_norm": 0.14146925508975983,
      "learning_rate": 3.366800123190638e-05,
      "loss": 0.1768,
      "step": 40520
    },
    {
      "epoch": 2.496458269171543,
      "grad_norm": 0.16300660371780396,
      "learning_rate": 3.362693768606919e-05,
      "loss": 0.1766,
      "step": 40530
    },
    {
      "epoch": 2.4970742223591005,
      "grad_norm": 0.1397225707769394,
      "learning_rate": 3.358587414023201e-05,
      "loss": 0.1767,
      "step": 40540
    },
    {
      "epoch": 2.4976901755466585,
      "grad_norm": 0.14467960596084595,
      "learning_rate": 3.3544810594394825e-05,
      "loss": 0.1742,
      "step": 40550
    },
    {
      "epoch": 2.498306128734216,
      "grad_norm": 0.14218810200691223,
      "learning_rate": 3.3503747048557645e-05,
      "loss": 0.1754,
      "step": 40560
    },
    {
      "epoch": 2.498922081921774,
      "grad_norm": 0.15368245542049408,
      "learning_rate": 3.346268350272046e-05,
      "loss": 0.1762,
      "step": 40570
    },
    {
      "epoch": 2.4995380351093317,
      "grad_norm": 0.15384170413017273,
      "learning_rate": 3.342161995688328e-05,
      "loss": 0.175,
      "step": 40580
    },
    {
      "epoch": 2.5001539882968893,
      "grad_norm": 0.16408516466617584,
      "learning_rate": 3.338055641104609e-05,
      "loss": 0.1758,
      "step": 40590
    },
    {
      "epoch": 2.5007699414844473,
      "grad_norm": 0.146222785115242,
      "learning_rate": 3.333949286520891e-05,
      "loss": 0.1763,
      "step": 40600
    },
    {
      "epoch": 2.501385894672005,
      "grad_norm": 0.12679466605186462,
      "learning_rate": 3.3298429319371725e-05,
      "loss": 0.1776,
      "step": 40610
    },
    {
      "epoch": 2.502001847859563,
      "grad_norm": 0.15328149497509003,
      "learning_rate": 3.325736577353455e-05,
      "loss": 0.1757,
      "step": 40620
    },
    {
      "epoch": 2.5026178010471205,
      "grad_norm": 0.13134951889514923,
      "learning_rate": 3.3216302227697365e-05,
      "loss": 0.1759,
      "step": 40630
    },
    {
      "epoch": 2.503233754234678,
      "grad_norm": 0.13099443912506104,
      "learning_rate": 3.3175238681860185e-05,
      "loss": 0.1772,
      "step": 40640
    },
    {
      "epoch": 2.503849707422236,
      "grad_norm": 0.1525202989578247,
      "learning_rate": 3.3134175136023e-05,
      "loss": 0.1762,
      "step": 40650
    },
    {
      "epoch": 2.5044656606097937,
      "grad_norm": 0.1642768830060959,
      "learning_rate": 3.309311159018582e-05,
      "loss": 0.1754,
      "step": 40660
    },
    {
      "epoch": 2.5050816137973513,
      "grad_norm": 0.12157131731510162,
      "learning_rate": 3.305204804434863e-05,
      "loss": 0.1753,
      "step": 40670
    },
    {
      "epoch": 2.5056975669849093,
      "grad_norm": 0.15054535865783691,
      "learning_rate": 3.301098449851145e-05,
      "loss": 0.176,
      "step": 40680
    },
    {
      "epoch": 2.506313520172467,
      "grad_norm": 0.1436392068862915,
      "learning_rate": 3.2969920952674265e-05,
      "loss": 0.1755,
      "step": 40690
    },
    {
      "epoch": 2.5069294733600245,
      "grad_norm": 0.14985843002796173,
      "learning_rate": 3.2928857406837085e-05,
      "loss": 0.1763,
      "step": 40700
    },
    {
      "epoch": 2.5075454265475825,
      "grad_norm": 0.14487344026565552,
      "learning_rate": 3.28877938609999e-05,
      "loss": 0.1763,
      "step": 40710
    },
    {
      "epoch": 2.50816137973514,
      "grad_norm": 0.15101876854896545,
      "learning_rate": 3.284673031516272e-05,
      "loss": 0.1782,
      "step": 40720
    },
    {
      "epoch": 2.5087773329226977,
      "grad_norm": 0.14588811993598938,
      "learning_rate": 3.280566676932553e-05,
      "loss": 0.1758,
      "step": 40730
    },
    {
      "epoch": 2.5093932861102557,
      "grad_norm": 0.13763655722141266,
      "learning_rate": 3.276460322348835e-05,
      "loss": 0.177,
      "step": 40740
    },
    {
      "epoch": 2.5100092392978133,
      "grad_norm": 0.16444514691829681,
      "learning_rate": 3.2723539677651165e-05,
      "loss": 0.1758,
      "step": 40750
    },
    {
      "epoch": 2.510625192485371,
      "grad_norm": 0.20268498361110687,
      "learning_rate": 3.2682476131813985e-05,
      "loss": 0.1739,
      "step": 40760
    },
    {
      "epoch": 2.511241145672929,
      "grad_norm": 0.16266945004463196,
      "learning_rate": 3.26414125859768e-05,
      "loss": 0.1758,
      "step": 40770
    },
    {
      "epoch": 2.5118570988604865,
      "grad_norm": 0.1536179482936859,
      "learning_rate": 3.260034904013962e-05,
      "loss": 0.1752,
      "step": 40780
    },
    {
      "epoch": 2.5124730520480445,
      "grad_norm": 0.15927225351333618,
      "learning_rate": 3.255928549430243e-05,
      "loss": 0.1742,
      "step": 40790
    },
    {
      "epoch": 2.513089005235602,
      "grad_norm": 0.1383526772260666,
      "learning_rate": 3.251822194846525e-05,
      "loss": 0.1759,
      "step": 40800
    },
    {
      "epoch": 2.51370495842316,
      "grad_norm": 0.17016582190990448,
      "learning_rate": 3.2477158402628065e-05,
      "loss": 0.1779,
      "step": 40810
    },
    {
      "epoch": 2.5143209116107177,
      "grad_norm": 0.13976068794727325,
      "learning_rate": 3.2436094856790885e-05,
      "loss": 0.1755,
      "step": 40820
    },
    {
      "epoch": 2.5149368647982753,
      "grad_norm": 0.14377433061599731,
      "learning_rate": 3.2395031310953705e-05,
      "loss": 0.1765,
      "step": 40830
    },
    {
      "epoch": 2.5155528179858333,
      "grad_norm": 0.14262203872203827,
      "learning_rate": 3.2353967765116525e-05,
      "loss": 0.1756,
      "step": 40840
    },
    {
      "epoch": 2.516168771173391,
      "grad_norm": 0.14844335615634918,
      "learning_rate": 3.231290421927934e-05,
      "loss": 0.176,
      "step": 40850
    },
    {
      "epoch": 2.5167847243609485,
      "grad_norm": 0.19396725296974182,
      "learning_rate": 3.227184067344216e-05,
      "loss": 0.1759,
      "step": 40860
    },
    {
      "epoch": 2.5174006775485065,
      "grad_norm": 0.1476842314004898,
      "learning_rate": 3.223077712760497e-05,
      "loss": 0.174,
      "step": 40870
    },
    {
      "epoch": 2.518016630736064,
      "grad_norm": 0.14910393953323364,
      "learning_rate": 3.218971358176779e-05,
      "loss": 0.1746,
      "step": 40880
    },
    {
      "epoch": 2.5186325839236217,
      "grad_norm": 0.15921473503112793,
      "learning_rate": 3.2148650035930605e-05,
      "loss": 0.1768,
      "step": 40890
    },
    {
      "epoch": 2.5192485371111797,
      "grad_norm": 0.14728158712387085,
      "learning_rate": 3.2107586490093425e-05,
      "loss": 0.1755,
      "step": 40900
    },
    {
      "epoch": 2.5198644902987373,
      "grad_norm": 0.13573119044303894,
      "learning_rate": 3.206652294425624e-05,
      "loss": 0.1742,
      "step": 40910
    },
    {
      "epoch": 2.520480443486295,
      "grad_norm": 0.15595228970050812,
      "learning_rate": 3.202545939841906e-05,
      "loss": 0.1755,
      "step": 40920
    },
    {
      "epoch": 2.521096396673853,
      "grad_norm": 0.1632581502199173,
      "learning_rate": 3.198439585258187e-05,
      "loss": 0.1748,
      "step": 40930
    },
    {
      "epoch": 2.5217123498614105,
      "grad_norm": 0.17036792635917664,
      "learning_rate": 3.194333230674469e-05,
      "loss": 0.1762,
      "step": 40940
    },
    {
      "epoch": 2.522328303048968,
      "grad_norm": 0.15645867586135864,
      "learning_rate": 3.1902268760907505e-05,
      "loss": 0.1764,
      "step": 40950
    },
    {
      "epoch": 2.522944256236526,
      "grad_norm": 0.16655342280864716,
      "learning_rate": 3.1861205215070325e-05,
      "loss": 0.1744,
      "step": 40960
    },
    {
      "epoch": 2.5235602094240837,
      "grad_norm": 0.15894827246665955,
      "learning_rate": 3.182014166923314e-05,
      "loss": 0.1756,
      "step": 40970
    },
    {
      "epoch": 2.5241761626116412,
      "grad_norm": 0.1771809458732605,
      "learning_rate": 3.177907812339596e-05,
      "loss": 0.1758,
      "step": 40980
    },
    {
      "epoch": 2.5247921157991993,
      "grad_norm": 0.16430877149105072,
      "learning_rate": 3.173801457755877e-05,
      "loss": 0.1765,
      "step": 40990
    },
    {
      "epoch": 2.525408068986757,
      "grad_norm": 0.14404602348804474,
      "learning_rate": 3.169695103172159e-05,
      "loss": 0.1744,
      "step": 41000
    },
    {
      "epoch": 2.526024022174315,
      "grad_norm": 0.1434137523174286,
      "learning_rate": 3.1655887485884405e-05,
      "loss": 0.1754,
      "step": 41010
    },
    {
      "epoch": 2.5266399753618725,
      "grad_norm": 0.1373824030160904,
      "learning_rate": 3.1614823940047225e-05,
      "loss": 0.1763,
      "step": 41020
    },
    {
      "epoch": 2.5272559285494305,
      "grad_norm": 0.15774382650852203,
      "learning_rate": 3.157376039421004e-05,
      "loss": 0.1763,
      "step": 41030
    },
    {
      "epoch": 2.527871881736988,
      "grad_norm": 0.18951648473739624,
      "learning_rate": 3.153269684837286e-05,
      "loss": 0.175,
      "step": 41040
    },
    {
      "epoch": 2.5284878349245457,
      "grad_norm": 0.17341628670692444,
      "learning_rate": 3.149163330253568e-05,
      "loss": 0.1759,
      "step": 41050
    },
    {
      "epoch": 2.5291037881121037,
      "grad_norm": 0.16110937297344208,
      "learning_rate": 3.145056975669849e-05,
      "loss": 0.176,
      "step": 41060
    },
    {
      "epoch": 2.5297197412996613,
      "grad_norm": 0.1530369371175766,
      "learning_rate": 3.140950621086131e-05,
      "loss": 0.1771,
      "step": 41070
    },
    {
      "epoch": 2.530335694487219,
      "grad_norm": 0.16920705139636993,
      "learning_rate": 3.136844266502413e-05,
      "loss": 0.177,
      "step": 41080
    },
    {
      "epoch": 2.530951647674777,
      "grad_norm": 0.1495298445224762,
      "learning_rate": 3.1327379119186945e-05,
      "loss": 0.1762,
      "step": 41090
    },
    {
      "epoch": 2.5315676008623345,
      "grad_norm": 0.1289730668067932,
      "learning_rate": 3.1286315573349765e-05,
      "loss": 0.1748,
      "step": 41100
    },
    {
      "epoch": 2.532183554049892,
      "grad_norm": 0.1465141624212265,
      "learning_rate": 3.124525202751258e-05,
      "loss": 0.1757,
      "step": 41110
    },
    {
      "epoch": 2.53279950723745,
      "grad_norm": 0.15065637230873108,
      "learning_rate": 3.12041884816754e-05,
      "loss": 0.1759,
      "step": 41120
    },
    {
      "epoch": 2.5334154604250076,
      "grad_norm": 0.1401273012161255,
      "learning_rate": 3.116312493583821e-05,
      "loss": 0.175,
      "step": 41130
    },
    {
      "epoch": 2.5340314136125652,
      "grad_norm": 0.1545286476612091,
      "learning_rate": 3.112206139000103e-05,
      "loss": 0.1768,
      "step": 41140
    },
    {
      "epoch": 2.5346473668001233,
      "grad_norm": 0.16657869517803192,
      "learning_rate": 3.1080997844163845e-05,
      "loss": 0.1768,
      "step": 41150
    },
    {
      "epoch": 2.535263319987681,
      "grad_norm": 0.14430034160614014,
      "learning_rate": 3.1039934298326665e-05,
      "loss": 0.1751,
      "step": 41160
    },
    {
      "epoch": 2.5358792731752384,
      "grad_norm": 0.15152141451835632,
      "learning_rate": 3.099887075248948e-05,
      "loss": 0.1757,
      "step": 41170
    },
    {
      "epoch": 2.5364952263627965,
      "grad_norm": 0.14606411755084991,
      "learning_rate": 3.09578072066523e-05,
      "loss": 0.1764,
      "step": 41180
    },
    {
      "epoch": 2.537111179550354,
      "grad_norm": 0.12543509900569916,
      "learning_rate": 3.091674366081511e-05,
      "loss": 0.1759,
      "step": 41190
    },
    {
      "epoch": 2.537727132737912,
      "grad_norm": 0.1253005713224411,
      "learning_rate": 3.087568011497793e-05,
      "loss": 0.1756,
      "step": 41200
    },
    {
      "epoch": 2.5383430859254696,
      "grad_norm": 0.14696544408798218,
      "learning_rate": 3.0834616569140744e-05,
      "loss": 0.1754,
      "step": 41210
    },
    {
      "epoch": 2.5389590391130277,
      "grad_norm": 0.14690852165222168,
      "learning_rate": 3.0793553023303565e-05,
      "loss": 0.1758,
      "step": 41220
    },
    {
      "epoch": 2.5395749923005853,
      "grad_norm": 0.13576669991016388,
      "learning_rate": 3.075248947746638e-05,
      "loss": 0.1746,
      "step": 41230
    },
    {
      "epoch": 2.540190945488143,
      "grad_norm": 0.15036052465438843,
      "learning_rate": 3.07114259316292e-05,
      "loss": 0.1757,
      "step": 41240
    },
    {
      "epoch": 2.540806898675701,
      "grad_norm": 0.13617931306362152,
      "learning_rate": 3.067036238579201e-05,
      "loss": 0.1756,
      "step": 41250
    },
    {
      "epoch": 2.5414228518632584,
      "grad_norm": 0.1460442692041397,
      "learning_rate": 3.062929883995483e-05,
      "loss": 0.176,
      "step": 41260
    },
    {
      "epoch": 2.542038805050816,
      "grad_norm": 0.13258549571037292,
      "learning_rate": 3.058823529411765e-05,
      "loss": 0.1759,
      "step": 41270
    },
    {
      "epoch": 2.542654758238374,
      "grad_norm": 0.13955998420715332,
      "learning_rate": 3.0547171748280464e-05,
      "loss": 0.1761,
      "step": 41280
    },
    {
      "epoch": 2.5432707114259316,
      "grad_norm": 0.17448289692401886,
      "learning_rate": 3.0506108202443284e-05,
      "loss": 0.1768,
      "step": 41290
    },
    {
      "epoch": 2.5438866646134892,
      "grad_norm": 0.14866799116134644,
      "learning_rate": 3.04650446566061e-05,
      "loss": 0.1756,
      "step": 41300
    },
    {
      "epoch": 2.5445026178010473,
      "grad_norm": 0.15017719566822052,
      "learning_rate": 3.0423981110768918e-05,
      "loss": 0.1755,
      "step": 41310
    },
    {
      "epoch": 2.545118570988605,
      "grad_norm": 0.13732491433620453,
      "learning_rate": 3.0382917564931734e-05,
      "loss": 0.1766,
      "step": 41320
    },
    {
      "epoch": 2.5457345241761624,
      "grad_norm": 0.1485985666513443,
      "learning_rate": 3.034185401909455e-05,
      "loss": 0.1756,
      "step": 41330
    },
    {
      "epoch": 2.5463504773637204,
      "grad_norm": 0.13132160902023315,
      "learning_rate": 3.0300790473257368e-05,
      "loss": 0.1746,
      "step": 41340
    },
    {
      "epoch": 2.546966430551278,
      "grad_norm": 0.168666809797287,
      "learning_rate": 3.0259726927420184e-05,
      "loss": 0.1748,
      "step": 41350
    },
    {
      "epoch": 2.5475823837388356,
      "grad_norm": 0.13975457847118378,
      "learning_rate": 3.0218663381583e-05,
      "loss": 0.1754,
      "step": 41360
    },
    {
      "epoch": 2.5481983369263936,
      "grad_norm": 0.15240360796451569,
      "learning_rate": 3.0177599835745818e-05,
      "loss": 0.1761,
      "step": 41370
    },
    {
      "epoch": 2.548814290113951,
      "grad_norm": 0.13484129309654236,
      "learning_rate": 3.0136536289908634e-05,
      "loss": 0.1745,
      "step": 41380
    },
    {
      "epoch": 2.5494302433015092,
      "grad_norm": 0.16299839317798615,
      "learning_rate": 3.009547274407145e-05,
      "loss": 0.1754,
      "step": 41390
    },
    {
      "epoch": 2.550046196489067,
      "grad_norm": 0.13692393898963928,
      "learning_rate": 3.0054409198234268e-05,
      "loss": 0.1742,
      "step": 41400
    },
    {
      "epoch": 2.5506621496766244,
      "grad_norm": 0.1309732049703598,
      "learning_rate": 3.0013345652397084e-05,
      "loss": 0.1745,
      "step": 41410
    },
    {
      "epoch": 2.5512781028641824,
      "grad_norm": 0.15497522056102753,
      "learning_rate": 2.99722821065599e-05,
      "loss": 0.1771,
      "step": 41420
    },
    {
      "epoch": 2.55189405605174,
      "grad_norm": 0.14547386765480042,
      "learning_rate": 2.9931218560722718e-05,
      "loss": 0.1746,
      "step": 41430
    },
    {
      "epoch": 2.552510009239298,
      "grad_norm": 0.16773398220539093,
      "learning_rate": 2.9890155014885534e-05,
      "loss": 0.1758,
      "step": 41440
    },
    {
      "epoch": 2.5531259624268556,
      "grad_norm": 0.15007895231246948,
      "learning_rate": 2.984909146904835e-05,
      "loss": 0.1748,
      "step": 41450
    },
    {
      "epoch": 2.553741915614413,
      "grad_norm": 0.15682925283908844,
      "learning_rate": 2.9808027923211167e-05,
      "loss": 0.1761,
      "step": 41460
    },
    {
      "epoch": 2.5543578688019712,
      "grad_norm": 0.16970649361610413,
      "learning_rate": 2.976696437737399e-05,
      "loss": 0.1762,
      "step": 41470
    },
    {
      "epoch": 2.554973821989529,
      "grad_norm": 0.14222602546215057,
      "learning_rate": 2.9725900831536808e-05,
      "loss": 0.1763,
      "step": 41480
    },
    {
      "epoch": 2.5555897751770864,
      "grad_norm": 0.14576740562915802,
      "learning_rate": 2.9684837285699624e-05,
      "loss": 0.1764,
      "step": 41490
    },
    {
      "epoch": 2.5562057283646444,
      "grad_norm": 0.13082504272460938,
      "learning_rate": 2.964377373986244e-05,
      "loss": 0.1748,
      "step": 41500
    },
    {
      "epoch": 2.556821681552202,
      "grad_norm": 0.15245021879673004,
      "learning_rate": 2.9602710194025258e-05,
      "loss": 0.1762,
      "step": 41510
    },
    {
      "epoch": 2.5574376347397596,
      "grad_norm": 0.14186377823352814,
      "learning_rate": 2.9561646648188074e-05,
      "loss": 0.1734,
      "step": 41520
    },
    {
      "epoch": 2.5580535879273176,
      "grad_norm": 0.15916839241981506,
      "learning_rate": 2.952058310235089e-05,
      "loss": 0.1764,
      "step": 41530
    },
    {
      "epoch": 2.558669541114875,
      "grad_norm": 0.14999942481517792,
      "learning_rate": 2.9479519556513707e-05,
      "loss": 0.1756,
      "step": 41540
    },
    {
      "epoch": 2.559285494302433,
      "grad_norm": 0.15319883823394775,
      "learning_rate": 2.9438456010676524e-05,
      "loss": 0.1756,
      "step": 41550
    },
    {
      "epoch": 2.559901447489991,
      "grad_norm": 0.1404428482055664,
      "learning_rate": 2.939739246483934e-05,
      "loss": 0.1758,
      "step": 41560
    },
    {
      "epoch": 2.5605174006775484,
      "grad_norm": 0.14485128223896027,
      "learning_rate": 2.9356328919002157e-05,
      "loss": 0.1773,
      "step": 41570
    },
    {
      "epoch": 2.561133353865106,
      "grad_norm": 0.1651362031698227,
      "learning_rate": 2.9315265373164974e-05,
      "loss": 0.1766,
      "step": 41580
    },
    {
      "epoch": 2.561749307052664,
      "grad_norm": 0.1535443663597107,
      "learning_rate": 2.927420182732779e-05,
      "loss": 0.1748,
      "step": 41590
    },
    {
      "epoch": 2.5623652602402216,
      "grad_norm": 0.14102689921855927,
      "learning_rate": 2.9233138281490607e-05,
      "loss": 0.1768,
      "step": 41600
    },
    {
      "epoch": 2.5629812134277796,
      "grad_norm": 0.15476422011852264,
      "learning_rate": 2.9192074735653424e-05,
      "loss": 0.1753,
      "step": 41610
    },
    {
      "epoch": 2.563597166615337,
      "grad_norm": 0.15786874294281006,
      "learning_rate": 2.915101118981624e-05,
      "loss": 0.1753,
      "step": 41620
    },
    {
      "epoch": 2.5642131198028952,
      "grad_norm": 0.1571197211742401,
      "learning_rate": 2.911405399856278e-05,
      "loss": 0.1759,
      "step": 41630
    },
    {
      "epoch": 2.564829072990453,
      "grad_norm": 0.1485919952392578,
      "learning_rate": 2.9072990452725595e-05,
      "loss": 0.1747,
      "step": 41640
    },
    {
      "epoch": 2.5654450261780104,
      "grad_norm": 0.17490841448307037,
      "learning_rate": 2.9031926906888412e-05,
      "loss": 0.1762,
      "step": 41650
    },
    {
      "epoch": 2.5660609793655684,
      "grad_norm": 0.1616678684949875,
      "learning_rate": 2.899086336105123e-05,
      "loss": 0.1761,
      "step": 41660
    },
    {
      "epoch": 2.566676932553126,
      "grad_norm": 0.14184771478176117,
      "learning_rate": 2.8949799815214045e-05,
      "loss": 0.1758,
      "step": 41670
    },
    {
      "epoch": 2.5672928857406836,
      "grad_norm": 0.15048480033874512,
      "learning_rate": 2.8908736269376862e-05,
      "loss": 0.1746,
      "step": 41680
    },
    {
      "epoch": 2.5679088389282416,
      "grad_norm": 0.14080636203289032,
      "learning_rate": 2.886767272353968e-05,
      "loss": 0.1766,
      "step": 41690
    },
    {
      "epoch": 2.568524792115799,
      "grad_norm": 0.15852469205856323,
      "learning_rate": 2.8826609177702495e-05,
      "loss": 0.1777,
      "step": 41700
    },
    {
      "epoch": 2.569140745303357,
      "grad_norm": 0.16853025555610657,
      "learning_rate": 2.8785545631865312e-05,
      "loss": 0.1744,
      "step": 41710
    },
    {
      "epoch": 2.569756698490915,
      "grad_norm": 0.14884422719478607,
      "learning_rate": 2.874448208602813e-05,
      "loss": 0.1749,
      "step": 41720
    },
    {
      "epoch": 2.5703726516784724,
      "grad_norm": 0.15907925367355347,
      "learning_rate": 2.8703418540190945e-05,
      "loss": 0.1757,
      "step": 41730
    },
    {
      "epoch": 2.57098860486603,
      "grad_norm": 0.14350809156894684,
      "learning_rate": 2.8662354994353762e-05,
      "loss": 0.1753,
      "step": 41740
    },
    {
      "epoch": 2.571604558053588,
      "grad_norm": 0.16260913014411926,
      "learning_rate": 2.862129144851658e-05,
      "loss": 0.1753,
      "step": 41750
    },
    {
      "epoch": 2.5722205112411456,
      "grad_norm": 0.14429868757724762,
      "learning_rate": 2.8580227902679395e-05,
      "loss": 0.1752,
      "step": 41760
    },
    {
      "epoch": 2.572836464428703,
      "grad_norm": 0.1432715505361557,
      "learning_rate": 2.8539164356842212e-05,
      "loss": 0.1757,
      "step": 41770
    },
    {
      "epoch": 2.573452417616261,
      "grad_norm": 0.13806170225143433,
      "learning_rate": 2.849810081100503e-05,
      "loss": 0.1755,
      "step": 41780
    },
    {
      "epoch": 2.574068370803819,
      "grad_norm": 0.13830916583538055,
      "learning_rate": 2.8457037265167845e-05,
      "loss": 0.1759,
      "step": 41790
    },
    {
      "epoch": 2.574684323991377,
      "grad_norm": 0.14786416292190552,
      "learning_rate": 2.8415973719330662e-05,
      "loss": 0.1753,
      "step": 41800
    },
    {
      "epoch": 2.5753002771789344,
      "grad_norm": 0.1523970663547516,
      "learning_rate": 2.8374910173493485e-05,
      "loss": 0.1757,
      "step": 41810
    },
    {
      "epoch": 2.5759162303664924,
      "grad_norm": 0.1396910697221756,
      "learning_rate": 2.8333846627656302e-05,
      "loss": 0.1747,
      "step": 41820
    },
    {
      "epoch": 2.57653218355405,
      "grad_norm": 0.143763929605484,
      "learning_rate": 2.829278308181912e-05,
      "loss": 0.1743,
      "step": 41830
    },
    {
      "epoch": 2.5771481367416076,
      "grad_norm": 0.13686026632785797,
      "learning_rate": 2.8251719535981935e-05,
      "loss": 0.1754,
      "step": 41840
    },
    {
      "epoch": 2.5777640899291656,
      "grad_norm": 0.14088939130306244,
      "learning_rate": 2.8210655990144752e-05,
      "loss": 0.1752,
      "step": 41850
    },
    {
      "epoch": 2.578380043116723,
      "grad_norm": 0.16858036816120148,
      "learning_rate": 2.816959244430757e-05,
      "loss": 0.1751,
      "step": 41860
    },
    {
      "epoch": 2.578995996304281,
      "grad_norm": 0.1500556766986847,
      "learning_rate": 2.8128528898470385e-05,
      "loss": 0.1757,
      "step": 41870
    },
    {
      "epoch": 2.579611949491839,
      "grad_norm": 0.15181764960289001,
      "learning_rate": 2.8087465352633202e-05,
      "loss": 0.1741,
      "step": 41880
    },
    {
      "epoch": 2.5802279026793964,
      "grad_norm": 0.13613757491111755,
      "learning_rate": 2.804640180679602e-05,
      "loss": 0.1746,
      "step": 41890
    },
    {
      "epoch": 2.580843855866954,
      "grad_norm": 0.1535746306180954,
      "learning_rate": 2.8005338260958835e-05,
      "loss": 0.1765,
      "step": 41900
    },
    {
      "epoch": 2.581459809054512,
      "grad_norm": 0.15207940340042114,
      "learning_rate": 2.796427471512165e-05,
      "loss": 0.1768,
      "step": 41910
    },
    {
      "epoch": 2.5820757622420696,
      "grad_norm": 0.14584679901599884,
      "learning_rate": 2.792321116928447e-05,
      "loss": 0.1751,
      "step": 41920
    },
    {
      "epoch": 2.582691715429627,
      "grad_norm": 0.1345582753419876,
      "learning_rate": 2.7882147623447285e-05,
      "loss": 0.1745,
      "step": 41930
    },
    {
      "epoch": 2.583307668617185,
      "grad_norm": 0.13763844966888428,
      "learning_rate": 2.78410840776101e-05,
      "loss": 0.1768,
      "step": 41940
    },
    {
      "epoch": 2.5839236218047428,
      "grad_norm": 0.14543834328651428,
      "learning_rate": 2.7800020531772918e-05,
      "loss": 0.1754,
      "step": 41950
    },
    {
      "epoch": 2.5845395749923004,
      "grad_norm": 0.15472255647182465,
      "learning_rate": 2.7758956985935735e-05,
      "loss": 0.175,
      "step": 41960
    },
    {
      "epoch": 2.5851555281798584,
      "grad_norm": 0.14460889995098114,
      "learning_rate": 2.771789344009855e-05,
      "loss": 0.1761,
      "step": 41970
    },
    {
      "epoch": 2.585771481367416,
      "grad_norm": 0.14344362914562225,
      "learning_rate": 2.7676829894261368e-05,
      "loss": 0.1765,
      "step": 41980
    },
    {
      "epoch": 2.5863874345549736,
      "grad_norm": 0.1488424837589264,
      "learning_rate": 2.7635766348424185e-05,
      "loss": 0.175,
      "step": 41990
    },
    {
      "epoch": 2.5870033877425316,
      "grad_norm": 0.15635454654693604,
      "learning_rate": 2.7594702802587e-05,
      "loss": 0.1762,
      "step": 42000
    },
    {
      "epoch": 2.587619340930089,
      "grad_norm": 0.1718529313802719,
      "learning_rate": 2.7553639256749818e-05,
      "loss": 0.1759,
      "step": 42010
    },
    {
      "epoch": 2.588235294117647,
      "grad_norm": 0.16778868436813354,
      "learning_rate": 2.7512575710912635e-05,
      "loss": 0.1763,
      "step": 42020
    },
    {
      "epoch": 2.5888512473052048,
      "grad_norm": 0.18536171317100525,
      "learning_rate": 2.7471512165075458e-05,
      "loss": 0.1763,
      "step": 42030
    },
    {
      "epoch": 2.589467200492763,
      "grad_norm": 0.15861769020557404,
      "learning_rate": 2.7430448619238275e-05,
      "loss": 0.1747,
      "step": 42040
    },
    {
      "epoch": 2.5900831536803204,
      "grad_norm": 0.1428586095571518,
      "learning_rate": 2.738938507340109e-05,
      "loss": 0.174,
      "step": 42050
    },
    {
      "epoch": 2.590699106867878,
      "grad_norm": 0.14762593805789948,
      "learning_rate": 2.7348321527563908e-05,
      "loss": 0.1757,
      "step": 42060
    },
    {
      "epoch": 2.591315060055436,
      "grad_norm": 0.13110890984535217,
      "learning_rate": 2.7307257981726725e-05,
      "loss": 0.1763,
      "step": 42070
    },
    {
      "epoch": 2.5919310132429936,
      "grad_norm": 0.16411028802394867,
      "learning_rate": 2.726619443588954e-05,
      "loss": 0.1754,
      "step": 42080
    },
    {
      "epoch": 2.592546966430551,
      "grad_norm": 0.17528414726257324,
      "learning_rate": 2.7225130890052358e-05,
      "loss": 0.1756,
      "step": 42090
    },
    {
      "epoch": 2.593162919618109,
      "grad_norm": 0.15380923449993134,
      "learning_rate": 2.7184067344215175e-05,
      "loss": 0.1764,
      "step": 42100
    },
    {
      "epoch": 2.5937788728056668,
      "grad_norm": 0.15164168179035187,
      "learning_rate": 2.714300379837799e-05,
      "loss": 0.1753,
      "step": 42110
    },
    {
      "epoch": 2.5943948259932244,
      "grad_norm": 0.15825295448303223,
      "learning_rate": 2.7101940252540808e-05,
      "loss": 0.1768,
      "step": 42120
    },
    {
      "epoch": 2.5950107791807824,
      "grad_norm": 0.1701578050851822,
      "learning_rate": 2.7060876706703625e-05,
      "loss": 0.1769,
      "step": 42130
    },
    {
      "epoch": 2.59562673236834,
      "grad_norm": 0.15994024276733398,
      "learning_rate": 2.701981316086644e-05,
      "loss": 0.1756,
      "step": 42140
    },
    {
      "epoch": 2.5962426855558975,
      "grad_norm": 0.15241639316082,
      "learning_rate": 2.6978749615029258e-05,
      "loss": 0.1751,
      "step": 42150
    },
    {
      "epoch": 2.5968586387434556,
      "grad_norm": 0.14386512339115143,
      "learning_rate": 2.6937686069192075e-05,
      "loss": 0.1752,
      "step": 42160
    },
    {
      "epoch": 2.597474591931013,
      "grad_norm": 0.16244825720787048,
      "learning_rate": 2.689662252335489e-05,
      "loss": 0.175,
      "step": 42170
    },
    {
      "epoch": 2.5980905451185707,
      "grad_norm": 0.1529383361339569,
      "learning_rate": 2.6855558977517708e-05,
      "loss": 0.1745,
      "step": 42180
    },
    {
      "epoch": 2.5987064983061288,
      "grad_norm": 0.14683012664318085,
      "learning_rate": 2.6814495431680525e-05,
      "loss": 0.1752,
      "step": 42190
    },
    {
      "epoch": 2.5993224514936863,
      "grad_norm": 0.16638684272766113,
      "learning_rate": 2.677343188584334e-05,
      "loss": 0.1768,
      "step": 42200
    },
    {
      "epoch": 2.5999384046812444,
      "grad_norm": 0.13797897100448608,
      "learning_rate": 2.6732368340006158e-05,
      "loss": 0.1755,
      "step": 42210
    },
    {
      "epoch": 2.600554357868802,
      "grad_norm": 0.1459871083498001,
      "learning_rate": 2.6691304794168975e-05,
      "loss": 0.1751,
      "step": 42220
    },
    {
      "epoch": 2.60117031105636,
      "grad_norm": 0.15090277791023254,
      "learning_rate": 2.665024124833179e-05,
      "loss": 0.175,
      "step": 42230
    },
    {
      "epoch": 2.6017862642439176,
      "grad_norm": 0.15414240956306458,
      "learning_rate": 2.6609177702494615e-05,
      "loss": 0.1754,
      "step": 42240
    },
    {
      "epoch": 2.602402217431475,
      "grad_norm": 0.15013514459133148,
      "learning_rate": 2.656811415665743e-05,
      "loss": 0.1761,
      "step": 42250
    },
    {
      "epoch": 2.603018170619033,
      "grad_norm": 0.15233944356441498,
      "learning_rate": 2.6527050610820248e-05,
      "loss": 0.1729,
      "step": 42260
    },
    {
      "epoch": 2.6036341238065908,
      "grad_norm": 0.1666223406791687,
      "learning_rate": 2.6485987064983065e-05,
      "loss": 0.1745,
      "step": 42270
    },
    {
      "epoch": 2.6042500769941483,
      "grad_norm": 0.1582709103822708,
      "learning_rate": 2.644492351914588e-05,
      "loss": 0.175,
      "step": 42280
    },
    {
      "epoch": 2.6048660301817064,
      "grad_norm": 0.15343691408634186,
      "learning_rate": 2.6403859973308698e-05,
      "loss": 0.1753,
      "step": 42290
    },
    {
      "epoch": 2.605481983369264,
      "grad_norm": 0.14933805167675018,
      "learning_rate": 2.6362796427471515e-05,
      "loss": 0.1748,
      "step": 42300
    },
    {
      "epoch": 2.6060979365568215,
      "grad_norm": 0.14791768789291382,
      "learning_rate": 2.632173288163433e-05,
      "loss": 0.1769,
      "step": 42310
    },
    {
      "epoch": 2.6067138897443796,
      "grad_norm": 0.1640223115682602,
      "learning_rate": 2.6280669335797148e-05,
      "loss": 0.1766,
      "step": 42320
    },
    {
      "epoch": 2.607329842931937,
      "grad_norm": 0.13968738913536072,
      "learning_rate": 2.6239605789959964e-05,
      "loss": 0.1754,
      "step": 42330
    },
    {
      "epoch": 2.6079457961194947,
      "grad_norm": 0.14138345420360565,
      "learning_rate": 2.619854224412278e-05,
      "loss": 0.1749,
      "step": 42340
    },
    {
      "epoch": 2.6085617493070528,
      "grad_norm": 0.14886009693145752,
      "learning_rate": 2.6157478698285598e-05,
      "loss": 0.1769,
      "step": 42350
    },
    {
      "epoch": 2.6091777024946103,
      "grad_norm": 0.13907434046268463,
      "learning_rate": 2.6116415152448414e-05,
      "loss": 0.1759,
      "step": 42360
    },
    {
      "epoch": 2.609793655682168,
      "grad_norm": 0.13840757310390472,
      "learning_rate": 2.607535160661123e-05,
      "loss": 0.175,
      "step": 42370
    },
    {
      "epoch": 2.610409608869726,
      "grad_norm": 0.1508616954088211,
      "learning_rate": 2.6034288060774048e-05,
      "loss": 0.1742,
      "step": 42380
    },
    {
      "epoch": 2.6110255620572835,
      "grad_norm": 0.15243148803710938,
      "learning_rate": 2.5993224514936864e-05,
      "loss": 0.1754,
      "step": 42390
    },
    {
      "epoch": 2.611641515244841,
      "grad_norm": 0.13993816077709198,
      "learning_rate": 2.595216096909968e-05,
      "loss": 0.1766,
      "step": 42400
    },
    {
      "epoch": 2.612257468432399,
      "grad_norm": 0.14332477748394012,
      "learning_rate": 2.5911097423262498e-05,
      "loss": 0.1762,
      "step": 42410
    },
    {
      "epoch": 2.6128734216199567,
      "grad_norm": 0.16341793537139893,
      "learning_rate": 2.5870033877425314e-05,
      "loss": 0.1759,
      "step": 42420
    },
    {
      "epoch": 2.6134893748075148,
      "grad_norm": 0.1504521369934082,
      "learning_rate": 2.582897033158813e-05,
      "loss": 0.1737,
      "step": 42430
    },
    {
      "epoch": 2.6141053279950723,
      "grad_norm": 0.1528588831424713,
      "learning_rate": 2.5787906785750948e-05,
      "loss": 0.1751,
      "step": 42440
    },
    {
      "epoch": 2.6147212811826304,
      "grad_norm": 0.16501642763614655,
      "learning_rate": 2.574684323991377e-05,
      "loss": 0.1753,
      "step": 42450
    },
    {
      "epoch": 2.615337234370188,
      "grad_norm": 0.16566409170627594,
      "learning_rate": 2.5705779694076588e-05,
      "loss": 0.1753,
      "step": 42460
    },
    {
      "epoch": 2.6159531875577455,
      "grad_norm": 0.14038200676441193,
      "learning_rate": 2.5664716148239404e-05,
      "loss": 0.174,
      "step": 42470
    },
    {
      "epoch": 2.6165691407453036,
      "grad_norm": 0.15526029467582703,
      "learning_rate": 2.562365260240222e-05,
      "loss": 0.1765,
      "step": 42480
    },
    {
      "epoch": 2.617185093932861,
      "grad_norm": 0.15704618394374847,
      "learning_rate": 2.5582589056565038e-05,
      "loss": 0.1765,
      "step": 42490
    },
    {
      "epoch": 2.6178010471204187,
      "grad_norm": 0.14674320816993713,
      "learning_rate": 2.5541525510727854e-05,
      "loss": 0.1759,
      "step": 42500
    },
    {
      "epoch": 2.6184170003079767,
      "grad_norm": 0.1569371223449707,
      "learning_rate": 2.550046196489067e-05,
      "loss": 0.1741,
      "step": 42510
    },
    {
      "epoch": 2.6190329534955343,
      "grad_norm": 0.1662636399269104,
      "learning_rate": 2.5459398419053488e-05,
      "loss": 0.1765,
      "step": 42520
    },
    {
      "epoch": 2.619648906683092,
      "grad_norm": 0.12976273894309998,
      "learning_rate": 2.5418334873216304e-05,
      "loss": 0.1761,
      "step": 42530
    },
    {
      "epoch": 2.62026485987065,
      "grad_norm": 0.15831834077835083,
      "learning_rate": 2.537727132737912e-05,
      "loss": 0.1758,
      "step": 42540
    },
    {
      "epoch": 2.6208808130582075,
      "grad_norm": 0.16249780356884003,
      "learning_rate": 2.5336207781541938e-05,
      "loss": 0.1772,
      "step": 42550
    },
    {
      "epoch": 2.621496766245765,
      "grad_norm": 0.16172340512275696,
      "learning_rate": 2.5295144235704754e-05,
      "loss": 0.1758,
      "step": 42560
    },
    {
      "epoch": 2.622112719433323,
      "grad_norm": 0.14410260319709778,
      "learning_rate": 2.525408068986757e-05,
      "loss": 0.1744,
      "step": 42570
    },
    {
      "epoch": 2.6227286726208807,
      "grad_norm": 0.15047042071819305,
      "learning_rate": 2.5213017144030387e-05,
      "loss": 0.1751,
      "step": 42580
    },
    {
      "epoch": 2.6233446258084383,
      "grad_norm": 0.17512880265712738,
      "learning_rate": 2.5171953598193204e-05,
      "loss": 0.1751,
      "step": 42590
    },
    {
      "epoch": 2.6239605789959963,
      "grad_norm": 0.14688238501548767,
      "learning_rate": 2.513089005235602e-05,
      "loss": 0.1749,
      "step": 42600
    },
    {
      "epoch": 2.624576532183554,
      "grad_norm": 0.14807158708572388,
      "learning_rate": 2.5089826506518837e-05,
      "loss": 0.1753,
      "step": 42610
    },
    {
      "epoch": 2.625192485371112,
      "grad_norm": 0.15916024148464203,
      "learning_rate": 2.5048762960681654e-05,
      "loss": 0.1757,
      "step": 42620
    },
    {
      "epoch": 2.6258084385586695,
      "grad_norm": 0.1573839634656906,
      "learning_rate": 2.500769941484447e-05,
      "loss": 0.1746,
      "step": 42630
    },
    {
      "epoch": 2.6264243917462275,
      "grad_norm": 0.1701909750699997,
      "learning_rate": 2.496663586900729e-05,
      "loss": 0.1772,
      "step": 42640
    },
    {
      "epoch": 2.627040344933785,
      "grad_norm": 0.15817435085773468,
      "learning_rate": 2.4925572323170107e-05,
      "loss": 0.1751,
      "step": 42650
    },
    {
      "epoch": 2.6276562981213427,
      "grad_norm": 0.17114031314849854,
      "learning_rate": 2.4884508777332924e-05,
      "loss": 0.1765,
      "step": 42660
    },
    {
      "epoch": 2.6282722513089007,
      "grad_norm": 0.14075815677642822,
      "learning_rate": 2.484344523149574e-05,
      "loss": 0.1765,
      "step": 42670
    },
    {
      "epoch": 2.6288882044964583,
      "grad_norm": 0.14405705034732819,
      "learning_rate": 2.4802381685658557e-05,
      "loss": 0.1745,
      "step": 42680
    },
    {
      "epoch": 2.629504157684016,
      "grad_norm": 0.16223257780075073,
      "learning_rate": 2.4761318139821374e-05,
      "loss": 0.1781,
      "step": 42690
    },
    {
      "epoch": 2.630120110871574,
      "grad_norm": 0.1503075808286667,
      "learning_rate": 2.472025459398419e-05,
      "loss": 0.1771,
      "step": 42700
    },
    {
      "epoch": 2.6307360640591315,
      "grad_norm": 0.14352445304393768,
      "learning_rate": 2.4679191048147007e-05,
      "loss": 0.1758,
      "step": 42710
    },
    {
      "epoch": 2.631352017246689,
      "grad_norm": 0.14439724385738373,
      "learning_rate": 2.4638127502309827e-05,
      "loss": 0.1761,
      "step": 42720
    },
    {
      "epoch": 2.631967970434247,
      "grad_norm": 0.15525662899017334,
      "learning_rate": 2.4597063956472644e-05,
      "loss": 0.1755,
      "step": 42730
    },
    {
      "epoch": 2.6325839236218047,
      "grad_norm": 0.14755088090896606,
      "learning_rate": 2.455600041063546e-05,
      "loss": 0.1738,
      "step": 42740
    },
    {
      "epoch": 2.6331998768093623,
      "grad_norm": 0.14189723134040833,
      "learning_rate": 2.4514936864798277e-05,
      "loss": 0.1781,
      "step": 42750
    },
    {
      "epoch": 2.6338158299969203,
      "grad_norm": 0.15757307410240173,
      "learning_rate": 2.4473873318961094e-05,
      "loss": 0.1764,
      "step": 42760
    },
    {
      "epoch": 2.634431783184478,
      "grad_norm": 0.14834384620189667,
      "learning_rate": 2.443280977312391e-05,
      "loss": 0.1745,
      "step": 42770
    },
    {
      "epoch": 2.6350477363720355,
      "grad_norm": 0.13788138329982758,
      "learning_rate": 2.4391746227286727e-05,
      "loss": 0.1751,
      "step": 42780
    },
    {
      "epoch": 2.6356636895595935,
      "grad_norm": 0.14636187255382538,
      "learning_rate": 2.4350682681449544e-05,
      "loss": 0.1751,
      "step": 42790
    },
    {
      "epoch": 2.636279642747151,
      "grad_norm": 0.17645388841629028,
      "learning_rate": 2.430961913561236e-05,
      "loss": 0.1759,
      "step": 42800
    },
    {
      "epoch": 2.636895595934709,
      "grad_norm": 0.15062354505062103,
      "learning_rate": 2.4268555589775177e-05,
      "loss": 0.1764,
      "step": 42810
    },
    {
      "epoch": 2.6375115491222667,
      "grad_norm": 0.1515788733959198,
      "learning_rate": 2.4227492043937997e-05,
      "loss": 0.1744,
      "step": 42820
    },
    {
      "epoch": 2.6381275023098243,
      "grad_norm": 0.1458786129951477,
      "learning_rate": 2.4186428498100814e-05,
      "loss": 0.1755,
      "step": 42830
    },
    {
      "epoch": 2.6387434554973823,
      "grad_norm": 0.16192029416561127,
      "learning_rate": 2.414536495226363e-05,
      "loss": 0.1758,
      "step": 42840
    },
    {
      "epoch": 2.63935940868494,
      "grad_norm": 0.13863015174865723,
      "learning_rate": 2.4104301406426447e-05,
      "loss": 0.1743,
      "step": 42850
    },
    {
      "epoch": 2.639975361872498,
      "grad_norm": 0.15818218886852264,
      "learning_rate": 2.4063237860589264e-05,
      "loss": 0.1757,
      "step": 42860
    },
    {
      "epoch": 2.6405913150600555,
      "grad_norm": 0.1390097290277481,
      "learning_rate": 2.402217431475208e-05,
      "loss": 0.1745,
      "step": 42870
    },
    {
      "epoch": 2.641207268247613,
      "grad_norm": 0.1579330712556839,
      "learning_rate": 2.3981110768914897e-05,
      "loss": 0.175,
      "step": 42880
    },
    {
      "epoch": 2.641823221435171,
      "grad_norm": 0.1524784415960312,
      "learning_rate": 2.3940047223077714e-05,
      "loss": 0.1766,
      "step": 42890
    },
    {
      "epoch": 2.6424391746227287,
      "grad_norm": 0.16199475526809692,
      "learning_rate": 2.389898367724053e-05,
      "loss": 0.1768,
      "step": 42900
    },
    {
      "epoch": 2.6430551278102863,
      "grad_norm": 0.14140315353870392,
      "learning_rate": 2.3857920131403347e-05,
      "loss": 0.1751,
      "step": 42910
    },
    {
      "epoch": 2.6436710809978443,
      "grad_norm": 0.15071389079093933,
      "learning_rate": 2.3816856585566164e-05,
      "loss": 0.1748,
      "step": 42920
    },
    {
      "epoch": 2.644287034185402,
      "grad_norm": 0.16258569061756134,
      "learning_rate": 2.3775793039728984e-05,
      "loss": 0.1737,
      "step": 42930
    },
    {
      "epoch": 2.6449029873729595,
      "grad_norm": 0.14503292739391327,
      "learning_rate": 2.37347294938918e-05,
      "loss": 0.1763,
      "step": 42940
    },
    {
      "epoch": 2.6455189405605175,
      "grad_norm": 0.15599733591079712,
      "learning_rate": 2.3693665948054617e-05,
      "loss": 0.1753,
      "step": 42950
    },
    {
      "epoch": 2.646134893748075,
      "grad_norm": 0.12367119640111923,
      "learning_rate": 2.3652602402217434e-05,
      "loss": 0.1748,
      "step": 42960
    },
    {
      "epoch": 2.6467508469356327,
      "grad_norm": 0.13836495578289032,
      "learning_rate": 2.361153885638025e-05,
      "loss": 0.1774,
      "step": 42970
    },
    {
      "epoch": 2.6473668001231907,
      "grad_norm": 0.14559420943260193,
      "learning_rate": 2.3570475310543067e-05,
      "loss": 0.1753,
      "step": 42980
    },
    {
      "epoch": 2.6479827533107483,
      "grad_norm": 0.16083025932312012,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 0.1756,
      "step": 42990
    },
    {
      "epoch": 2.648598706498306,
      "grad_norm": 0.154496431350708,
      "learning_rate": 2.34883482188687e-05,
      "loss": 0.1758,
      "step": 43000
    },
    {
      "epoch": 2.649214659685864,
      "grad_norm": 0.15676149725914001,
      "learning_rate": 2.3447284673031517e-05,
      "loss": 0.1759,
      "step": 43010
    },
    {
      "epoch": 2.6498306128734215,
      "grad_norm": 0.1586066484451294,
      "learning_rate": 2.3406221127194334e-05,
      "loss": 0.1752,
      "step": 43020
    },
    {
      "epoch": 2.6504465660609795,
      "grad_norm": 0.13593962788581848,
      "learning_rate": 2.336515758135715e-05,
      "loss": 0.1743,
      "step": 43030
    },
    {
      "epoch": 2.651062519248537,
      "grad_norm": 0.14063501358032227,
      "learning_rate": 2.332409403551997e-05,
      "loss": 0.1762,
      "step": 43040
    },
    {
      "epoch": 2.651678472436095,
      "grad_norm": 0.15723112225532532,
      "learning_rate": 2.3283030489682787e-05,
      "loss": 0.1762,
      "step": 43050
    },
    {
      "epoch": 2.6522944256236527,
      "grad_norm": 0.16290323436260223,
      "learning_rate": 2.3241966943845604e-05,
      "loss": 0.1761,
      "step": 43060
    },
    {
      "epoch": 2.6529103788112103,
      "grad_norm": 0.14016719162464142,
      "learning_rate": 2.320090339800842e-05,
      "loss": 0.1747,
      "step": 43070
    },
    {
      "epoch": 2.6535263319987683,
      "grad_norm": 0.16084490716457367,
      "learning_rate": 2.3159839852171237e-05,
      "loss": 0.1741,
      "step": 43080
    },
    {
      "epoch": 2.654142285186326,
      "grad_norm": 0.13213908672332764,
      "learning_rate": 2.3118776306334054e-05,
      "loss": 0.1756,
      "step": 43090
    },
    {
      "epoch": 2.6547582383738835,
      "grad_norm": 0.14738649129867554,
      "learning_rate": 2.307771276049687e-05,
      "loss": 0.1765,
      "step": 43100
    },
    {
      "epoch": 2.6553741915614415,
      "grad_norm": 0.15043611824512482,
      "learning_rate": 2.3036649214659687e-05,
      "loss": 0.1764,
      "step": 43110
    },
    {
      "epoch": 2.655990144748999,
      "grad_norm": 0.1663593202829361,
      "learning_rate": 2.2995585668822504e-05,
      "loss": 0.1742,
      "step": 43120
    },
    {
      "epoch": 2.6566060979365567,
      "grad_norm": 0.1283825933933258,
      "learning_rate": 2.295452212298532e-05,
      "loss": 0.1763,
      "step": 43130
    },
    {
      "epoch": 2.6572220511241147,
      "grad_norm": 0.15810617804527283,
      "learning_rate": 2.291345857714814e-05,
      "loss": 0.1754,
      "step": 43140
    },
    {
      "epoch": 2.6578380043116723,
      "grad_norm": 0.16585519909858704,
      "learning_rate": 2.2872395031310957e-05,
      "loss": 0.1755,
      "step": 43150
    },
    {
      "epoch": 2.65845395749923,
      "grad_norm": 0.14851081371307373,
      "learning_rate": 2.2831331485473773e-05,
      "loss": 0.1774,
      "step": 43160
    },
    {
      "epoch": 2.659069910686788,
      "grad_norm": 0.1343211829662323,
      "learning_rate": 2.279026793963659e-05,
      "loss": 0.1754,
      "step": 43170
    },
    {
      "epoch": 2.6596858638743455,
      "grad_norm": 0.15135595202445984,
      "learning_rate": 2.2749204393799407e-05,
      "loss": 0.1757,
      "step": 43180
    },
    {
      "epoch": 2.660301817061903,
      "grad_norm": 0.15197999775409698,
      "learning_rate": 2.2708140847962223e-05,
      "loss": 0.1749,
      "step": 43190
    },
    {
      "epoch": 2.660917770249461,
      "grad_norm": 0.14296844601631165,
      "learning_rate": 2.266707730212504e-05,
      "loss": 0.1754,
      "step": 43200
    },
    {
      "epoch": 2.6615337234370187,
      "grad_norm": 0.1846902221441269,
      "learning_rate": 2.2626013756287857e-05,
      "loss": 0.1761,
      "step": 43210
    },
    {
      "epoch": 2.6621496766245767,
      "grad_norm": 0.1535496711730957,
      "learning_rate": 2.2584950210450673e-05,
      "loss": 0.176,
      "step": 43220
    },
    {
      "epoch": 2.6627656298121343,
      "grad_norm": 0.14584210515022278,
      "learning_rate": 2.254388666461349e-05,
      "loss": 0.173,
      "step": 43230
    },
    {
      "epoch": 2.6633815829996923,
      "grad_norm": 0.15208053588867188,
      "learning_rate": 2.2502823118776307e-05,
      "loss": 0.1738,
      "step": 43240
    },
    {
      "epoch": 2.66399753618725,
      "grad_norm": 0.14611564576625824,
      "learning_rate": 2.2461759572939127e-05,
      "loss": 0.1751,
      "step": 43250
    },
    {
      "epoch": 2.6646134893748075,
      "grad_norm": 0.16304491460323334,
      "learning_rate": 2.2420696027101943e-05,
      "loss": 0.1759,
      "step": 43260
    },
    {
      "epoch": 2.6652294425623655,
      "grad_norm": 0.16408927738666534,
      "learning_rate": 2.237963248126476e-05,
      "loss": 0.1754,
      "step": 43270
    },
    {
      "epoch": 2.665845395749923,
      "grad_norm": 0.1603207141160965,
      "learning_rate": 2.2338568935427577e-05,
      "loss": 0.1754,
      "step": 43280
    },
    {
      "epoch": 2.6664613489374807,
      "grad_norm": 0.16186735033988953,
      "learning_rate": 2.2297505389590393e-05,
      "loss": 0.1752,
      "step": 43290
    },
    {
      "epoch": 2.6670773021250387,
      "grad_norm": 0.14835231006145477,
      "learning_rate": 2.225644184375321e-05,
      "loss": 0.1752,
      "step": 43300
    },
    {
      "epoch": 2.6676932553125963,
      "grad_norm": 0.15482982993125916,
      "learning_rate": 2.2215378297916027e-05,
      "loss": 0.1772,
      "step": 43310
    },
    {
      "epoch": 2.668309208500154,
      "grad_norm": 0.14469368755817413,
      "learning_rate": 2.2174314752078843e-05,
      "loss": 0.1758,
      "step": 43320
    },
    {
      "epoch": 2.668925161687712,
      "grad_norm": 0.1524498015642166,
      "learning_rate": 2.213325120624166e-05,
      "loss": 0.1749,
      "step": 43330
    },
    {
      "epoch": 2.6695411148752695,
      "grad_norm": 0.1718311756849289,
      "learning_rate": 2.2092187660404477e-05,
      "loss": 0.1753,
      "step": 43340
    },
    {
      "epoch": 2.670157068062827,
      "grad_norm": 0.14540769159793854,
      "learning_rate": 2.2051124114567297e-05,
      "loss": 0.1768,
      "step": 43350
    },
    {
      "epoch": 2.670773021250385,
      "grad_norm": 0.15176327526569366,
      "learning_rate": 2.2010060568730113e-05,
      "loss": 0.1742,
      "step": 43360
    },
    {
      "epoch": 2.6713889744379427,
      "grad_norm": 0.15380610525608063,
      "learning_rate": 2.196899702289293e-05,
      "loss": 0.1765,
      "step": 43370
    },
    {
      "epoch": 2.6720049276255002,
      "grad_norm": 0.145295649766922,
      "learning_rate": 2.1927933477055747e-05,
      "loss": 0.1749,
      "step": 43380
    },
    {
      "epoch": 2.6726208808130583,
      "grad_norm": 0.14013592898845673,
      "learning_rate": 2.1886869931218563e-05,
      "loss": 0.1755,
      "step": 43390
    },
    {
      "epoch": 2.673236834000616,
      "grad_norm": 0.14570163190364838,
      "learning_rate": 2.184580638538138e-05,
      "loss": 0.176,
      "step": 43400
    },
    {
      "epoch": 2.6738527871881734,
      "grad_norm": 0.14308708906173706,
      "learning_rate": 2.1804742839544197e-05,
      "loss": 0.1753,
      "step": 43410
    },
    {
      "epoch": 2.6744687403757315,
      "grad_norm": 0.14705494046211243,
      "learning_rate": 2.1763679293707013e-05,
      "loss": 0.1752,
      "step": 43420
    },
    {
      "epoch": 2.675084693563289,
      "grad_norm": 0.15019309520721436,
      "learning_rate": 2.172261574786983e-05,
      "loss": 0.1745,
      "step": 43430
    },
    {
      "epoch": 2.675700646750847,
      "grad_norm": 0.1356307715177536,
      "learning_rate": 2.1681552202032646e-05,
      "loss": 0.1743,
      "step": 43440
    },
    {
      "epoch": 2.6763165999384047,
      "grad_norm": 0.15108853578567505,
      "learning_rate": 2.1640488656195463e-05,
      "loss": 0.1752,
      "step": 43450
    },
    {
      "epoch": 2.6769325531259627,
      "grad_norm": 0.12876464426517487,
      "learning_rate": 2.1599425110358283e-05,
      "loss": 0.1757,
      "step": 43460
    },
    {
      "epoch": 2.6775485063135203,
      "grad_norm": 0.15060025453567505,
      "learning_rate": 2.15583615645211e-05,
      "loss": 0.175,
      "step": 43470
    },
    {
      "epoch": 2.678164459501078,
      "grad_norm": 0.16704462468624115,
      "learning_rate": 2.1517298018683916e-05,
      "loss": 0.1753,
      "step": 43480
    },
    {
      "epoch": 2.678780412688636,
      "grad_norm": 0.15777859091758728,
      "learning_rate": 2.1476234472846733e-05,
      "loss": 0.1748,
      "step": 43490
    },
    {
      "epoch": 2.6793963658761935,
      "grad_norm": 0.14400915801525116,
      "learning_rate": 2.143517092700955e-05,
      "loss": 0.1757,
      "step": 43500
    },
    {
      "epoch": 2.680012319063751,
      "grad_norm": 0.15478745102882385,
      "learning_rate": 2.1394107381172366e-05,
      "loss": 0.1768,
      "step": 43510
    },
    {
      "epoch": 2.680628272251309,
      "grad_norm": 0.1521037220954895,
      "learning_rate": 2.1353043835335183e-05,
      "loss": 0.1754,
      "step": 43520
    },
    {
      "epoch": 2.6812442254388666,
      "grad_norm": 0.15483267605304718,
      "learning_rate": 2.1311980289498e-05,
      "loss": 0.1735,
      "step": 43530
    },
    {
      "epoch": 2.6818601786264242,
      "grad_norm": 0.13650324940681458,
      "learning_rate": 2.1270916743660816e-05,
      "loss": 0.177,
      "step": 43540
    },
    {
      "epoch": 2.6824761318139823,
      "grad_norm": 0.1399882584810257,
      "learning_rate": 2.1229853197823633e-05,
      "loss": 0.1758,
      "step": 43550
    },
    {
      "epoch": 2.68309208500154,
      "grad_norm": 0.14664408564567566,
      "learning_rate": 2.118878965198645e-05,
      "loss": 0.1752,
      "step": 43560
    },
    {
      "epoch": 2.6837080381890974,
      "grad_norm": 0.15802469849586487,
      "learning_rate": 2.114772610614927e-05,
      "loss": 0.1758,
      "step": 43570
    },
    {
      "epoch": 2.6843239913766554,
      "grad_norm": 0.1376975178718567,
      "learning_rate": 2.1106662560312086e-05,
      "loss": 0.1752,
      "step": 43580
    },
    {
      "epoch": 2.684939944564213,
      "grad_norm": 0.1636730581521988,
      "learning_rate": 2.1065599014474903e-05,
      "loss": 0.1753,
      "step": 43590
    },
    {
      "epoch": 2.6855558977517706,
      "grad_norm": 0.14109329879283905,
      "learning_rate": 2.102453546863772e-05,
      "loss": 0.1751,
      "step": 43600
    },
    {
      "epoch": 2.6861718509393286,
      "grad_norm": 0.1556679606437683,
      "learning_rate": 2.0983471922800536e-05,
      "loss": 0.1747,
      "step": 43610
    },
    {
      "epoch": 2.6867878041268862,
      "grad_norm": 0.14364898204803467,
      "learning_rate": 2.0942408376963353e-05,
      "loss": 0.1749,
      "step": 43620
    },
    {
      "epoch": 2.6874037573144443,
      "grad_norm": 0.1529705971479416,
      "learning_rate": 2.090134483112617e-05,
      "loss": 0.1765,
      "step": 43630
    },
    {
      "epoch": 2.688019710502002,
      "grad_norm": 0.1609773337841034,
      "learning_rate": 2.0860281285288986e-05,
      "loss": 0.1747,
      "step": 43640
    },
    {
      "epoch": 2.68863566368956,
      "grad_norm": 0.16250386834144592,
      "learning_rate": 2.0819217739451803e-05,
      "loss": 0.1757,
      "step": 43650
    },
    {
      "epoch": 2.6892516168771174,
      "grad_norm": 0.16640768945217133,
      "learning_rate": 2.077815419361462e-05,
      "loss": 0.1749,
      "step": 43660
    },
    {
      "epoch": 2.689867570064675,
      "grad_norm": 0.1427353173494339,
      "learning_rate": 2.073709064777744e-05,
      "loss": 0.1759,
      "step": 43670
    },
    {
      "epoch": 2.690483523252233,
      "grad_norm": 0.16272713243961334,
      "learning_rate": 2.0696027101940256e-05,
      "loss": 0.1757,
      "step": 43680
    },
    {
      "epoch": 2.6910994764397906,
      "grad_norm": 0.1392998844385147,
      "learning_rate": 2.0654963556103073e-05,
      "loss": 0.1751,
      "step": 43690
    },
    {
      "epoch": 2.691715429627348,
      "grad_norm": 0.15570320188999176,
      "learning_rate": 2.061390001026589e-05,
      "loss": 0.1759,
      "step": 43700
    },
    {
      "epoch": 2.6923313828149062,
      "grad_norm": 0.15697741508483887,
      "learning_rate": 2.0572836464428706e-05,
      "loss": 0.1755,
      "step": 43710
    },
    {
      "epoch": 2.692947336002464,
      "grad_norm": 0.13024604320526123,
      "learning_rate": 2.0531772918591523e-05,
      "loss": 0.1754,
      "step": 43720
    },
    {
      "epoch": 2.6935632891900214,
      "grad_norm": 0.1722198873758316,
      "learning_rate": 2.049070937275434e-05,
      "loss": 0.1765,
      "step": 43730
    },
    {
      "epoch": 2.6941792423775794,
      "grad_norm": 0.1743685007095337,
      "learning_rate": 2.0449645826917156e-05,
      "loss": 0.1758,
      "step": 43740
    },
    {
      "epoch": 2.694795195565137,
      "grad_norm": 0.12767696380615234,
      "learning_rate": 2.0408582281079973e-05,
      "loss": 0.1747,
      "step": 43750
    },
    {
      "epoch": 2.6954111487526946,
      "grad_norm": 0.16143319010734558,
      "learning_rate": 2.036751873524279e-05,
      "loss": 0.1754,
      "step": 43760
    },
    {
      "epoch": 2.6960271019402526,
      "grad_norm": 0.16374579071998596,
      "learning_rate": 2.0326455189405606e-05,
      "loss": 0.1756,
      "step": 43770
    },
    {
      "epoch": 2.69664305512781,
      "grad_norm": 0.14558565616607666,
      "learning_rate": 2.0285391643568423e-05,
      "loss": 0.1738,
      "step": 43780
    },
    {
      "epoch": 2.697259008315368,
      "grad_norm": 0.15451586246490479,
      "learning_rate": 2.0244328097731243e-05,
      "loss": 0.1751,
      "step": 43790
    },
    {
      "epoch": 2.697874961502926,
      "grad_norm": 0.14486433565616608,
      "learning_rate": 2.020326455189406e-05,
      "loss": 0.1759,
      "step": 43800
    },
    {
      "epoch": 2.6984909146904834,
      "grad_norm": 0.15191079676151276,
      "learning_rate": 2.0162201006056876e-05,
      "loss": 0.1748,
      "step": 43810
    },
    {
      "epoch": 2.699106867878041,
      "grad_norm": 0.14876939356327057,
      "learning_rate": 2.0121137460219693e-05,
      "loss": 0.1744,
      "step": 43820
    },
    {
      "epoch": 2.699722821065599,
      "grad_norm": 0.15659256279468536,
      "learning_rate": 2.008007391438251e-05,
      "loss": 0.1749,
      "step": 43830
    },
    {
      "epoch": 2.7003387742531566,
      "grad_norm": 0.1745317131280899,
      "learning_rate": 2.0039010368545326e-05,
      "loss": 0.1766,
      "step": 43840
    },
    {
      "epoch": 2.7009547274407146,
      "grad_norm": 0.13819162547588348,
      "learning_rate": 1.9997946822708143e-05,
      "loss": 0.1755,
      "step": 43850
    },
    {
      "epoch": 2.701570680628272,
      "grad_norm": 0.16894014179706573,
      "learning_rate": 1.995688327687096e-05,
      "loss": 0.1755,
      "step": 43860
    },
    {
      "epoch": 2.7021866338158302,
      "grad_norm": 0.1440047025680542,
      "learning_rate": 1.9919926085617494e-05,
      "loss": 0.1752,
      "step": 43870
    },
    {
      "epoch": 2.702802587003388,
      "grad_norm": 0.14139987528324127,
      "learning_rate": 1.987886253978031e-05,
      "loss": 0.1754,
      "step": 43880
    },
    {
      "epoch": 2.7034185401909454,
      "grad_norm": 0.15138456225395203,
      "learning_rate": 1.9837798993943127e-05,
      "loss": 0.1763,
      "step": 43890
    },
    {
      "epoch": 2.7040344933785034,
      "grad_norm": 0.169159397482872,
      "learning_rate": 1.9796735448105944e-05,
      "loss": 0.1748,
      "step": 43900
    },
    {
      "epoch": 2.704650446566061,
      "grad_norm": 0.15544898808002472,
      "learning_rate": 1.9755671902268764e-05,
      "loss": 0.1745,
      "step": 43910
    },
    {
      "epoch": 2.7052663997536186,
      "grad_norm": 0.16922827064990997,
      "learning_rate": 1.971460835643158e-05,
      "loss": 0.1761,
      "step": 43920
    },
    {
      "epoch": 2.7058823529411766,
      "grad_norm": 0.15318982303142548,
      "learning_rate": 1.9673544810594397e-05,
      "loss": 0.175,
      "step": 43930
    },
    {
      "epoch": 2.706498306128734,
      "grad_norm": 0.15849825739860535,
      "learning_rate": 1.9632481264757214e-05,
      "loss": 0.175,
      "step": 43940
    },
    {
      "epoch": 2.707114259316292,
      "grad_norm": 0.16150981187820435,
      "learning_rate": 1.959141771892003e-05,
      "loss": 0.1736,
      "step": 43950
    },
    {
      "epoch": 2.70773021250385,
      "grad_norm": 0.15018875896930695,
      "learning_rate": 1.9550354173082847e-05,
      "loss": 0.1745,
      "step": 43960
    },
    {
      "epoch": 2.7083461656914074,
      "grad_norm": 0.1500973254442215,
      "learning_rate": 1.9509290627245664e-05,
      "loss": 0.1753,
      "step": 43970
    },
    {
      "epoch": 2.708962118878965,
      "grad_norm": 0.152256041765213,
      "learning_rate": 1.946822708140848e-05,
      "loss": 0.1759,
      "step": 43980
    },
    {
      "epoch": 2.709578072066523,
      "grad_norm": 0.14564751088619232,
      "learning_rate": 1.9427163535571297e-05,
      "loss": 0.1754,
      "step": 43990
    },
    {
      "epoch": 2.7101940252540806,
      "grad_norm": 0.15240417420864105,
      "learning_rate": 1.9386099989734114e-05,
      "loss": 0.1747,
      "step": 44000
    },
    {
      "epoch": 2.710809978441638,
      "grad_norm": 0.1479889303445816,
      "learning_rate": 1.934503644389693e-05,
      "loss": 0.1759,
      "step": 44010
    },
    {
      "epoch": 2.711425931629196,
      "grad_norm": 0.1520577371120453,
      "learning_rate": 1.930397289805975e-05,
      "loss": 0.1773,
      "step": 44020
    },
    {
      "epoch": 2.712041884816754,
      "grad_norm": 0.1496848315000534,
      "learning_rate": 1.9262909352222567e-05,
      "loss": 0.1752,
      "step": 44030
    },
    {
      "epoch": 2.712657838004312,
      "grad_norm": 0.15325166285037994,
      "learning_rate": 1.9221845806385384e-05,
      "loss": 0.1756,
      "step": 44040
    },
    {
      "epoch": 2.7132737911918694,
      "grad_norm": 0.1451369673013687,
      "learning_rate": 1.91807822605482e-05,
      "loss": 0.1769,
      "step": 44050
    },
    {
      "epoch": 2.7138897443794274,
      "grad_norm": 0.16266165673732758,
      "learning_rate": 1.9139718714711017e-05,
      "loss": 0.1748,
      "step": 44060
    },
    {
      "epoch": 2.714505697566985,
      "grad_norm": 0.16100522875785828,
      "learning_rate": 1.9098655168873834e-05,
      "loss": 0.1759,
      "step": 44070
    },
    {
      "epoch": 2.7151216507545426,
      "grad_norm": 0.169472336769104,
      "learning_rate": 1.905759162303665e-05,
      "loss": 0.1742,
      "step": 44080
    },
    {
      "epoch": 2.7157376039421006,
      "grad_norm": 0.1469627469778061,
      "learning_rate": 1.9016528077199467e-05,
      "loss": 0.1751,
      "step": 44090
    },
    {
      "epoch": 2.716353557129658,
      "grad_norm": 0.14777450263500214,
      "learning_rate": 1.8975464531362284e-05,
      "loss": 0.1757,
      "step": 44100
    },
    {
      "epoch": 2.716969510317216,
      "grad_norm": 0.16296131908893585,
      "learning_rate": 1.89344009855251e-05,
      "loss": 0.1756,
      "step": 44110
    },
    {
      "epoch": 2.717585463504774,
      "grad_norm": 0.15883837640285492,
      "learning_rate": 1.889333743968792e-05,
      "loss": 0.1767,
      "step": 44120
    },
    {
      "epoch": 2.7182014166923314,
      "grad_norm": 0.13247652351856232,
      "learning_rate": 1.8852273893850737e-05,
      "loss": 0.1738,
      "step": 44130
    },
    {
      "epoch": 2.718817369879889,
      "grad_norm": 0.16012927889823914,
      "learning_rate": 1.8811210348013554e-05,
      "loss": 0.1757,
      "step": 44140
    },
    {
      "epoch": 2.719433323067447,
      "grad_norm": 0.1411997377872467,
      "learning_rate": 1.877014680217637e-05,
      "loss": 0.1741,
      "step": 44150
    },
    {
      "epoch": 2.7200492762550046,
      "grad_norm": 0.16447046399116516,
      "learning_rate": 1.8729083256339187e-05,
      "loss": 0.175,
      "step": 44160
    },
    {
      "epoch": 2.720665229442562,
      "grad_norm": 0.16904622316360474,
      "learning_rate": 1.8688019710502004e-05,
      "loss": 0.1753,
      "step": 44170
    },
    {
      "epoch": 2.72128118263012,
      "grad_norm": 0.147609144449234,
      "learning_rate": 1.864695616466482e-05,
      "loss": 0.1762,
      "step": 44180
    },
    {
      "epoch": 2.721897135817678,
      "grad_norm": 0.13124649226665497,
      "learning_rate": 1.8605892618827637e-05,
      "loss": 0.1767,
      "step": 44190
    },
    {
      "epoch": 2.7225130890052354,
      "grad_norm": 0.147687628865242,
      "learning_rate": 1.8564829072990454e-05,
      "loss": 0.1757,
      "step": 44200
    },
    {
      "epoch": 2.7231290421927934,
      "grad_norm": 0.1448567658662796,
      "learning_rate": 1.852376552715327e-05,
      "loss": 0.1746,
      "step": 44210
    },
    {
      "epoch": 2.723744995380351,
      "grad_norm": 0.1627662479877472,
      "learning_rate": 1.8482701981316087e-05,
      "loss": 0.1761,
      "step": 44220
    },
    {
      "epoch": 2.724360948567909,
      "grad_norm": 0.1589156538248062,
      "learning_rate": 1.8441638435478907e-05,
      "loss": 0.1757,
      "step": 44230
    },
    {
      "epoch": 2.7249769017554666,
      "grad_norm": 0.1515921950340271,
      "learning_rate": 1.8400574889641724e-05,
      "loss": 0.1756,
      "step": 44240
    },
    {
      "epoch": 2.725592854943024,
      "grad_norm": 0.1483149528503418,
      "learning_rate": 1.835951134380454e-05,
      "loss": 0.1742,
      "step": 44250
    },
    {
      "epoch": 2.726208808130582,
      "grad_norm": 0.155783012509346,
      "learning_rate": 1.8318447797967357e-05,
      "loss": 0.1759,
      "step": 44260
    },
    {
      "epoch": 2.72682476131814,
      "grad_norm": 0.1426982283592224,
      "learning_rate": 1.8277384252130173e-05,
      "loss": 0.1759,
      "step": 44270
    },
    {
      "epoch": 2.727440714505698,
      "grad_norm": 0.1536683738231659,
      "learning_rate": 1.823632070629299e-05,
      "loss": 0.1759,
      "step": 44280
    },
    {
      "epoch": 2.7280566676932554,
      "grad_norm": 0.16038449108600616,
      "learning_rate": 1.8195257160455807e-05,
      "loss": 0.1759,
      "step": 44290
    },
    {
      "epoch": 2.728672620880813,
      "grad_norm": 0.14191986620426178,
      "learning_rate": 1.8154193614618623e-05,
      "loss": 0.1758,
      "step": 44300
    },
    {
      "epoch": 2.729288574068371,
      "grad_norm": 0.148042693734169,
      "learning_rate": 1.811313006878144e-05,
      "loss": 0.1756,
      "step": 44310
    },
    {
      "epoch": 2.7299045272559286,
      "grad_norm": 0.14731445908546448,
      "learning_rate": 1.8072066522944257e-05,
      "loss": 0.1761,
      "step": 44320
    },
    {
      "epoch": 2.730520480443486,
      "grad_norm": 0.1673537641763687,
      "learning_rate": 1.8031002977107077e-05,
      "loss": 0.1772,
      "step": 44330
    },
    {
      "epoch": 2.731136433631044,
      "grad_norm": 0.16310618817806244,
      "learning_rate": 1.7989939431269893e-05,
      "loss": 0.1747,
      "step": 44340
    },
    {
      "epoch": 2.7317523868186018,
      "grad_norm": 0.1420089155435562,
      "learning_rate": 1.794887588543271e-05,
      "loss": 0.175,
      "step": 44350
    },
    {
      "epoch": 2.7323683400061594,
      "grad_norm": 0.15664884448051453,
      "learning_rate": 1.7907812339595527e-05,
      "loss": 0.1764,
      "step": 44360
    },
    {
      "epoch": 2.7329842931937174,
      "grad_norm": 0.13974876701831818,
      "learning_rate": 1.7866748793758343e-05,
      "loss": 0.1767,
      "step": 44370
    },
    {
      "epoch": 2.733600246381275,
      "grad_norm": 0.13634958863258362,
      "learning_rate": 1.782568524792116e-05,
      "loss": 0.1754,
      "step": 44380
    },
    {
      "epoch": 2.7342161995688326,
      "grad_norm": 0.16270485520362854,
      "learning_rate": 1.7784621702083977e-05,
      "loss": 0.1756,
      "step": 44390
    },
    {
      "epoch": 2.7348321527563906,
      "grad_norm": 0.15351158380508423,
      "learning_rate": 1.7743558156246793e-05,
      "loss": 0.1759,
      "step": 44400
    },
    {
      "epoch": 2.735448105943948,
      "grad_norm": 0.14054372906684875,
      "learning_rate": 1.770249461040961e-05,
      "loss": 0.1757,
      "step": 44410
    },
    {
      "epoch": 2.7360640591315057,
      "grad_norm": 0.13445858657360077,
      "learning_rate": 1.7661431064572427e-05,
      "loss": 0.1743,
      "step": 44420
    },
    {
      "epoch": 2.7366800123190638,
      "grad_norm": 0.13562509417533875,
      "learning_rate": 1.7620367518735243e-05,
      "loss": 0.1755,
      "step": 44430
    },
    {
      "epoch": 2.7372959655066214,
      "grad_norm": 0.15378238260746002,
      "learning_rate": 1.7579303972898063e-05,
      "loss": 0.1748,
      "step": 44440
    },
    {
      "epoch": 2.7379119186941794,
      "grad_norm": 0.16276855766773224,
      "learning_rate": 1.753824042706088e-05,
      "loss": 0.1746,
      "step": 44450
    },
    {
      "epoch": 2.738527871881737,
      "grad_norm": 0.1887577921152115,
      "learning_rate": 1.7497176881223697e-05,
      "loss": 0.1748,
      "step": 44460
    },
    {
      "epoch": 2.739143825069295,
      "grad_norm": 0.1305774599313736,
      "learning_rate": 1.7456113335386513e-05,
      "loss": 0.1756,
      "step": 44470
    },
    {
      "epoch": 2.7397597782568526,
      "grad_norm": 0.14485245943069458,
      "learning_rate": 1.741504978954933e-05,
      "loss": 0.1749,
      "step": 44480
    },
    {
      "epoch": 2.74037573144441,
      "grad_norm": 0.1412232369184494,
      "learning_rate": 1.7373986243712147e-05,
      "loss": 0.1753,
      "step": 44490
    },
    {
      "epoch": 2.740991684631968,
      "grad_norm": 0.16735586524009705,
      "learning_rate": 1.7332922697874963e-05,
      "loss": 0.1747,
      "step": 44500
    },
    {
      "epoch": 2.7416076378195258,
      "grad_norm": 0.14964506030082703,
      "learning_rate": 1.729185915203778e-05,
      "loss": 0.1763,
      "step": 44510
    },
    {
      "epoch": 2.7422235910070833,
      "grad_norm": 0.15302516520023346,
      "learning_rate": 1.7250795606200596e-05,
      "loss": 0.1742,
      "step": 44520
    },
    {
      "epoch": 2.7428395441946414,
      "grad_norm": 0.14505401253700256,
      "learning_rate": 1.7209732060363413e-05,
      "loss": 0.1752,
      "step": 44530
    },
    {
      "epoch": 2.743455497382199,
      "grad_norm": 0.1440560221672058,
      "learning_rate": 1.716866851452623e-05,
      "loss": 0.1761,
      "step": 44540
    },
    {
      "epoch": 2.7440714505697565,
      "grad_norm": 0.18970361351966858,
      "learning_rate": 1.712760496868905e-05,
      "loss": 0.1756,
      "step": 44550
    },
    {
      "epoch": 2.7446874037573146,
      "grad_norm": 0.17126516997814178,
      "learning_rate": 1.7086541422851866e-05,
      "loss": 0.1762,
      "step": 44560
    },
    {
      "epoch": 2.745303356944872,
      "grad_norm": 0.1435653120279312,
      "learning_rate": 1.7045477877014683e-05,
      "loss": 0.1746,
      "step": 44570
    },
    {
      "epoch": 2.7459193101324297,
      "grad_norm": 0.14436940848827362,
      "learning_rate": 1.70044143311775e-05,
      "loss": 0.1763,
      "step": 44580
    },
    {
      "epoch": 2.7465352633199878,
      "grad_norm": 0.14988376200199127,
      "learning_rate": 1.6963350785340316e-05,
      "loss": 0.1769,
      "step": 44590
    },
    {
      "epoch": 2.7471512165075453,
      "grad_norm": 0.14639955759048462,
      "learning_rate": 1.6922287239503133e-05,
      "loss": 0.1752,
      "step": 44600
    },
    {
      "epoch": 2.747767169695103,
      "grad_norm": 0.17057858407497406,
      "learning_rate": 1.688122369366595e-05,
      "loss": 0.1751,
      "step": 44610
    },
    {
      "epoch": 2.748383122882661,
      "grad_norm": 0.16079650819301605,
      "learning_rate": 1.6840160147828766e-05,
      "loss": 0.1766,
      "step": 44620
    },
    {
      "epoch": 2.7489990760702185,
      "grad_norm": 0.15338601171970367,
      "learning_rate": 1.6799096601991583e-05,
      "loss": 0.1762,
      "step": 44630
    },
    {
      "epoch": 2.7496150292577766,
      "grad_norm": 0.1601652354001999,
      "learning_rate": 1.67580330561544e-05,
      "loss": 0.1758,
      "step": 44640
    },
    {
      "epoch": 2.750230982445334,
      "grad_norm": 0.16599392890930176,
      "learning_rate": 1.671696951031722e-05,
      "loss": 0.175,
      "step": 44650
    },
    {
      "epoch": 2.750846935632892,
      "grad_norm": 0.16210635006427765,
      "learning_rate": 1.6675905964480036e-05,
      "loss": 0.1758,
      "step": 44660
    },
    {
      "epoch": 2.7514628888204498,
      "grad_norm": 0.16380053758621216,
      "learning_rate": 1.6634842418642853e-05,
      "loss": 0.1753,
      "step": 44670
    },
    {
      "epoch": 2.7520788420080073,
      "grad_norm": 0.1556481271982193,
      "learning_rate": 1.659377887280567e-05,
      "loss": 0.175,
      "step": 44680
    },
    {
      "epoch": 2.7526947951955654,
      "grad_norm": 0.15404435992240906,
      "learning_rate": 1.6552715326968486e-05,
      "loss": 0.1753,
      "step": 44690
    },
    {
      "epoch": 2.753310748383123,
      "grad_norm": 0.1632278561592102,
      "learning_rate": 1.6511651781131303e-05,
      "loss": 0.1748,
      "step": 44700
    },
    {
      "epoch": 2.7539267015706805,
      "grad_norm": 0.1317841112613678,
      "learning_rate": 1.647058823529412e-05,
      "loss": 0.1764,
      "step": 44710
    },
    {
      "epoch": 2.7545426547582386,
      "grad_norm": 0.12893186509609222,
      "learning_rate": 1.6429524689456936e-05,
      "loss": 0.174,
      "step": 44720
    },
    {
      "epoch": 2.755158607945796,
      "grad_norm": 0.16426950693130493,
      "learning_rate": 1.6388461143619753e-05,
      "loss": 0.175,
      "step": 44730
    },
    {
      "epoch": 2.7557745611333537,
      "grad_norm": 0.14768654108047485,
      "learning_rate": 1.634739759778257e-05,
      "loss": 0.175,
      "step": 44740
    },
    {
      "epoch": 2.7563905143209118,
      "grad_norm": 0.1557392179965973,
      "learning_rate": 1.6306334051945386e-05,
      "loss": 0.1747,
      "step": 44750
    },
    {
      "epoch": 2.7570064675084693,
      "grad_norm": 0.17169928550720215,
      "learning_rate": 1.6265270506108203e-05,
      "loss": 0.1755,
      "step": 44760
    },
    {
      "epoch": 2.757622420696027,
      "grad_norm": 0.16318780183792114,
      "learning_rate": 1.622420696027102e-05,
      "loss": 0.1734,
      "step": 44770
    },
    {
      "epoch": 2.758238373883585,
      "grad_norm": 0.14466853439807892,
      "learning_rate": 1.618314341443384e-05,
      "loss": 0.1746,
      "step": 44780
    },
    {
      "epoch": 2.7588543270711425,
      "grad_norm": 0.16873493790626526,
      "learning_rate": 1.6142079868596656e-05,
      "loss": 0.1763,
      "step": 44790
    },
    {
      "epoch": 2.7594702802587,
      "grad_norm": 0.13718312978744507,
      "learning_rate": 1.6101016322759473e-05,
      "loss": 0.1747,
      "step": 44800
    },
    {
      "epoch": 2.760086233446258,
      "grad_norm": 0.15180909633636475,
      "learning_rate": 1.605995277692229e-05,
      "loss": 0.176,
      "step": 44810
    },
    {
      "epoch": 2.7607021866338157,
      "grad_norm": 0.16029232740402222,
      "learning_rate": 1.6018889231085106e-05,
      "loss": 0.1756,
      "step": 44820
    },
    {
      "epoch": 2.7613181398213733,
      "grad_norm": 0.16603675484657288,
      "learning_rate": 1.5977825685247923e-05,
      "loss": 0.1738,
      "step": 44830
    },
    {
      "epoch": 2.7619340930089313,
      "grad_norm": 0.1391817182302475,
      "learning_rate": 1.593676213941074e-05,
      "loss": 0.1759,
      "step": 44840
    },
    {
      "epoch": 2.762550046196489,
      "grad_norm": 0.1640833169221878,
      "learning_rate": 1.5895698593573556e-05,
      "loss": 0.176,
      "step": 44850
    },
    {
      "epoch": 2.763165999384047,
      "grad_norm": 0.1535925567150116,
      "learning_rate": 1.5854635047736373e-05,
      "loss": 0.174,
      "step": 44860
    },
    {
      "epoch": 2.7637819525716045,
      "grad_norm": 0.1363399773836136,
      "learning_rate": 1.581357150189919e-05,
      "loss": 0.1773,
      "step": 44870
    },
    {
      "epoch": 2.7643979057591626,
      "grad_norm": 0.1546938568353653,
      "learning_rate": 1.5772507956062006e-05,
      "loss": 0.1753,
      "step": 44880
    },
    {
      "epoch": 2.76501385894672,
      "grad_norm": 0.15096022188663483,
      "learning_rate": 1.5731444410224823e-05,
      "loss": 0.1755,
      "step": 44890
    },
    {
      "epoch": 2.7656298121342777,
      "grad_norm": 0.1516983062028885,
      "learning_rate": 1.569038086438764e-05,
      "loss": 0.1747,
      "step": 44900
    },
    {
      "epoch": 2.7662457653218357,
      "grad_norm": 0.15415069460868835,
      "learning_rate": 1.564931731855046e-05,
      "loss": 0.1755,
      "step": 44910
    },
    {
      "epoch": 2.7668617185093933,
      "grad_norm": 0.17817078530788422,
      "learning_rate": 1.5608253772713276e-05,
      "loss": 0.1756,
      "step": 44920
    },
    {
      "epoch": 2.767477671696951,
      "grad_norm": 0.15763291716575623,
      "learning_rate": 1.5567190226876093e-05,
      "loss": 0.1735,
      "step": 44930
    },
    {
      "epoch": 2.768093624884509,
      "grad_norm": 0.13614052534103394,
      "learning_rate": 1.552612668103891e-05,
      "loss": 0.1772,
      "step": 44940
    },
    {
      "epoch": 2.7687095780720665,
      "grad_norm": 0.16616009175777435,
      "learning_rate": 1.5485063135201726e-05,
      "loss": 0.1754,
      "step": 44950
    },
    {
      "epoch": 2.769325531259624,
      "grad_norm": 0.15022389590740204,
      "learning_rate": 1.5443999589364543e-05,
      "loss": 0.1747,
      "step": 44960
    },
    {
      "epoch": 2.769941484447182,
      "grad_norm": 0.1762254685163498,
      "learning_rate": 1.540293604352736e-05,
      "loss": 0.1758,
      "step": 44970
    },
    {
      "epoch": 2.7705574376347397,
      "grad_norm": 0.14133739471435547,
      "learning_rate": 1.5361872497690176e-05,
      "loss": 0.1751,
      "step": 44980
    },
    {
      "epoch": 2.7711733908222973,
      "grad_norm": 0.14305530488491058,
      "learning_rate": 1.5320808951852993e-05,
      "loss": 0.1759,
      "step": 44990
    },
    {
      "epoch": 2.7717893440098553,
      "grad_norm": 0.14574696123600006,
      "learning_rate": 1.527974540601581e-05,
      "loss": 0.1757,
      "step": 45000
    },
    {
      "epoch": 2.772405297197413,
      "grad_norm": 0.148891881108284,
      "learning_rate": 1.5238681860178628e-05,
      "loss": 0.1753,
      "step": 45010
    },
    {
      "epoch": 2.7730212503849705,
      "grad_norm": 0.1377657651901245,
      "learning_rate": 1.5197618314341444e-05,
      "loss": 0.1739,
      "step": 45020
    },
    {
      "epoch": 2.7736372035725285,
      "grad_norm": 0.16157297790050507,
      "learning_rate": 1.515655476850426e-05,
      "loss": 0.1761,
      "step": 45030
    },
    {
      "epoch": 2.774253156760086,
      "grad_norm": 0.14855389297008514,
      "learning_rate": 1.5115491222667077e-05,
      "loss": 0.1752,
      "step": 45040
    },
    {
      "epoch": 2.774869109947644,
      "grad_norm": 0.15731000900268555,
      "learning_rate": 1.5074427676829894e-05,
      "loss": 0.1767,
      "step": 45050
    },
    {
      "epoch": 2.7754850631352017,
      "grad_norm": 0.1672016829252243,
      "learning_rate": 1.503336413099271e-05,
      "loss": 0.175,
      "step": 45060
    },
    {
      "epoch": 2.7761010163227597,
      "grad_norm": 0.16168831288814545,
      "learning_rate": 1.4992300585155527e-05,
      "loss": 0.1758,
      "step": 45070
    },
    {
      "epoch": 2.7767169695103173,
      "grad_norm": 0.1790953427553177,
      "learning_rate": 1.4951237039318347e-05,
      "loss": 0.1768,
      "step": 45080
    },
    {
      "epoch": 2.777332922697875,
      "grad_norm": 0.13596245646476746,
      "learning_rate": 1.4910173493481164e-05,
      "loss": 0.175,
      "step": 45090
    },
    {
      "epoch": 2.777948875885433,
      "grad_norm": 0.14911478757858276,
      "learning_rate": 1.486910994764398e-05,
      "loss": 0.1737,
      "step": 45100
    },
    {
      "epoch": 2.7785648290729905,
      "grad_norm": 0.15936322510242462,
      "learning_rate": 1.4828046401806797e-05,
      "loss": 0.1744,
      "step": 45110
    },
    {
      "epoch": 2.779180782260548,
      "grad_norm": 0.16551901400089264,
      "learning_rate": 1.4786982855969614e-05,
      "loss": 0.1752,
      "step": 45120
    },
    {
      "epoch": 2.779796735448106,
      "grad_norm": 0.1497432291507721,
      "learning_rate": 1.474591931013243e-05,
      "loss": 0.1752,
      "step": 45130
    },
    {
      "epoch": 2.7804126886356637,
      "grad_norm": 0.14407415688037872,
      "learning_rate": 1.4704855764295247e-05,
      "loss": 0.1748,
      "step": 45140
    },
    {
      "epoch": 2.7810286418232213,
      "grad_norm": 0.1443556398153305,
      "learning_rate": 1.4663792218458064e-05,
      "loss": 0.1754,
      "step": 45150
    },
    {
      "epoch": 2.7816445950107793,
      "grad_norm": 0.1503223478794098,
      "learning_rate": 1.462272867262088e-05,
      "loss": 0.1768,
      "step": 45160
    },
    {
      "epoch": 2.782260548198337,
      "grad_norm": 0.13577552139759064,
      "learning_rate": 1.4581665126783697e-05,
      "loss": 0.1739,
      "step": 45170
    },
    {
      "epoch": 2.7828765013858945,
      "grad_norm": 0.15445782244205475,
      "learning_rate": 1.4540601580946517e-05,
      "loss": 0.1752,
      "step": 45180
    },
    {
      "epoch": 2.7834924545734525,
      "grad_norm": 0.14651057124137878,
      "learning_rate": 1.4499538035109334e-05,
      "loss": 0.1736,
      "step": 45190
    },
    {
      "epoch": 2.78410840776101,
      "grad_norm": 0.18212801218032837,
      "learning_rate": 1.445847448927215e-05,
      "loss": 0.1763,
      "step": 45200
    },
    {
      "epoch": 2.7847243609485677,
      "grad_norm": 0.13988345861434937,
      "learning_rate": 1.4417410943434967e-05,
      "loss": 0.1746,
      "step": 45210
    },
    {
      "epoch": 2.7853403141361257,
      "grad_norm": 0.15193979442119598,
      "learning_rate": 1.4376347397597784e-05,
      "loss": 0.1747,
      "step": 45220
    },
    {
      "epoch": 2.7859562673236833,
      "grad_norm": 0.14913377165794373,
      "learning_rate": 1.43352838517606e-05,
      "loss": 0.1741,
      "step": 45230
    },
    {
      "epoch": 2.786572220511241,
      "grad_norm": 0.14603139460086823,
      "learning_rate": 1.4294220305923417e-05,
      "loss": 0.176,
      "step": 45240
    },
    {
      "epoch": 2.787188173698799,
      "grad_norm": 0.13233551383018494,
      "learning_rate": 1.4253156760086234e-05,
      "loss": 0.1746,
      "step": 45250
    },
    {
      "epoch": 2.7878041268863565,
      "grad_norm": 0.13728594779968262,
      "learning_rate": 1.421209321424905e-05,
      "loss": 0.1749,
      "step": 45260
    },
    {
      "epoch": 2.7884200800739145,
      "grad_norm": 0.14493264257907867,
      "learning_rate": 1.4171029668411867e-05,
      "loss": 0.1741,
      "step": 45270
    },
    {
      "epoch": 2.789036033261472,
      "grad_norm": 0.15646861493587494,
      "learning_rate": 1.4129966122574684e-05,
      "loss": 0.1765,
      "step": 45280
    },
    {
      "epoch": 2.78965198644903,
      "grad_norm": 0.15284696221351624,
      "learning_rate": 1.4088902576737504e-05,
      "loss": 0.1754,
      "step": 45290
    },
    {
      "epoch": 2.7902679396365877,
      "grad_norm": 0.13898824155330658,
      "learning_rate": 1.404783903090032e-05,
      "loss": 0.1759,
      "step": 45300
    },
    {
      "epoch": 2.7908838928241453,
      "grad_norm": 0.1410764753818512,
      "learning_rate": 1.4006775485063137e-05,
      "loss": 0.1752,
      "step": 45310
    },
    {
      "epoch": 2.7914998460117033,
      "grad_norm": 0.144644632935524,
      "learning_rate": 1.3965711939225954e-05,
      "loss": 0.1761,
      "step": 45320
    },
    {
      "epoch": 2.792115799199261,
      "grad_norm": 0.1412656456232071,
      "learning_rate": 1.392464839338877e-05,
      "loss": 0.1755,
      "step": 45330
    },
    {
      "epoch": 2.7927317523868185,
      "grad_norm": 0.15570193529129028,
      "learning_rate": 1.3883584847551587e-05,
      "loss": 0.175,
      "step": 45340
    },
    {
      "epoch": 2.7933477055743765,
      "grad_norm": 0.17017728090286255,
      "learning_rate": 1.3842521301714404e-05,
      "loss": 0.1764,
      "step": 45350
    },
    {
      "epoch": 2.793963658761934,
      "grad_norm": 0.1518217772245407,
      "learning_rate": 1.380145775587722e-05,
      "loss": 0.1746,
      "step": 45360
    },
    {
      "epoch": 2.7945796119494917,
      "grad_norm": 0.14643974602222443,
      "learning_rate": 1.3760394210040037e-05,
      "loss": 0.1753,
      "step": 45370
    },
    {
      "epoch": 2.7951955651370497,
      "grad_norm": 0.1687633991241455,
      "learning_rate": 1.3719330664202854e-05,
      "loss": 0.1754,
      "step": 45380
    },
    {
      "epoch": 2.7958115183246073,
      "grad_norm": 0.13554902374744415,
      "learning_rate": 1.367826711836567e-05,
      "loss": 0.1755,
      "step": 45390
    },
    {
      "epoch": 2.796427471512165,
      "grad_norm": 0.1302306205034256,
      "learning_rate": 1.363720357252849e-05,
      "loss": 0.1737,
      "step": 45400
    },
    {
      "epoch": 2.797043424699723,
      "grad_norm": 0.16797177493572235,
      "learning_rate": 1.3596140026691307e-05,
      "loss": 0.1755,
      "step": 45410
    },
    {
      "epoch": 2.7976593778872805,
      "grad_norm": 0.1574430912733078,
      "learning_rate": 1.3555076480854124e-05,
      "loss": 0.1738,
      "step": 45420
    },
    {
      "epoch": 2.798275331074838,
      "grad_norm": 0.14784306287765503,
      "learning_rate": 1.351401293501694e-05,
      "loss": 0.1748,
      "step": 45430
    },
    {
      "epoch": 2.798891284262396,
      "grad_norm": 0.16308872401714325,
      "learning_rate": 1.3472949389179757e-05,
      "loss": 0.1735,
      "step": 45440
    },
    {
      "epoch": 2.7995072374499537,
      "grad_norm": 0.1447211503982544,
      "learning_rate": 1.3431885843342574e-05,
      "loss": 0.1769,
      "step": 45450
    },
    {
      "epoch": 2.8001231906375117,
      "grad_norm": 0.14534194767475128,
      "learning_rate": 1.339082229750539e-05,
      "loss": 0.1745,
      "step": 45460
    },
    {
      "epoch": 2.8007391438250693,
      "grad_norm": 0.14045962691307068,
      "learning_rate": 1.3349758751668207e-05,
      "loss": 0.1755,
      "step": 45470
    },
    {
      "epoch": 2.8013550970126273,
      "grad_norm": 0.15187771618366241,
      "learning_rate": 1.3308695205831024e-05,
      "loss": 0.1752,
      "step": 45480
    },
    {
      "epoch": 2.801971050200185,
      "grad_norm": 0.1626199334859848,
      "learning_rate": 1.326763165999384e-05,
      "loss": 0.1761,
      "step": 45490
    },
    {
      "epoch": 2.8025870033877425,
      "grad_norm": 0.16175612807273865,
      "learning_rate": 1.3226568114156659e-05,
      "loss": 0.1756,
      "step": 45500
    },
    {
      "epoch": 2.8032029565753005,
      "grad_norm": 0.14109116792678833,
      "learning_rate": 1.3185504568319477e-05,
      "loss": 0.1751,
      "step": 45510
    },
    {
      "epoch": 2.803818909762858,
      "grad_norm": 0.15179085731506348,
      "learning_rate": 1.3144441022482294e-05,
      "loss": 0.1773,
      "step": 45520
    },
    {
      "epoch": 2.8044348629504157,
      "grad_norm": 0.14905531704425812,
      "learning_rate": 1.310337747664511e-05,
      "loss": 0.1749,
      "step": 45530
    },
    {
      "epoch": 2.8050508161379737,
      "grad_norm": 0.16349972784519196,
      "learning_rate": 1.3062313930807927e-05,
      "loss": 0.1739,
      "step": 45540
    },
    {
      "epoch": 2.8056667693255313,
      "grad_norm": 0.15169841051101685,
      "learning_rate": 1.3021250384970744e-05,
      "loss": 0.1758,
      "step": 45550
    },
    {
      "epoch": 2.806282722513089,
      "grad_norm": 0.1294660121202469,
      "learning_rate": 1.298018683913356e-05,
      "loss": 0.1748,
      "step": 45560
    },
    {
      "epoch": 2.806898675700647,
      "grad_norm": 0.17339293658733368,
      "learning_rate": 1.2939123293296377e-05,
      "loss": 0.1756,
      "step": 45570
    },
    {
      "epoch": 2.8075146288882045,
      "grad_norm": 0.14910189807415009,
      "learning_rate": 1.2898059747459194e-05,
      "loss": 0.1746,
      "step": 45580
    },
    {
      "epoch": 2.808130582075762,
      "grad_norm": 0.15632590651512146,
      "learning_rate": 1.285699620162201e-05,
      "loss": 0.1747,
      "step": 45590
    },
    {
      "epoch": 2.80874653526332,
      "grad_norm": 0.15366710722446442,
      "learning_rate": 1.2815932655784827e-05,
      "loss": 0.1749,
      "step": 45600
    },
    {
      "epoch": 2.8093624884508777,
      "grad_norm": 0.15643934905529022,
      "learning_rate": 1.2774869109947645e-05,
      "loss": 0.1757,
      "step": 45610
    },
    {
      "epoch": 2.8099784416384352,
      "grad_norm": 0.16044378280639648,
      "learning_rate": 1.2733805564110462e-05,
      "loss": 0.1741,
      "step": 45620
    },
    {
      "epoch": 2.8105943948259933,
      "grad_norm": 0.13656969368457794,
      "learning_rate": 1.2692742018273278e-05,
      "loss": 0.1748,
      "step": 45630
    },
    {
      "epoch": 2.811210348013551,
      "grad_norm": 0.1445196568965912,
      "learning_rate": 1.2651678472436095e-05,
      "loss": 0.1757,
      "step": 45640
    },
    {
      "epoch": 2.811826301201109,
      "grad_norm": 0.13627687096595764,
      "learning_rate": 1.2610614926598913e-05,
      "loss": 0.1748,
      "step": 45650
    },
    {
      "epoch": 2.8124422543886665,
      "grad_norm": 0.19664618372917175,
      "learning_rate": 1.256955138076173e-05,
      "loss": 0.1752,
      "step": 45660
    },
    {
      "epoch": 2.813058207576224,
      "grad_norm": 0.155241459608078,
      "learning_rate": 1.2528487834924547e-05,
      "loss": 0.1726,
      "step": 45670
    },
    {
      "epoch": 2.813674160763782,
      "grad_norm": 0.1556827425956726,
      "learning_rate": 1.2487424289087363e-05,
      "loss": 0.1743,
      "step": 45680
    },
    {
      "epoch": 2.8142901139513397,
      "grad_norm": 0.15880730748176575,
      "learning_rate": 1.244636074325018e-05,
      "loss": 0.1745,
      "step": 45690
    },
    {
      "epoch": 2.8149060671388977,
      "grad_norm": 0.13315469026565552,
      "learning_rate": 1.2405297197412997e-05,
      "loss": 0.176,
      "step": 45700
    },
    {
      "epoch": 2.8155220203264553,
      "grad_norm": 0.1536553055047989,
      "learning_rate": 1.2364233651575815e-05,
      "loss": 0.1766,
      "step": 45710
    },
    {
      "epoch": 2.816137973514013,
      "grad_norm": 0.1527082771062851,
      "learning_rate": 1.2323170105738632e-05,
      "loss": 0.1752,
      "step": 45720
    },
    {
      "epoch": 2.816753926701571,
      "grad_norm": 0.15668100118637085,
      "learning_rate": 1.2282106559901448e-05,
      "loss": 0.1745,
      "step": 45730
    },
    {
      "epoch": 2.8173698798891285,
      "grad_norm": 0.14227263629436493,
      "learning_rate": 1.2241043014064265e-05,
      "loss": 0.1744,
      "step": 45740
    },
    {
      "epoch": 2.817985833076686,
      "grad_norm": 0.15381288528442383,
      "learning_rate": 1.2199979468227082e-05,
      "loss": 0.1754,
      "step": 45750
    },
    {
      "epoch": 2.818601786264244,
      "grad_norm": 0.15362171828746796,
      "learning_rate": 1.2158915922389898e-05,
      "loss": 0.1758,
      "step": 45760
    },
    {
      "epoch": 2.8192177394518017,
      "grad_norm": 0.1467577964067459,
      "learning_rate": 1.2117852376552715e-05,
      "loss": 0.1778,
      "step": 45770
    },
    {
      "epoch": 2.8198336926393592,
      "grad_norm": 0.14461025595664978,
      "learning_rate": 1.2076788830715533e-05,
      "loss": 0.1754,
      "step": 45780
    },
    {
      "epoch": 2.8204496458269173,
      "grad_norm": 0.18531107902526855,
      "learning_rate": 1.203572528487835e-05,
      "loss": 0.1752,
      "step": 45790
    },
    {
      "epoch": 2.821065599014475,
      "grad_norm": 0.15214422345161438,
      "learning_rate": 1.1994661739041167e-05,
      "loss": 0.1752,
      "step": 45800
    },
    {
      "epoch": 2.8216815522020324,
      "grad_norm": 0.15622493624687195,
      "learning_rate": 1.1953598193203983e-05,
      "loss": 0.1742,
      "step": 45810
    },
    {
      "epoch": 2.8222975053895905,
      "grad_norm": 0.15431153774261475,
      "learning_rate": 1.19125346473668e-05,
      "loss": 0.1753,
      "step": 45820
    },
    {
      "epoch": 2.822913458577148,
      "grad_norm": 0.15230819582939148,
      "learning_rate": 1.1871471101529617e-05,
      "loss": 0.1749,
      "step": 45830
    },
    {
      "epoch": 2.8235294117647056,
      "grad_norm": 0.1481948345899582,
      "learning_rate": 1.1830407555692435e-05,
      "loss": 0.1764,
      "step": 45840
    },
    {
      "epoch": 2.8241453649522636,
      "grad_norm": 0.1645299792289734,
      "learning_rate": 1.1789344009855252e-05,
      "loss": 0.1756,
      "step": 45850
    },
    {
      "epoch": 2.8247613181398212,
      "grad_norm": 0.145926833152771,
      "learning_rate": 1.1748280464018068e-05,
      "loss": 0.174,
      "step": 45860
    },
    {
      "epoch": 2.8253772713273793,
      "grad_norm": 0.17219257354736328,
      "learning_rate": 1.1707216918180885e-05,
      "loss": 0.1761,
      "step": 45870
    },
    {
      "epoch": 2.825993224514937,
      "grad_norm": 0.151207834482193,
      "learning_rate": 1.1666153372343701e-05,
      "loss": 0.1766,
      "step": 45880
    },
    {
      "epoch": 2.826609177702495,
      "grad_norm": 0.16617891192436218,
      "learning_rate": 1.1625089826506518e-05,
      "loss": 0.1747,
      "step": 45890
    },
    {
      "epoch": 2.8272251308900525,
      "grad_norm": 0.16995564103126526,
      "learning_rate": 1.1584026280669336e-05,
      "loss": 0.1745,
      "step": 45900
    },
    {
      "epoch": 2.82784108407761,
      "grad_norm": 0.150996133685112,
      "learning_rate": 1.1542962734832153e-05,
      "loss": 0.1762,
      "step": 45910
    },
    {
      "epoch": 2.828457037265168,
      "grad_norm": 0.148680180311203,
      "learning_rate": 1.150600554357869e-05,
      "loss": 0.1765,
      "step": 45920
    },
    {
      "epoch": 2.8290729904527256,
      "grad_norm": 0.16498294472694397,
      "learning_rate": 1.1464941997741506e-05,
      "loss": 0.1759,
      "step": 45930
    },
    {
      "epoch": 2.8296889436402832,
      "grad_norm": 0.15425454080104828,
      "learning_rate": 1.1423878451904323e-05,
      "loss": 0.1758,
      "step": 45940
    },
    {
      "epoch": 2.8303048968278413,
      "grad_norm": 0.13620735704898834,
      "learning_rate": 1.138281490606714e-05,
      "loss": 0.1756,
      "step": 45950
    },
    {
      "epoch": 2.830920850015399,
      "grad_norm": 0.17254683375358582,
      "learning_rate": 1.1341751360229956e-05,
      "loss": 0.1753,
      "step": 45960
    },
    {
      "epoch": 2.8315368032029564,
      "grad_norm": 0.1462998390197754,
      "learning_rate": 1.1300687814392773e-05,
      "loss": 0.1747,
      "step": 45970
    },
    {
      "epoch": 2.8321527563905144,
      "grad_norm": 0.15344637632369995,
      "learning_rate": 1.1259624268555591e-05,
      "loss": 0.175,
      "step": 45980
    },
    {
      "epoch": 2.832768709578072,
      "grad_norm": 0.14619436860084534,
      "learning_rate": 1.1218560722718408e-05,
      "loss": 0.1752,
      "step": 45990
    },
    {
      "epoch": 2.8333846627656296,
      "grad_norm": 0.18466006219387054,
      "learning_rate": 1.1177497176881224e-05,
      "loss": 0.1759,
      "step": 46000
    },
    {
      "epoch": 2.8340006159531876,
      "grad_norm": 0.17161506414413452,
      "learning_rate": 1.1136433631044041e-05,
      "loss": 0.1753,
      "step": 46010
    },
    {
      "epoch": 2.8346165691407452,
      "grad_norm": 0.174977108836174,
      "learning_rate": 1.1095370085206858e-05,
      "loss": 0.1757,
      "step": 46020
    },
    {
      "epoch": 2.835232522328303,
      "grad_norm": 0.1535809487104416,
      "learning_rate": 1.1054306539369676e-05,
      "loss": 0.1754,
      "step": 46030
    },
    {
      "epoch": 2.835848475515861,
      "grad_norm": 0.14809076488018036,
      "learning_rate": 1.1013242993532493e-05,
      "loss": 0.1745,
      "step": 46040
    },
    {
      "epoch": 2.8364644287034184,
      "grad_norm": 0.14839507639408112,
      "learning_rate": 1.097217944769531e-05,
      "loss": 0.1761,
      "step": 46050
    },
    {
      "epoch": 2.8370803818909764,
      "grad_norm": 0.13329611718654633,
      "learning_rate": 1.0931115901858126e-05,
      "loss": 0.1746,
      "step": 46060
    },
    {
      "epoch": 2.837696335078534,
      "grad_norm": 0.15598146617412567,
      "learning_rate": 1.0890052356020943e-05,
      "loss": 0.1761,
      "step": 46070
    },
    {
      "epoch": 2.838312288266092,
      "grad_norm": 0.16368746757507324,
      "learning_rate": 1.0848988810183761e-05,
      "loss": 0.1769,
      "step": 46080
    },
    {
      "epoch": 2.8389282414536496,
      "grad_norm": 0.1415170282125473,
      "learning_rate": 1.0807925264346578e-05,
      "loss": 0.1739,
      "step": 46090
    },
    {
      "epoch": 2.839544194641207,
      "grad_norm": 0.1586022824048996,
      "learning_rate": 1.0766861718509394e-05,
      "loss": 0.1751,
      "step": 46100
    },
    {
      "epoch": 2.8401601478287652,
      "grad_norm": 0.15341858565807343,
      "learning_rate": 1.0725798172672211e-05,
      "loss": 0.1759,
      "step": 46110
    },
    {
      "epoch": 2.840776101016323,
      "grad_norm": 0.14980082213878632,
      "learning_rate": 1.0684734626835028e-05,
      "loss": 0.1751,
      "step": 46120
    },
    {
      "epoch": 2.8413920542038804,
      "grad_norm": 0.16480818390846252,
      "learning_rate": 1.0643671080997844e-05,
      "loss": 0.1756,
      "step": 46130
    },
    {
      "epoch": 2.8420080073914384,
      "grad_norm": 0.15377697348594666,
      "learning_rate": 1.0602607535160663e-05,
      "loss": 0.175,
      "step": 46140
    },
    {
      "epoch": 2.842623960578996,
      "grad_norm": 0.13944199681282043,
      "learning_rate": 1.056154398932348e-05,
      "loss": 0.1747,
      "step": 46150
    },
    {
      "epoch": 2.8432399137665536,
      "grad_norm": 0.15466177463531494,
      "learning_rate": 1.0520480443486296e-05,
      "loss": 0.1764,
      "step": 46160
    },
    {
      "epoch": 2.8438558669541116,
      "grad_norm": 0.1646757274866104,
      "learning_rate": 1.0479416897649112e-05,
      "loss": 0.1754,
      "step": 46170
    },
    {
      "epoch": 2.844471820141669,
      "grad_norm": 0.1361217051744461,
      "learning_rate": 1.0438353351811929e-05,
      "loss": 0.1747,
      "step": 46180
    },
    {
      "epoch": 2.845087773329227,
      "grad_norm": 0.13405579328536987,
      "learning_rate": 1.0397289805974747e-05,
      "loss": 0.175,
      "step": 46190
    },
    {
      "epoch": 2.845703726516785,
      "grad_norm": 0.14267592132091522,
      "learning_rate": 1.0356226260137564e-05,
      "loss": 0.1749,
      "step": 46200
    },
    {
      "epoch": 2.8463196797043424,
      "grad_norm": 0.13930487632751465,
      "learning_rate": 1.031516271430038e-05,
      "loss": 0.1753,
      "step": 46210
    },
    {
      "epoch": 2.8469356328919,
      "grad_norm": 0.1415126472711563,
      "learning_rate": 1.0274099168463197e-05,
      "loss": 0.175,
      "step": 46220
    },
    {
      "epoch": 2.847551586079458,
      "grad_norm": 0.16102276742458344,
      "learning_rate": 1.0233035622626014e-05,
      "loss": 0.1748,
      "step": 46230
    },
    {
      "epoch": 2.8481675392670156,
      "grad_norm": 0.15453922748565674,
      "learning_rate": 1.0191972076788832e-05,
      "loss": 0.1757,
      "step": 46240
    },
    {
      "epoch": 2.848783492454573,
      "grad_norm": 0.1679893136024475,
      "learning_rate": 1.0150908530951649e-05,
      "loss": 0.1749,
      "step": 46250
    },
    {
      "epoch": 2.849399445642131,
      "grad_norm": 0.14118514955043793,
      "learning_rate": 1.0109844985114466e-05,
      "loss": 0.1748,
      "step": 46260
    },
    {
      "epoch": 2.850015398829689,
      "grad_norm": 0.14125248789787292,
      "learning_rate": 1.0068781439277282e-05,
      "loss": 0.1738,
      "step": 46270
    },
    {
      "epoch": 2.850631352017247,
      "grad_norm": 0.14345413446426392,
      "learning_rate": 1.0027717893440099e-05,
      "loss": 0.177,
      "step": 46280
    },
    {
      "epoch": 2.8512473052048044,
      "grad_norm": 0.14110983908176422,
      "learning_rate": 9.986654347602916e-06,
      "loss": 0.1746,
      "step": 46290
    },
    {
      "epoch": 2.8518632583923624,
      "grad_norm": 0.14982634782791138,
      "learning_rate": 9.945590801765734e-06,
      "loss": 0.1736,
      "step": 46300
    },
    {
      "epoch": 2.85247921157992,
      "grad_norm": 0.14567483961582184,
      "learning_rate": 9.90452725592855e-06,
      "loss": 0.1755,
      "step": 46310
    },
    {
      "epoch": 2.8530951647674776,
      "grad_norm": 0.13794992864131927,
      "learning_rate": 9.863463710091367e-06,
      "loss": 0.1745,
      "step": 46320
    },
    {
      "epoch": 2.8537111179550356,
      "grad_norm": 0.15224647521972656,
      "learning_rate": 9.822400164254184e-06,
      "loss": 0.1751,
      "step": 46330
    },
    {
      "epoch": 2.854327071142593,
      "grad_norm": 0.13704267144203186,
      "learning_rate": 9.781336618417e-06,
      "loss": 0.1764,
      "step": 46340
    },
    {
      "epoch": 2.854943024330151,
      "grad_norm": 0.13717471063137054,
      "learning_rate": 9.740273072579819e-06,
      "loss": 0.1751,
      "step": 46350
    },
    {
      "epoch": 2.855558977517709,
      "grad_norm": 0.15941454470157623,
      "learning_rate": 9.699209526742636e-06,
      "loss": 0.1751,
      "step": 46360
    },
    {
      "epoch": 2.8561749307052664,
      "grad_norm": 0.15000073611736298,
      "learning_rate": 9.658145980905452e-06,
      "loss": 0.1744,
      "step": 46370
    },
    {
      "epoch": 2.856790883892824,
      "grad_norm": 0.13326658308506012,
      "learning_rate": 9.617082435068269e-06,
      "loss": 0.1749,
      "step": 46380
    },
    {
      "epoch": 2.857406837080382,
      "grad_norm": 0.14177599549293518,
      "learning_rate": 9.576018889231086e-06,
      "loss": 0.175,
      "step": 46390
    },
    {
      "epoch": 2.8580227902679396,
      "grad_norm": 0.15730242431163788,
      "learning_rate": 9.534955343393904e-06,
      "loss": 0.1748,
      "step": 46400
    },
    {
      "epoch": 2.858638743455497,
      "grad_norm": 0.1457529366016388,
      "learning_rate": 9.49389179755672e-06,
      "loss": 0.1757,
      "step": 46410
    },
    {
      "epoch": 2.859254696643055,
      "grad_norm": 0.15470686554908752,
      "learning_rate": 9.452828251719537e-06,
      "loss": 0.1749,
      "step": 46420
    },
    {
      "epoch": 2.859870649830613,
      "grad_norm": 0.147264301776886,
      "learning_rate": 9.411764705882354e-06,
      "loss": 0.1742,
      "step": 46430
    },
    {
      "epoch": 2.8604866030181704,
      "grad_norm": 0.17429009079933167,
      "learning_rate": 9.37070116004517e-06,
      "loss": 0.1748,
      "step": 46440
    },
    {
      "epoch": 2.8611025562057284,
      "grad_norm": 0.18352195620536804,
      "learning_rate": 9.329637614207987e-06,
      "loss": 0.1757,
      "step": 46450
    },
    {
      "epoch": 2.861718509393286,
      "grad_norm": 0.15595093369483948,
      "learning_rate": 9.288574068370805e-06,
      "loss": 0.1742,
      "step": 46460
    },
    {
      "epoch": 2.862334462580844,
      "grad_norm": 0.14984455704689026,
      "learning_rate": 9.247510522533622e-06,
      "loss": 0.1747,
      "step": 46470
    },
    {
      "epoch": 2.8629504157684016,
      "grad_norm": 0.16182665526866913,
      "learning_rate": 9.206446976696439e-06,
      "loss": 0.1758,
      "step": 46480
    },
    {
      "epoch": 2.8635663689559596,
      "grad_norm": 0.14336466789245605,
      "learning_rate": 9.165383430859255e-06,
      "loss": 0.1761,
      "step": 46490
    },
    {
      "epoch": 2.864182322143517,
      "grad_norm": 0.12726177275180817,
      "learning_rate": 9.124319885022072e-06,
      "loss": 0.1751,
      "step": 46500
    },
    {
      "epoch": 2.864798275331075,
      "grad_norm": 0.1553283929824829,
      "learning_rate": 9.08325633918489e-06,
      "loss": 0.1743,
      "step": 46510
    },
    {
      "epoch": 2.865414228518633,
      "grad_norm": 0.1521623581647873,
      "learning_rate": 9.042192793347707e-06,
      "loss": 0.1737,
      "step": 46520
    },
    {
      "epoch": 2.8660301817061904,
      "grad_norm": 0.13581040501594543,
      "learning_rate": 9.001129247510524e-06,
      "loss": 0.1738,
      "step": 46530
    },
    {
      "epoch": 2.866646134893748,
      "grad_norm": 0.15745221078395844,
      "learning_rate": 8.96006570167334e-06,
      "loss": 0.1751,
      "step": 46540
    },
    {
      "epoch": 2.867262088081306,
      "grad_norm": 0.1446298062801361,
      "learning_rate": 8.919002155836157e-06,
      "loss": 0.1746,
      "step": 46550
    },
    {
      "epoch": 2.8678780412688636,
      "grad_norm": 0.14823824167251587,
      "learning_rate": 8.877938609998974e-06,
      "loss": 0.1744,
      "step": 46560
    },
    {
      "epoch": 2.868493994456421,
      "grad_norm": 0.16815097630023956,
      "learning_rate": 8.836875064161792e-06,
      "loss": 0.1748,
      "step": 46570
    },
    {
      "epoch": 2.869109947643979,
      "grad_norm": 0.16706807911396027,
      "learning_rate": 8.795811518324609e-06,
      "loss": 0.1753,
      "step": 46580
    },
    {
      "epoch": 2.869725900831537,
      "grad_norm": 0.14469511806964874,
      "learning_rate": 8.754747972487425e-06,
      "loss": 0.1737,
      "step": 46590
    },
    {
      "epoch": 2.8703418540190944,
      "grad_norm": 0.15534380078315735,
      "learning_rate": 8.713684426650242e-06,
      "loss": 0.176,
      "step": 46600
    },
    {
      "epoch": 2.8709578072066524,
      "grad_norm": 0.14691047370433807,
      "learning_rate": 8.672620880813059e-06,
      "loss": 0.1744,
      "step": 46610
    },
    {
      "epoch": 2.87157376039421,
      "grad_norm": 0.1641692966222763,
      "learning_rate": 8.631557334975875e-06,
      "loss": 0.1752,
      "step": 46620
    },
    {
      "epoch": 2.8721897135817676,
      "grad_norm": 0.16140078008174896,
      "learning_rate": 8.590493789138694e-06,
      "loss": 0.1752,
      "step": 46630
    },
    {
      "epoch": 2.8728056667693256,
      "grad_norm": 0.1518113613128662,
      "learning_rate": 8.54943024330151e-06,
      "loss": 0.1742,
      "step": 46640
    },
    {
      "epoch": 2.873421619956883,
      "grad_norm": 0.1680101454257965,
      "learning_rate": 8.508366697464327e-06,
      "loss": 0.175,
      "step": 46650
    },
    {
      "epoch": 2.8740375731444407,
      "grad_norm": 0.1334703117609024,
      "learning_rate": 8.467303151627144e-06,
      "loss": 0.1773,
      "step": 46660
    },
    {
      "epoch": 2.8746535263319988,
      "grad_norm": 0.1557387113571167,
      "learning_rate": 8.42623960578996e-06,
      "loss": 0.1732,
      "step": 46670
    },
    {
      "epoch": 2.8752694795195564,
      "grad_norm": 0.17188584804534912,
      "learning_rate": 8.385176059952777e-06,
      "loss": 0.1752,
      "step": 46680
    },
    {
      "epoch": 2.8758854327071144,
      "grad_norm": 0.1474313586950302,
      "learning_rate": 8.348218868699313e-06,
      "loss": 0.176,
      "step": 46690
    },
    {
      "epoch": 2.876501385894672,
      "grad_norm": 0.17567892372608185,
      "learning_rate": 8.30715532286213e-06,
      "loss": 0.1756,
      "step": 46700
    },
    {
      "epoch": 2.87711733908223,
      "grad_norm": 0.16196447610855103,
      "learning_rate": 8.266091777024946e-06,
      "loss": 0.175,
      "step": 46710
    },
    {
      "epoch": 2.8777332922697876,
      "grad_norm": 0.1447051465511322,
      "learning_rate": 8.225028231187763e-06,
      "loss": 0.1761,
      "step": 46720
    },
    {
      "epoch": 2.878349245457345,
      "grad_norm": 0.1759338229894638,
      "learning_rate": 8.18396468535058e-06,
      "loss": 0.1746,
      "step": 46730
    },
    {
      "epoch": 2.878965198644903,
      "grad_norm": 0.1445128470659256,
      "learning_rate": 8.142901139513398e-06,
      "loss": 0.1753,
      "step": 46740
    },
    {
      "epoch": 2.8795811518324608,
      "grad_norm": 0.1638585478067398,
      "learning_rate": 8.101837593676215e-06,
      "loss": 0.1748,
      "step": 46750
    },
    {
      "epoch": 2.8801971050200184,
      "grad_norm": 0.15620741248130798,
      "learning_rate": 8.060774047839031e-06,
      "loss": 0.1753,
      "step": 46760
    },
    {
      "epoch": 2.8808130582075764,
      "grad_norm": 0.16039088368415833,
      "learning_rate": 8.019710502001848e-06,
      "loss": 0.1747,
      "step": 46770
    },
    {
      "epoch": 2.881429011395134,
      "grad_norm": 0.14875181019306183,
      "learning_rate": 7.978646956164665e-06,
      "loss": 0.1751,
      "step": 46780
    },
    {
      "epoch": 2.8820449645826915,
      "grad_norm": 0.16356579959392548,
      "learning_rate": 7.937583410327481e-06,
      "loss": 0.1759,
      "step": 46790
    },
    {
      "epoch": 2.8826609177702496,
      "grad_norm": 0.14587950706481934,
      "learning_rate": 7.8965198644903e-06,
      "loss": 0.1744,
      "step": 46800
    },
    {
      "epoch": 2.883276870957807,
      "grad_norm": 0.15594615042209625,
      "learning_rate": 7.855456318653116e-06,
      "loss": 0.1763,
      "step": 46810
    },
    {
      "epoch": 2.8838928241453647,
      "grad_norm": 0.14441895484924316,
      "learning_rate": 7.814392772815933e-06,
      "loss": 0.1749,
      "step": 46820
    },
    {
      "epoch": 2.8845087773329228,
      "grad_norm": 0.1386708766222,
      "learning_rate": 7.77332922697875e-06,
      "loss": 0.1739,
      "step": 46830
    },
    {
      "epoch": 2.8851247305204804,
      "grad_norm": 0.15119104087352753,
      "learning_rate": 7.732265681141566e-06,
      "loss": 0.1754,
      "step": 46840
    },
    {
      "epoch": 2.885740683708038,
      "grad_norm": 0.1652262657880783,
      "learning_rate": 7.691202135304385e-06,
      "loss": 0.1757,
      "step": 46850
    },
    {
      "epoch": 2.886356636895596,
      "grad_norm": 0.1693708747625351,
      "learning_rate": 7.650138589467201e-06,
      "loss": 0.176,
      "step": 46860
    },
    {
      "epoch": 2.8869725900831535,
      "grad_norm": 0.16113877296447754,
      "learning_rate": 7.609075043630018e-06,
      "loss": 0.1765,
      "step": 46870
    },
    {
      "epoch": 2.8875885432707116,
      "grad_norm": 0.13346977531909943,
      "learning_rate": 7.568011497792835e-06,
      "loss": 0.1742,
      "step": 46880
    },
    {
      "epoch": 2.888204496458269,
      "grad_norm": 0.15616559982299805,
      "learning_rate": 7.526947951955651e-06,
      "loss": 0.1742,
      "step": 46890
    },
    {
      "epoch": 2.888820449645827,
      "grad_norm": 0.1674695611000061,
      "learning_rate": 7.48588440611847e-06,
      "loss": 0.175,
      "step": 46900
    },
    {
      "epoch": 2.8894364028333848,
      "grad_norm": 0.13987502455711365,
      "learning_rate": 7.444820860281286e-06,
      "loss": 0.1754,
      "step": 46910
    },
    {
      "epoch": 2.8900523560209423,
      "grad_norm": 0.14933542907238007,
      "learning_rate": 7.403757314444103e-06,
      "loss": 0.1756,
      "step": 46920
    },
    {
      "epoch": 2.8906683092085004,
      "grad_norm": 0.14896707236766815,
      "learning_rate": 7.3626937686069195e-06,
      "loss": 0.1762,
      "step": 46930
    },
    {
      "epoch": 2.891284262396058,
      "grad_norm": 0.16441930830478668,
      "learning_rate": 7.321630222769736e-06,
      "loss": 0.1755,
      "step": 46940
    },
    {
      "epoch": 2.8919002155836155,
      "grad_norm": 0.14722074568271637,
      "learning_rate": 7.280566676932553e-06,
      "loss": 0.1761,
      "step": 46950
    },
    {
      "epoch": 2.8925161687711736,
      "grad_norm": 0.14074663817882538,
      "learning_rate": 7.239503131095371e-06,
      "loss": 0.1758,
      "step": 46960
    },
    {
      "epoch": 2.893132121958731,
      "grad_norm": 0.14574171602725983,
      "learning_rate": 7.198439585258188e-06,
      "loss": 0.1757,
      "step": 46970
    },
    {
      "epoch": 2.8937480751462887,
      "grad_norm": 0.14780454337596893,
      "learning_rate": 7.1573760394210045e-06,
      "loss": 0.1749,
      "step": 46980
    },
    {
      "epoch": 2.8943640283338468,
      "grad_norm": 0.1523042768239975,
      "learning_rate": 7.116312493583821e-06,
      "loss": 0.1758,
      "step": 46990
    },
    {
      "epoch": 2.8949799815214043,
      "grad_norm": 0.18954019248485565,
      "learning_rate": 7.075248947746638e-06,
      "loss": 0.1761,
      "step": 47000
    },
    {
      "epoch": 2.895595934708962,
      "grad_norm": 0.1564820408821106,
      "learning_rate": 7.034185401909456e-06,
      "loss": 0.1755,
      "step": 47010
    },
    {
      "epoch": 2.89621188789652,
      "grad_norm": 0.18579235672950745,
      "learning_rate": 6.993121856072273e-06,
      "loss": 0.1751,
      "step": 47020
    },
    {
      "epoch": 2.8968278410840775,
      "grad_norm": 0.1503463089466095,
      "learning_rate": 6.952058310235089e-06,
      "loss": 0.1744,
      "step": 47030
    },
    {
      "epoch": 2.897443794271635,
      "grad_norm": 0.1586371511220932,
      "learning_rate": 6.910994764397906e-06,
      "loss": 0.1756,
      "step": 47040
    },
    {
      "epoch": 2.898059747459193,
      "grad_norm": 0.1475217193365097,
      "learning_rate": 6.869931218560723e-06,
      "loss": 0.1751,
      "step": 47050
    },
    {
      "epoch": 2.8986757006467507,
      "grad_norm": 0.14499016106128693,
      "learning_rate": 6.828867672723541e-06,
      "loss": 0.1738,
      "step": 47060
    },
    {
      "epoch": 2.8992916538343088,
      "grad_norm": 0.16370828449726105,
      "learning_rate": 6.787804126886358e-06,
      "loss": 0.1756,
      "step": 47070
    },
    {
      "epoch": 2.8999076070218663,
      "grad_norm": 0.16216795146465302,
      "learning_rate": 6.746740581049174e-06,
      "loss": 0.175,
      "step": 47080
    },
    {
      "epoch": 2.900523560209424,
      "grad_norm": 0.15849387645721436,
      "learning_rate": 6.705677035211991e-06,
      "loss": 0.1747,
      "step": 47090
    },
    {
      "epoch": 2.901139513396982,
      "grad_norm": 0.15131843090057373,
      "learning_rate": 6.664613489374808e-06,
      "loss": 0.1755,
      "step": 47100
    },
    {
      "epoch": 2.9017554665845395,
      "grad_norm": 0.1493905484676361,
      "learning_rate": 6.623549943537624e-06,
      "loss": 0.1743,
      "step": 47110
    },
    {
      "epoch": 2.9023714197720976,
      "grad_norm": 0.1478637307882309,
      "learning_rate": 6.582486397700443e-06,
      "loss": 0.1763,
      "step": 47120
    },
    {
      "epoch": 2.902987372959655,
      "grad_norm": 0.1504574716091156,
      "learning_rate": 6.541422851863259e-06,
      "loss": 0.1758,
      "step": 47130
    },
    {
      "epoch": 2.9036033261472127,
      "grad_norm": 0.1550094485282898,
      "learning_rate": 6.500359306026076e-06,
      "loss": 0.1745,
      "step": 47140
    },
    {
      "epoch": 2.9042192793347708,
      "grad_norm": 0.14506720006465912,
      "learning_rate": 6.459295760188893e-06,
      "loss": 0.1766,
      "step": 47150
    },
    {
      "epoch": 2.9048352325223283,
      "grad_norm": 0.15615402162075043,
      "learning_rate": 6.418232214351709e-06,
      "loss": 0.1748,
      "step": 47160
    },
    {
      "epoch": 2.905451185709886,
      "grad_norm": 0.1561259627342224,
      "learning_rate": 6.377168668514527e-06,
      "loss": 0.1754,
      "step": 47170
    },
    {
      "epoch": 2.906067138897444,
      "grad_norm": 0.17374484241008759,
      "learning_rate": 6.336105122677343e-06,
      "loss": 0.1774,
      "step": 47180
    },
    {
      "epoch": 2.9066830920850015,
      "grad_norm": 0.1497955471277237,
      "learning_rate": 6.295041576840161e-06,
      "loss": 0.175,
      "step": 47190
    },
    {
      "epoch": 2.907299045272559,
      "grad_norm": 0.13628169894218445,
      "learning_rate": 6.2539780310029775e-06,
      "loss": 0.1739,
      "step": 47200
    },
    {
      "epoch": 2.907914998460117,
      "grad_norm": 0.15004761517047882,
      "learning_rate": 6.212914485165794e-06,
      "loss": 0.176,
      "step": 47210
    },
    {
      "epoch": 2.9085309516476747,
      "grad_norm": 0.14360779523849487,
      "learning_rate": 6.171850939328612e-06,
      "loss": 0.1751,
      "step": 47220
    },
    {
      "epoch": 2.9091469048352323,
      "grad_norm": 0.1352565586566925,
      "learning_rate": 6.130787393491428e-06,
      "loss": 0.1751,
      "step": 47230
    },
    {
      "epoch": 2.9097628580227903,
      "grad_norm": 0.13368110358715057,
      "learning_rate": 6.089723847654245e-06,
      "loss": 0.174,
      "step": 47240
    },
    {
      "epoch": 2.910378811210348,
      "grad_norm": 0.1631641387939453,
      "learning_rate": 6.0486603018170625e-06,
      "loss": 0.1765,
      "step": 47250
    },
    {
      "epoch": 2.9109947643979055,
      "grad_norm": 0.15620554983615875,
      "learning_rate": 6.007596755979879e-06,
      "loss": 0.176,
      "step": 47260
    },
    {
      "epoch": 2.9116107175854635,
      "grad_norm": 0.14734672009944916,
      "learning_rate": 5.966533210142696e-06,
      "loss": 0.1756,
      "step": 47270
    },
    {
      "epoch": 2.912226670773021,
      "grad_norm": 0.1543974131345749,
      "learning_rate": 5.925469664305512e-06,
      "loss": 0.1754,
      "step": 47280
    },
    {
      "epoch": 2.912842623960579,
      "grad_norm": 0.16530905663967133,
      "learning_rate": 5.88440611846833e-06,
      "loss": 0.1755,
      "step": 47290
    },
    {
      "epoch": 2.9134585771481367,
      "grad_norm": 0.14564847946166992,
      "learning_rate": 5.8433425726311466e-06,
      "loss": 0.1746,
      "step": 47300
    },
    {
      "epoch": 2.9140745303356947,
      "grad_norm": 0.15358884632587433,
      "learning_rate": 5.802279026793963e-06,
      "loss": 0.1744,
      "step": 47310
    },
    {
      "epoch": 2.9146904835232523,
      "grad_norm": 0.15597842633724213,
      "learning_rate": 5.761215480956781e-06,
      "loss": 0.1755,
      "step": 47320
    },
    {
      "epoch": 2.91530643671081,
      "grad_norm": 0.15602970123291016,
      "learning_rate": 5.720151935119597e-06,
      "loss": 0.1759,
      "step": 47330
    },
    {
      "epoch": 2.915922389898368,
      "grad_norm": 0.15374645590782166,
      "learning_rate": 5.679088389282415e-06,
      "loss": 0.1755,
      "step": 47340
    },
    {
      "epoch": 2.9165383430859255,
      "grad_norm": 0.13587354123592377,
      "learning_rate": 5.6380248434452315e-06,
      "loss": 0.1748,
      "step": 47350
    },
    {
      "epoch": 2.917154296273483,
      "grad_norm": 0.14726606011390686,
      "learning_rate": 5.596961297608048e-06,
      "loss": 0.1751,
      "step": 47360
    },
    {
      "epoch": 2.917770249461041,
      "grad_norm": 0.15051382780075073,
      "learning_rate": 5.555897751770866e-06,
      "loss": 0.1752,
      "step": 47370
    },
    {
      "epoch": 2.9183862026485987,
      "grad_norm": 0.15822571516036987,
      "learning_rate": 5.514834205933682e-06,
      "loss": 0.175,
      "step": 47380
    },
    {
      "epoch": 2.9190021558361563,
      "grad_norm": 0.14327360689640045,
      "learning_rate": 5.473770660096499e-06,
      "loss": 0.1754,
      "step": 47390
    },
    {
      "epoch": 2.9196181090237143,
      "grad_norm": 0.1649523824453354,
      "learning_rate": 5.4327071142593165e-06,
      "loss": 0.1759,
      "step": 47400
    },
    {
      "epoch": 2.920234062211272,
      "grad_norm": 0.14695289731025696,
      "learning_rate": 5.391643568422133e-06,
      "loss": 0.1754,
      "step": 47410
    },
    {
      "epoch": 2.9208500153988295,
      "grad_norm": 0.16623470187187195,
      "learning_rate": 5.350580022584951e-06,
      "loss": 0.1763,
      "step": 47420
    },
    {
      "epoch": 2.9214659685863875,
      "grad_norm": 0.16960608959197998,
      "learning_rate": 5.309516476747767e-06,
      "loss": 0.1754,
      "step": 47430
    },
    {
      "epoch": 2.922081921773945,
      "grad_norm": 0.14432469010353088,
      "learning_rate": 5.268452930910584e-06,
      "loss": 0.1751,
      "step": 47440
    },
    {
      "epoch": 2.9226978749615027,
      "grad_norm": 0.16063031554222107,
      "learning_rate": 5.227389385073401e-06,
      "loss": 0.1752,
      "step": 47450
    },
    {
      "epoch": 2.9233138281490607,
      "grad_norm": 0.16486284136772156,
      "learning_rate": 5.186325839236218e-06,
      "loss": 0.176,
      "step": 47460
    },
    {
      "epoch": 2.9239297813366183,
      "grad_norm": 0.15203379094600677,
      "learning_rate": 5.145262293399035e-06,
      "loss": 0.1753,
      "step": 47470
    },
    {
      "epoch": 2.9245457345241763,
      "grad_norm": 0.15872721374034882,
      "learning_rate": 5.104198747561852e-06,
      "loss": 0.1761,
      "step": 47480
    },
    {
      "epoch": 2.925161687711734,
      "grad_norm": 0.158762127161026,
      "learning_rate": 5.063135201724669e-06,
      "loss": 0.1758,
      "step": 47490
    },
    {
      "epoch": 2.9257776408992915,
      "grad_norm": 0.15173472464084625,
      "learning_rate": 5.022071655887486e-06,
      "loss": 0.1749,
      "step": 47500
    },
    {
      "epoch": 2.9263935940868495,
      "grad_norm": 0.14806942641735077,
      "learning_rate": 4.981008110050303e-06,
      "loss": 0.175,
      "step": 47510
    },
    {
      "epoch": 2.927009547274407,
      "grad_norm": 0.15551894903182983,
      "learning_rate": 4.93994456421312e-06,
      "loss": 0.1761,
      "step": 47520
    },
    {
      "epoch": 2.927625500461965,
      "grad_norm": 0.15535292029380798,
      "learning_rate": 4.898881018375937e-06,
      "loss": 0.176,
      "step": 47530
    },
    {
      "epoch": 2.9282414536495227,
      "grad_norm": 0.1487734466791153,
      "learning_rate": 4.857817472538754e-06,
      "loss": 0.1756,
      "step": 47540
    },
    {
      "epoch": 2.9288574068370803,
      "grad_norm": 0.16843274235725403,
      "learning_rate": 4.8167539267015704e-06,
      "loss": 0.1753,
      "step": 47550
    },
    {
      "epoch": 2.9294733600246383,
      "grad_norm": 0.16064006090164185,
      "learning_rate": 4.775690380864388e-06,
      "loss": 0.1735,
      "step": 47560
    },
    {
      "epoch": 2.930089313212196,
      "grad_norm": 0.17641016840934753,
      "learning_rate": 4.7346268350272046e-06,
      "loss": 0.1753,
      "step": 47570
    },
    {
      "epoch": 2.9307052663997535,
      "grad_norm": 0.1429729014635086,
      "learning_rate": 4.693563289190022e-06,
      "loss": 0.1734,
      "step": 47580
    },
    {
      "epoch": 2.9313212195873115,
      "grad_norm": 0.13350264728069305,
      "learning_rate": 4.652499743352839e-06,
      "loss": 0.1752,
      "step": 47590
    },
    {
      "epoch": 2.931937172774869,
      "grad_norm": 0.14372289180755615,
      "learning_rate": 4.611436197515655e-06,
      "loss": 0.1751,
      "step": 47600
    },
    {
      "epoch": 2.9325531259624267,
      "grad_norm": 0.15937066078186035,
      "learning_rate": 4.570372651678473e-06,
      "loss": 0.1755,
      "step": 47610
    },
    {
      "epoch": 2.9331690791499847,
      "grad_norm": 0.14185777306556702,
      "learning_rate": 4.5293091058412895e-06,
      "loss": 0.1755,
      "step": 47620
    },
    {
      "epoch": 2.9337850323375423,
      "grad_norm": 0.17419345676898956,
      "learning_rate": 4.488245560004106e-06,
      "loss": 0.175,
      "step": 47630
    },
    {
      "epoch": 2.9344009855251,
      "grad_norm": 0.14289076626300812,
      "learning_rate": 4.447182014166924e-06,
      "loss": 0.1752,
      "step": 47640
    },
    {
      "epoch": 2.935016938712658,
      "grad_norm": 0.16799116134643555,
      "learning_rate": 4.40611846832974e-06,
      "loss": 0.175,
      "step": 47650
    },
    {
      "epoch": 2.9356328919002155,
      "grad_norm": 0.15738137066364288,
      "learning_rate": 4.365054922492558e-06,
      "loss": 0.1759,
      "step": 47660
    },
    {
      "epoch": 2.936248845087773,
      "grad_norm": 0.15997883677482605,
      "learning_rate": 4.3239913766553745e-06,
      "loss": 0.1747,
      "step": 47670
    },
    {
      "epoch": 2.936864798275331,
      "grad_norm": 0.1643553078174591,
      "learning_rate": 4.282927830818191e-06,
      "loss": 0.1757,
      "step": 47680
    },
    {
      "epoch": 2.9374807514628887,
      "grad_norm": 0.17192158102989197,
      "learning_rate": 4.241864284981009e-06,
      "loss": 0.1753,
      "step": 47690
    },
    {
      "epoch": 2.9380967046504467,
      "grad_norm": 0.1503887176513672,
      "learning_rate": 4.200800739143825e-06,
      "loss": 0.1747,
      "step": 47700
    },
    {
      "epoch": 2.9387126578380043,
      "grad_norm": 0.16687077283859253,
      "learning_rate": 4.159737193306642e-06,
      "loss": 0.1763,
      "step": 47710
    },
    {
      "epoch": 2.9393286110255623,
      "grad_norm": 0.14487440884113312,
      "learning_rate": 4.118673647469459e-06,
      "loss": 0.1753,
      "step": 47720
    },
    {
      "epoch": 2.93994456421312,
      "grad_norm": 0.14522358775138855,
      "learning_rate": 4.077610101632276e-06,
      "loss": 0.1743,
      "step": 47730
    },
    {
      "epoch": 2.9405605174006775,
      "grad_norm": 0.13939720392227173,
      "learning_rate": 4.0365465557950935e-06,
      "loss": 0.1758,
      "step": 47740
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 0.14745675027370453,
      "learning_rate": 3.99548300995791e-06,
      "loss": 0.1745,
      "step": 47750
    },
    {
      "epoch": 2.941792423775793,
      "grad_norm": 0.1532200276851654,
      "learning_rate": 3.954419464120727e-06,
      "loss": 0.1752,
      "step": 47760
    },
    {
      "epoch": 2.9424083769633507,
      "grad_norm": 0.1621510535478592,
      "learning_rate": 3.913355918283544e-06,
      "loss": 0.1754,
      "step": 47770
    },
    {
      "epoch": 2.9430243301509087,
      "grad_norm": 0.14551027119159698,
      "learning_rate": 3.872292372446361e-06,
      "loss": 0.1758,
      "step": 47780
    },
    {
      "epoch": 2.9436402833384663,
      "grad_norm": 0.15304188430309296,
      "learning_rate": 3.8312288266091785e-06,
      "loss": 0.1745,
      "step": 47790
    },
    {
      "epoch": 2.944256236526024,
      "grad_norm": 0.15128707885742188,
      "learning_rate": 3.790165280771995e-06,
      "loss": 0.1753,
      "step": 47800
    },
    {
      "epoch": 2.944872189713582,
      "grad_norm": 0.13733552396297455,
      "learning_rate": 3.7491017349348118e-06,
      "loss": 0.1753,
      "step": 47810
    },
    {
      "epoch": 2.9454881429011395,
      "grad_norm": 0.17354974150657654,
      "learning_rate": 3.708038189097629e-06,
      "loss": 0.1757,
      "step": 47820
    },
    {
      "epoch": 2.946104096088697,
      "grad_norm": 0.1714949756860733,
      "learning_rate": 3.6669746432604455e-06,
      "loss": 0.1753,
      "step": 47830
    },
    {
      "epoch": 2.946720049276255,
      "grad_norm": 0.16169382631778717,
      "learning_rate": 3.6259110974232626e-06,
      "loss": 0.1756,
      "step": 47840
    },
    {
      "epoch": 2.9473360024638127,
      "grad_norm": 0.17160114645957947,
      "learning_rate": 3.5848475515860797e-06,
      "loss": 0.1758,
      "step": 47850
    },
    {
      "epoch": 2.9479519556513702,
      "grad_norm": 0.15060169994831085,
      "learning_rate": 3.5437840057488963e-06,
      "loss": 0.1752,
      "step": 47860
    },
    {
      "epoch": 2.9485679088389283,
      "grad_norm": 0.16035126149654388,
      "learning_rate": 3.502720459911714e-06,
      "loss": 0.1762,
      "step": 47870
    },
    {
      "epoch": 2.949183862026486,
      "grad_norm": 0.14902184903621674,
      "learning_rate": 3.4616569140745304e-06,
      "loss": 0.1746,
      "step": 47880
    },
    {
      "epoch": 2.949799815214044,
      "grad_norm": 0.16011910140514374,
      "learning_rate": 3.420593368237347e-06,
      "loss": 0.1739,
      "step": 47890
    },
    {
      "epoch": 2.9504157684016015,
      "grad_norm": 0.18544700741767883,
      "learning_rate": 3.3795298224001646e-06,
      "loss": 0.1751,
      "step": 47900
    },
    {
      "epoch": 2.9510317215891595,
      "grad_norm": 0.15674236416816711,
      "learning_rate": 3.3384662765629812e-06,
      "loss": 0.1748,
      "step": 47910
    },
    {
      "epoch": 2.951647674776717,
      "grad_norm": 0.15985709428787231,
      "learning_rate": 3.297402730725798e-06,
      "loss": 0.175,
      "step": 47920
    },
    {
      "epoch": 2.9522636279642747,
      "grad_norm": 0.14676137268543243,
      "learning_rate": 3.2563391848886154e-06,
      "loss": 0.1748,
      "step": 47930
    },
    {
      "epoch": 2.9528795811518327,
      "grad_norm": 0.15455399453639984,
      "learning_rate": 3.215275639051432e-06,
      "loss": 0.1736,
      "step": 47940
    },
    {
      "epoch": 2.9534955343393903,
      "grad_norm": 0.16581101715564728,
      "learning_rate": 3.1742120932142495e-06,
      "loss": 0.1757,
      "step": 47950
    },
    {
      "epoch": 2.954111487526948,
      "grad_norm": 0.1326993852853775,
      "learning_rate": 3.133148547377066e-06,
      "loss": 0.1753,
      "step": 47960
    },
    {
      "epoch": 2.954727440714506,
      "grad_norm": 0.14711609482765198,
      "learning_rate": 3.0920850015398833e-06,
      "loss": 0.1766,
      "step": 47970
    },
    {
      "epoch": 2.9553433939020635,
      "grad_norm": 0.17231450974941254,
      "learning_rate": 3.0510214557027e-06,
      "loss": 0.1741,
      "step": 47980
    },
    {
      "epoch": 2.955959347089621,
      "grad_norm": 0.14757314324378967,
      "learning_rate": 3.009957909865517e-06,
      "loss": 0.1755,
      "step": 47990
    },
    {
      "epoch": 2.956575300277179,
      "grad_norm": 0.13419334590435028,
      "learning_rate": 2.968894364028334e-06,
      "loss": 0.1752,
      "step": 48000
    },
    {
      "epoch": 2.9571912534647367,
      "grad_norm": 0.14296503365039825,
      "learning_rate": 2.927830818191151e-06,
      "loss": 0.1736,
      "step": 48010
    },
    {
      "epoch": 2.9578072066522942,
      "grad_norm": 0.13714754581451416,
      "learning_rate": 2.8867672723539678e-06,
      "loss": 0.1748,
      "step": 48020
    },
    {
      "epoch": 2.9584231598398523,
      "grad_norm": 0.14538219571113586,
      "learning_rate": 2.845703726516785e-06,
      "loss": 0.1749,
      "step": 48030
    },
    {
      "epoch": 2.95903911302741,
      "grad_norm": 0.13647684454917908,
      "learning_rate": 2.804640180679602e-06,
      "loss": 0.1744,
      "step": 48040
    },
    {
      "epoch": 2.9596550662149674,
      "grad_norm": 0.1420404016971588,
      "learning_rate": 2.763576634842419e-06,
      "loss": 0.1746,
      "step": 48050
    },
    {
      "epoch": 2.9602710194025255,
      "grad_norm": 0.1295545995235443,
      "learning_rate": 2.7225130890052356e-06,
      "loss": 0.1745,
      "step": 48060
    },
    {
      "epoch": 2.960886972590083,
      "grad_norm": 0.15237966179847717,
      "learning_rate": 2.6814495431680527e-06,
      "loss": 0.1749,
      "step": 48070
    },
    {
      "epoch": 2.9615029257776406,
      "grad_norm": 0.15596550703048706,
      "learning_rate": 2.64038599733087e-06,
      "loss": 0.1754,
      "step": 48080
    },
    {
      "epoch": 2.9621188789651987,
      "grad_norm": 0.15493857860565186,
      "learning_rate": 2.599322451493687e-06,
      "loss": 0.1763,
      "step": 48090
    },
    {
      "epoch": 2.9627348321527562,
      "grad_norm": 0.15287432074546814,
      "learning_rate": 2.5582589056565035e-06,
      "loss": 0.1758,
      "step": 48100
    },
    {
      "epoch": 2.9633507853403143,
      "grad_norm": 0.1476539820432663,
      "learning_rate": 2.5171953598193206e-06,
      "loss": 0.1744,
      "step": 48110
    },
    {
      "epoch": 2.963966738527872,
      "grad_norm": 0.1626543551683426,
      "learning_rate": 2.4761318139821377e-06,
      "loss": 0.1737,
      "step": 48120
    },
    {
      "epoch": 2.96458269171543,
      "grad_norm": 0.1713712364435196,
      "learning_rate": 2.4350682681449547e-06,
      "loss": 0.1749,
      "step": 48130
    },
    {
      "epoch": 2.9651986449029875,
      "grad_norm": 0.13653318583965302,
      "learning_rate": 2.3940047223077714e-06,
      "loss": 0.1749,
      "step": 48140
    },
    {
      "epoch": 2.965814598090545,
      "grad_norm": 0.1597023904323578,
      "learning_rate": 2.3529411764705885e-06,
      "loss": 0.1753,
      "step": 48150
    },
    {
      "epoch": 2.966430551278103,
      "grad_norm": 0.155809223651886,
      "learning_rate": 2.3118776306334055e-06,
      "loss": 0.1742,
      "step": 48160
    },
    {
      "epoch": 2.9670465044656607,
      "grad_norm": 0.14389322698116302,
      "learning_rate": 2.2708140847962226e-06,
      "loss": 0.1759,
      "step": 48170
    },
    {
      "epoch": 2.9676624576532182,
      "grad_norm": 0.1271405965089798,
      "learning_rate": 2.2297505389590392e-06,
      "loss": 0.1738,
      "step": 48180
    },
    {
      "epoch": 2.9682784108407763,
      "grad_norm": 0.12922579050064087,
      "learning_rate": 2.1886869931218563e-06,
      "loss": 0.1746,
      "step": 48190
    },
    {
      "epoch": 2.968894364028334,
      "grad_norm": 0.15285535156726837,
      "learning_rate": 2.1476234472846734e-06,
      "loss": 0.1747,
      "step": 48200
    },
    {
      "epoch": 2.9695103172158914,
      "grad_norm": 0.14612428843975067,
      "learning_rate": 2.10655990144749e-06,
      "loss": 0.1775,
      "step": 48210
    },
    {
      "epoch": 2.9701262704034495,
      "grad_norm": 0.14260943233966827,
      "learning_rate": 2.065496355610307e-06,
      "loss": 0.1746,
      "step": 48220
    },
    {
      "epoch": 2.970742223591007,
      "grad_norm": 0.14083059132099152,
      "learning_rate": 2.0244328097731238e-06,
      "loss": 0.1743,
      "step": 48230
    },
    {
      "epoch": 2.9713581767785646,
      "grad_norm": 0.17389962077140808,
      "learning_rate": 1.983369263935941e-06,
      "loss": 0.175,
      "step": 48240
    },
    {
      "epoch": 2.9719741299661226,
      "grad_norm": 0.16797712445259094,
      "learning_rate": 1.942305718098758e-06,
      "loss": 0.1753,
      "step": 48250
    },
    {
      "epoch": 2.9725900831536802,
      "grad_norm": 0.17600177228450775,
      "learning_rate": 1.9012421722615748e-06,
      "loss": 0.1746,
      "step": 48260
    },
    {
      "epoch": 2.973206036341238,
      "grad_norm": 0.14632390439510345,
      "learning_rate": 1.8601786264243918e-06,
      "loss": 0.1747,
      "step": 48270
    },
    {
      "epoch": 2.973821989528796,
      "grad_norm": 0.1594030112028122,
      "learning_rate": 1.819115080587209e-06,
      "loss": 0.1749,
      "step": 48280
    },
    {
      "epoch": 2.9744379427163534,
      "grad_norm": 0.1384885311126709,
      "learning_rate": 1.7780515347500258e-06,
      "loss": 0.1741,
      "step": 48290
    },
    {
      "epoch": 2.9750538959039114,
      "grad_norm": 0.1354471743106842,
      "learning_rate": 1.7369879889128426e-06,
      "loss": 0.175,
      "step": 48300
    },
    {
      "epoch": 2.975669849091469,
      "grad_norm": 0.1410149186849594,
      "learning_rate": 1.6959244430756597e-06,
      "loss": 0.1748,
      "step": 48310
    },
    {
      "epoch": 2.976285802279027,
      "grad_norm": 0.14436356723308563,
      "learning_rate": 1.6548608972384766e-06,
      "loss": 0.1752,
      "step": 48320
    },
    {
      "epoch": 2.9769017554665846,
      "grad_norm": 0.14389045536518097,
      "learning_rate": 1.6137973514012936e-06,
      "loss": 0.1736,
      "step": 48330
    },
    {
      "epoch": 2.9775177086541422,
      "grad_norm": 0.15199576318264008,
      "learning_rate": 1.5727338055641103e-06,
      "loss": 0.1761,
      "step": 48340
    },
    {
      "epoch": 2.9781336618417003,
      "grad_norm": 0.14553385972976685,
      "learning_rate": 1.5316702597269274e-06,
      "loss": 0.1756,
      "step": 48350
    },
    {
      "epoch": 2.978749615029258,
      "grad_norm": 0.17803294956684113,
      "learning_rate": 1.4906067138897444e-06,
      "loss": 0.1752,
      "step": 48360
    },
    {
      "epoch": 2.9793655682168154,
      "grad_norm": 0.13508723676204681,
      "learning_rate": 1.4495431680525613e-06,
      "loss": 0.1755,
      "step": 48370
    },
    {
      "epoch": 2.9799815214043734,
      "grad_norm": 0.15397129952907562,
      "learning_rate": 1.4084796222153784e-06,
      "loss": 0.1758,
      "step": 48380
    },
    {
      "epoch": 2.980597474591931,
      "grad_norm": 0.14130009710788727,
      "learning_rate": 1.3674160763781952e-06,
      "loss": 0.1748,
      "step": 48390
    },
    {
      "epoch": 2.9812134277794886,
      "grad_norm": 0.14220528304576874,
      "learning_rate": 1.3263525305410123e-06,
      "loss": 0.175,
      "step": 48400
    },
    {
      "epoch": 2.9818293809670466,
      "grad_norm": 0.16347438097000122,
      "learning_rate": 1.2852889847038292e-06,
      "loss": 0.1747,
      "step": 48410
    },
    {
      "epoch": 2.982445334154604,
      "grad_norm": 0.13070492446422577,
      "learning_rate": 1.2442254388666462e-06,
      "loss": 0.1763,
      "step": 48420
    },
    {
      "epoch": 2.983061287342162,
      "grad_norm": 0.14953066408634186,
      "learning_rate": 1.2031618930294631e-06,
      "loss": 0.1737,
      "step": 48430
    },
    {
      "epoch": 2.98367724052972,
      "grad_norm": 0.13611960411071777,
      "learning_rate": 1.1620983471922802e-06,
      "loss": 0.174,
      "step": 48440
    },
    {
      "epoch": 2.9842931937172774,
      "grad_norm": 0.1455146223306656,
      "learning_rate": 1.121034801355097e-06,
      "loss": 0.1752,
      "step": 48450
    },
    {
      "epoch": 2.984909146904835,
      "grad_norm": 0.14940859377384186,
      "learning_rate": 1.0799712555179141e-06,
      "loss": 0.1745,
      "step": 48460
    },
    {
      "epoch": 2.985525100092393,
      "grad_norm": 0.1442696899175644,
      "learning_rate": 1.038907709680731e-06,
      "loss": 0.175,
      "step": 48470
    },
    {
      "epoch": 2.9861410532799506,
      "grad_norm": 0.1520506590604782,
      "learning_rate": 9.97844163843548e-07,
      "loss": 0.1747,
      "step": 48480
    },
    {
      "epoch": 2.9867570064675086,
      "grad_norm": 0.16623209416866302,
      "learning_rate": 9.56780618006365e-07,
      "loss": 0.1744,
      "step": 48490
    },
    {
      "epoch": 2.987372959655066,
      "grad_norm": 0.1480473428964615,
      "learning_rate": 9.157170721691819e-07,
      "loss": 0.1742,
      "step": 48500
    },
    {
      "epoch": 2.987988912842624,
      "grad_norm": 0.15448595583438873,
      "learning_rate": 8.746535263319987e-07,
      "loss": 0.1742,
      "step": 48510
    },
    {
      "epoch": 2.988604866030182,
      "grad_norm": 0.1593835949897766,
      "learning_rate": 8.335899804948158e-07,
      "loss": 0.1742,
      "step": 48520
    },
    {
      "epoch": 2.9892208192177394,
      "grad_norm": 0.15572524070739746,
      "learning_rate": 7.925264346576327e-07,
      "loss": 0.1755,
      "step": 48530
    },
    {
      "epoch": 2.9898367724052974,
      "grad_norm": 0.14163599908351898,
      "learning_rate": 7.514628888204497e-07,
      "loss": 0.1749,
      "step": 48540
    },
    {
      "epoch": 2.990452725592855,
      "grad_norm": 0.16167108714580536,
      "learning_rate": 7.103993429832667e-07,
      "loss": 0.1753,
      "step": 48550
    },
    {
      "epoch": 2.9910686787804126,
      "grad_norm": 0.15283988416194916,
      "learning_rate": 6.693357971460836e-07,
      "loss": 0.1749,
      "step": 48560
    },
    {
      "epoch": 2.9916846319679706,
      "grad_norm": 0.13658303022384644,
      "learning_rate": 6.282722513089005e-07,
      "loss": 0.1761,
      "step": 48570
    },
    {
      "epoch": 2.992300585155528,
      "grad_norm": 0.16909447312355042,
      "learning_rate": 5.872087054717175e-07,
      "loss": 0.176,
      "step": 48580
    },
    {
      "epoch": 2.992916538343086,
      "grad_norm": 0.14216329157352448,
      "learning_rate": 5.461451596345345e-07,
      "loss": 0.1749,
      "step": 48590
    },
    {
      "epoch": 2.993532491530644,
      "grad_norm": 0.16448858380317688,
      "learning_rate": 5.050816137973514e-07,
      "loss": 0.1751,
      "step": 48600
    },
    {
      "epoch": 2.9941484447182014,
      "grad_norm": 0.14717449247837067,
      "learning_rate": 4.640180679601684e-07,
      "loss": 0.175,
      "step": 48610
    },
    {
      "epoch": 2.994764397905759,
      "grad_norm": 0.13984808325767517,
      "learning_rate": 4.229545221229854e-07,
      "loss": 0.1744,
      "step": 48620
    },
    {
      "epoch": 2.995380351093317,
      "grad_norm": 0.16400963068008423,
      "learning_rate": 3.818909762858023e-07,
      "loss": 0.1757,
      "step": 48630
    },
    {
      "epoch": 2.9959963042808746,
      "grad_norm": 0.14907078444957733,
      "learning_rate": 3.4082743044861926e-07,
      "loss": 0.175,
      "step": 48640
    },
    {
      "epoch": 2.996612257468432,
      "grad_norm": 0.15238738059997559,
      "learning_rate": 2.997638846114362e-07,
      "loss": 0.1758,
      "step": 48650
    },
    {
      "epoch": 2.99722821065599,
      "grad_norm": 0.13754543662071228,
      "learning_rate": 2.587003387742532e-07,
      "loss": 0.1764,
      "step": 48660
    },
    {
      "epoch": 2.997844163843548,
      "grad_norm": 0.16794413328170776,
      "learning_rate": 2.1763679293707013e-07,
      "loss": 0.1745,
      "step": 48670
    },
    {
      "epoch": 2.9984601170311054,
      "grad_norm": 0.1350601464509964,
      "learning_rate": 1.7657324709988707e-07,
      "loss": 0.1751,
      "step": 48680
    },
    {
      "epoch": 2.9990760702186634,
      "grad_norm": 0.15020686388015747,
      "learning_rate": 1.3550970126270404e-07,
      "loss": 0.175,
      "step": 48690
    },
    {
      "epoch": 2.999692023406221,
      "grad_norm": 0.1470002382993698,
      "learning_rate": 9.4446155425521e-08,
      "loss": 0.1762,
      "step": 48700
    }
  ],
  "logging_steps": 10,
  "max_steps": 48705,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.25627953412608e+17,
  "train_batch_size": 6,
  "trial_name": null,
  "trial_params": null
}
